{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21f13f2-fb06-4889-be76-d0296c6d7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f56d0-df48-4587-8112-25e14d25244f",
   "metadata": {},
   "source": [
    "`Ignite` is a High-level library to help with training neural networks in PyTorch. It comes with an `Engine` to setup a training loop, various metrics, handlers and a helpful contrib section! \n",
    "\n",
    "Below we import the following:\n",
    "* **Engine**: Runs a given process_function over each batch of a dataset, emitting events as it goes.\n",
    "* **Events**: Allows users to attach functions to an `Engine` to fire functions at a specific event. Eg: `EPOCH_COMPLETED`, `ITERATION_STARTED`, etc.\n",
    "* **Accuracy**: Metric to calculate accuracy over a dataset, for binary, multiclass, multilabel cases. \n",
    "* **Loss**: General metric that takes a loss function as a parameter, calculate loss over a dataset.\n",
    "* **RunningAverage**: General metric to attach to Engine during training. \n",
    "* **ModelCheckpoint**: Handler to checkpoint models. \n",
    "* **EarlyStopping**: Handler to stop training based on a score function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926f1444-7745-41b0-ae11-5f0ed99348d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage, ConfusionMatrix\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb08178-874b-47ed-9eb9-d8ef7e96d401",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc3a17e-6db8-4101-9530-4bdfb9131739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transform to normalize the data\n",
    "# transform = transforms.Compose([transforms.ToTensor(),\n",
    "#                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# # Download and load the training data\n",
    "# trainset = datasets.FashionMNIST('./data', download=True, train=True, transform=transform)\n",
    "# train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # Download and load the test data\n",
    "# validationset = datasets.FashionMNIST('./data', download=True, train=False, transform=transform)\n",
    "# val_loader = DataLoader(validationset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dddbd12-6031-47a7-9dd8-26bfbea79d9d",
   "metadata": {},
   "source": [
    "## simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ff3d42-5682-4d73-ae41-94b2b70c0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.convlayer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.convlayer2 = nn.Sequential(\n",
    "            nn.Conv2d(32,64,3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*6*6,600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(600, 120)\n",
    "        self.fc3 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convlayer1(x)\n",
    "        x = self.convlayer2(x)\n",
    "        x = x.view(-1,64*6*6)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af3589e-bd94-4dfa-890c-013a7f7f054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model,and defining optimizer and loss\n",
    "model = CNN()\n",
    "# moving model to gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d5cc6d-f19a-4b83-a377-bc8f0fa473b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.utils as torch_utils\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "\n",
    "# Core of the library, contains an engine for training and evaluating, most of the classic machine learning metrics \n",
    "# and a variety of handlers to ease the pain of training and validation of neural networks.\n",
    "\n",
    "from ignite.engine import Engine # Runs a given process_function over each batch of a dataset, emitting events as it goes.\n",
    "from ignite.engine import Events # Events that are fired by the Engine during execution.\n",
    "from ignite.metrics import RunningAverage # Compute running average of a metric or the output of process function.\n",
    "from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
    "\n",
    "from simple_nmt.utils import get_grad_norm, get_parameter_norm\n",
    "\n",
    "\n",
    "VERBOSE_SILENT = 0\n",
    "VERBOSE_EPOCH_WISE = 1\n",
    "VERBOSE_BATCH_WISE = 2\n",
    "\n",
    "# Gradient init -> FeedForward -> Loss -> back prop -> Gradient descent -> 현제상태 출력 // 을 만들거야. \n",
    "# Train, Valid engine두개를 선언하고\n",
    "# 그 두개를 물고 있는 엔진을 하나 또 설정해\n",
    "class MaximumLikelihoodEstimationEngine(Engine):\n",
    "\n",
    "    def __init__(self, func, model, crit, optimizer, lr_scheduler, config):\n",
    "        self.model = model\n",
    "        self.crit = crit # criterion (loss)\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.config = config\n",
    "\n",
    "        super().__init__(func)\n",
    "        # func을 Engine class에 보내 실행하기.. 근데 그럼 어떤 효과가 잇는거지?\n",
    "        '''https://pytorch.org/ignite/_modules/ignite/engine/engine.html#Engine'''\n",
    "        ''' 이해 안되면 이거 해보기.\n",
    "        class adding():\n",
    "            def __init__(self, a,b):\n",
    "                print(a+b)\n",
    "                return a+b\n",
    "            \n",
    "        class bdding(adding):\n",
    "            def __init__(self):\n",
    "                z = 1\n",
    "                super(bdding, self).__init__(1,3)\n",
    "\n",
    "        bdding()\n",
    "        '''    \n",
    "        \n",
    "\n",
    "        self.best_loss = np.inf\n",
    "        self.scaler = GradScaler() \n",
    "\n",
    "\n",
    "    # static : https://wikidocs.net/21054\n",
    "    @staticmethod\n",
    "    #@profile\n",
    "    def train(engine, mini_batch):\n",
    "        '''\n",
    "            engine : 여기에 Engine을 받나본데? // train engine, valid engine\n",
    "            mini_batch : train_loader가 return하는 객체\n",
    "                pytorch.DataLoader? 이런 비슷한거 있음.\n",
    "        '''\n",
    "        # You have to reset the gradients of all model parameters\n",
    "        # before to take another step in gradient descent.\n",
    "\n",
    "        '''\n",
    "            engine.state.iteration ; Number of iterations the engine has completed. Initialized as 0 and the first iteration is 1.\n",
    "                https://pytorch.org/ignite/v0.4.6/concepts.html#state\n",
    "            engine.iteration_per_update : update당 Iteration 숫자... 먼지 모르겟음.\n",
    "        '''\n",
    "        # accumulate gradient update\n",
    "        engine.model.train() # 파라미터 학습 할 것임.\n",
    "        if engine.state.iteration % engine.config.iteration_per_update == 1 or engine.config.iteration_per_update == 1: # 만약 설정을 config.iteration_per_update 을 1로 했을 경우, 매 iter마다 업데이트, 만약 설정을 10으로 하면 11마다 zero grad함.\n",
    "            if engine.state.iteration > 1:\n",
    "                engine.optimizer.zero_grad()\n",
    "\n",
    "        device = next(engine.model.parameters()).device # 모델이 어느 gpu에 있는지 따오는거\n",
    "        # 현재 모델이 어느 gpu에 있는지 구해서, 미니배치안에 잇는 텐서들을 해당 디바이스에 보내준다.\n",
    "\n",
    "        mini_batch.src = (mini_batch.src[0].to(device), mini_batch.src[1]) # encoder tuple, length // 근데 tuple gpu로 옮기는데 length는 안옮기네\n",
    "        mini_batch.tgt = (mini_batch.tgt[0].to(device), mini_batch.tgt[1]) # decoder x, y\n",
    "            # tuple은 tensor와 length가 있음.\n",
    "\n",
    "            # Raw target variable has both BOS and EOS token. \n",
    "            # The output of sequence-to-sequence does not have BOS token. \n",
    "            # Thus, remove BOS token for reference.\n",
    "        x, y = mini_batch.src, mini_batch.tgt[0][:, 1:]\n",
    "            # |x| = (batch_size, length) : x에서는 tensor, length를 가져가고\n",
    "            # |y| = (batch_size, length) : y(decoder)에서는 length가 빠지고, tensor들만 들어감. \n",
    "            # encoder에는 isinstance라는게 들어가서 tuple인지 확인했었어.\n",
    "            # y의 원래 텐서 모습은 {BOS, y_1, y_2, EOS}이건데, Teacher forcing답안용이기때문에 {y_1, y_2, eos} // 모델에 학습 데이터 들어갈땐 {BOS, y_1, y_2} 들어갈거야.\n",
    "\n",
    "        with autocast(not engine.config.off_autocast):\n",
    "                # Take feed-forward\n",
    "                # Similar as before, the input of decoder does not have EOS token.\n",
    "                # Thus, remove EOS token for decoder input.\n",
    "            y_hat = engine.model(x, mini_batch.tgt[0][:, :-1])  \n",
    "                # 모델에 학습 데이터 들어갈땐 {BOS, y_1, y_2} 들어갈거야.\n",
    "                # |y_hat| = (b, l, |V|) : 각 미니배치별, length별, 단어별 로그 확률 값이 들어가 있음.\n",
    "\n",
    "            loss = engine.crit(\n",
    "                y_hat.contiguous().view(-1, y_hat.size(-1)), # flatten\n",
    "                y.contiguous().view(-1) # flatten\n",
    "            )\n",
    "\n",
    "            backward_target = loss.div(y.size(0)).div(engine.config.iteration_per_update) \n",
    "                # NLL을 보면 -1/n *sigma(sgima)인데, Loss정의시 1/n을 안했음. 그래서 직접 나눠줘야함.\n",
    "                # 그래서 미니 배치 사이즈로 나눠주고, 이터레이션 퍼 업데이트로 나눠줌.\n",
    "\n",
    "        # gpu가 있을때 autocast가 수행\n",
    "        if engine.config.gpu_id >= 0 and not engine.config.off_autocast:\n",
    "            engine.scaler.scale(backward_target).backward()\n",
    "        else:\n",
    "            backward_target.backward()\n",
    "\n",
    "        word_count = int(mini_batch.tgt[1].sum())\n",
    "            # 단어의 갯수를 알아야 로스를 정확하게 구할 수 있음.\n",
    "            # 1 : 에는 각 batch별 length가 들어가 있을 것이기 때문에\n",
    "        p_norm = float(get_parameter_norm(engine.model.parameters())) # parameter norm\n",
    "        g_norm = float(get_grad_norm(engine.model.parameters())) # gradient norm : 안정적일수록 작아짐.\n",
    "        '''\n",
    "                @torch.no_grad()\n",
    "                def get_grad_norm(parameters, norm_type=2):\n",
    "                    parameters = list(filter(lambda p: p.grad is not None, parameters)) # 파라미터의수\n",
    "\n",
    "                    total_norm = 0\n",
    "\n",
    "                    try:\n",
    "                        for p in parameters:\n",
    "                            total_norm += (p.grad.data**norm_type).sum() # ||L2||_norm\n",
    "                        total_norm = total_norm ** (1. / norm_type) # \n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                    return total_norm\n",
    "\n",
    "\n",
    "                @torch.no_grad()\n",
    "                def get_parameter_norm(parameters, norm_type=2):\n",
    "                    total_norm = 0\n",
    "\n",
    "                    try:\n",
    "                        for p in parameters:\n",
    "                            total_norm += (p.data**norm_type).sum()\n",
    "                        total_norm = total_norm ** (1. / norm_type)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                    return total_norm\n",
    "        \n",
    "        '''\n",
    "\n",
    "\n",
    "        if engine.state.iteration % engine.config.iteration_per_update == 0 and \\\n",
    "            engine.state.iteration > 0:\n",
    "                # In order to avoid gradient exploding, we apply gradient clipping.\n",
    "            torch_utils.clip_grad_norm_(\n",
    "                engine.model.parameters(),\n",
    "                engine.config.max_grad_norm,\n",
    "            )\n",
    "\n",
    "            # Take a step of gradient descent.\n",
    "            # GPU가 있을 경우에, scale을 함.\n",
    "            if engine.config.gpu_id >= 0 and not engine.config.off_autocast:\n",
    "                # Use scaler instead of engine.optimizer.step() if using GPU.\n",
    "                engine.scaler.step(engine.optimizer)\n",
    "                engine.scaler.update()\n",
    "            else:\n",
    "                engine.optimizer.step()\n",
    "\n",
    "            # 만약 lr 스케쥴을 한다그러면..\n",
    "            # if engine.config.use_noam_decay and engine.lr_scheduler is not None:\n",
    "            #     engine.lr_scheduler.step()\n",
    "\n",
    "\n",
    "        loss = float(loss / word_count)\n",
    "            # 단어단 로스를 구할 수 있음.\n",
    "        ppl = np.exp(loss)\n",
    "\n",
    "        return {\n",
    "            'loss': loss, # 나중에 화면에 출력할 것.\n",
    "            'ppl': ppl,\n",
    "            '|param|': p_norm if not np.isnan(p_norm) and not np.isinf(p_norm) else 0., # 학습 초기에 None, inf뜨는 경우가 있었다. 그러면 학습이 안되므로 0을 리턴하도록한다.\n",
    "            '|g_param|': g_norm if not np.isnan(g_norm) and not np.isinf(g_norm) else 0.,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def validate(engine, mini_batch):\n",
    "        engine.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            device = next(engine.model.parameters()).device\n",
    "            mini_batch.src = (mini_batch.src[0].to(device), mini_batch.src[1])\n",
    "            mini_batch.tgt = (mini_batch.tgt[0].to(device), mini_batch.tgt[1])\n",
    "\n",
    "            x, y = mini_batch.src, mini_batch.tgt[0][:, 1:]\n",
    "            # |x| = (batch_size, length)\n",
    "            # |y| = (batch_size, length)\n",
    "\n",
    "            with autocast(not engine.config.off_autocast):\n",
    "                y_hat = engine.model(x, mini_batch.tgt[0][:, :-1])\n",
    "                # |y_hat| = (batch_size, n_classes)\n",
    "                loss = engine.crit(\n",
    "                    y_hat.contiguous().view(-1, y_hat.size(-1)),\n",
    "                    y.contiguous().view(-1),\n",
    "                )\n",
    "        \n",
    "        word_count = int(mini_batch.tgt[1].sum())\n",
    "        loss = float(loss / word_count)\n",
    "        ppl = np.exp(loss)\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'ppl': ppl,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attach(\n",
    "        train_engine, validation_engine,\n",
    "        training_metric_names = ['loss', 'ppl', '|param|', '|g_param|'],\n",
    "        validation_metric_names = ['loss', 'ppl'],\n",
    "        verbose=VERBOSE_BATCH_WISE, # 0,1,2 중 하나.\n",
    "    ):\n",
    "        # Attaching would be repaeted for serveral metrics.\n",
    "        # Thus, we can reduce the repeated codes by using this function.\n",
    "        def attach_running_average(engine, metric_name):\n",
    "            RunningAverage(output_transform=lambda x: x[metric_name]).attach(\n",
    "                engine,\n",
    "                metric_name,\n",
    "            )\n",
    "\n",
    "        for metric_name in training_metric_names:\n",
    "            attach_running_average(train_engine, metric_name)\n",
    "\n",
    "        if verbose >= VERBOSE_BATCH_WISE:\n",
    "            pbar = ProgressBar(bar_format=None, ncols=120) # progressbar만들어서 성과나오고.\n",
    "            pbar.attach(train_engine, training_metric_names)\n",
    "\n",
    "        if verbose >= VERBOSE_EPOCH_WISE:\n",
    "\n",
    "            # 에폭이 끝날때마다 train_engine에서 print안에 있는 친구들을 출력하라고 함.\n",
    "            @train_engine.on(Events.EPOCH_COMPLETED)\n",
    "            def print_train_logs(engine):\n",
    "                avg_p_norm = engine.state.metrics['|param|']\n",
    "                avg_g_norm = engine.state.metrics['|g_param|']\n",
    "                avg_loss = engine.state.metrics['loss']\n",
    "\n",
    "                print('Epoch {} - |param|={:.2e} |g_param|={:.2e} loss={:.4e} ppl={:.2f}'.format(\n",
    "                    engine.state.epoch,\n",
    "                    avg_p_norm,\n",
    "                    avg_g_norm,\n",
    "                    avg_loss,\n",
    "                    np.exp(avg_loss),\n",
    "                ))\n",
    "\n",
    "        # valid에 대해서도 동일하게 진행\n",
    "        for metric_name in validation_metric_names:\n",
    "            attach_running_average(validation_engine, metric_name)\n",
    "\n",
    "        if verbose >= VERBOSE_BATCH_WISE:\n",
    "            pbar = ProgressBar(bar_format=None, ncols=120)\n",
    "            pbar.attach(validation_engine, validation_metric_names)\n",
    "\n",
    "        if verbose >= VERBOSE_EPOCH_WISE:\n",
    "            @validation_engine.on(Events.EPOCH_COMPLETED)\n",
    "            def print_valid_logs(engine):\n",
    "                avg_loss = engine.state.metrics['loss']\n",
    "\n",
    "                print('Validation - loss={:.4e} ppl={:.2f} best_loss={:.4e} best_ppl={:.2f}'.format(\n",
    "                    avg_loss,\n",
    "                    np.exp(avg_loss),\n",
    "                    engine.best_loss,\n",
    "                    np.exp(engine.best_loss),\n",
    "                ))\n",
    "\n",
    "    @staticmethod\n",
    "    def resume_training(engine, resume_epoch):\n",
    "        engine.state.iteration = (resume_epoch - 1) * len(engine.state.dataloader)\n",
    "        engine.state.epoch = (resume_epoch - 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def check_best(engine):\n",
    "        loss = float(engine.state.metrics['loss'])\n",
    "        if loss <= engine.best_loss:\n",
    "            engine.best_loss = loss\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(engine, train_engine, config, src_vocab, tgt_vocab):\n",
    "        avg_train_loss = train_engine.state.metrics['loss']\n",
    "        avg_valid_loss = engine.state.metrics['loss']\n",
    "\n",
    "        # Set a filename for model of last epoch.\n",
    "        # We need to put every information to filename, as much as possible.\n",
    "        model_fn = config.model_fn.split('.')\n",
    "        \n",
    "        model_fn = model_fn[:-1] + ['%02d' % train_engine.state.epoch,\n",
    "                                    '%.2f-%.2f' % (avg_train_loss,\n",
    "                                                   np.exp(avg_train_loss)\n",
    "                                                   ),\n",
    "                                    '%.2f-%.2f' % (avg_valid_loss,\n",
    "                                                   np.exp(avg_valid_loss)\n",
    "                                                   )\n",
    "                                    ] + [model_fn[-1]]\n",
    "\n",
    "        model_fn = '.'.join(model_fn)\n",
    "\n",
    "        # Unlike other tasks, we need to save current model, not best model.\n",
    "        torch.save(\n",
    "            {\n",
    "                'model': engine.model.state_dict(),\n",
    "                'opt': train_engine.optimizer.state_dict(),\n",
    "                'config': config,\n",
    "                'src_vocab': src_vocab,\n",
    "                'tgt_vocab': tgt_vocab,\n",
    "            }, model_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79cf7a1b-7102-435b-b327-f90d86486ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "target_engine_class = MaximumLikelihoodEstimationEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb11e8-0cc9-4436-a3bf-ece94cd6fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_engine_class(target_engine_class.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a678a69-d5c6-49cc-b4e8-14a18a66ca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MaximumLikelihoodEstimationEngine at 0x14c6cff40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'lr_decay_start' : 0,\n",
    "          'init_epoch' : 0,\n",
    "          'n_epochs' : 10,\n",
    "          'lr_step' : 1,\n",
    "          'lr_gamma' : 0.5}\n",
    "\n",
    "def get_scheduler(optimizer, config):\n",
    "    '''Adam 같은 경우는 쓸 필요가 없고, SGD같은 경우 써야한데'''\n",
    "    '''\n",
    "         LR |\n",
    "          1 |-----------------\n",
    "            |\n",
    "            |                  --\n",
    "            |                      -- ... 10번부터는 lr을 반씩 줄여가면서 진행\n",
    "            |_____________________________________\n",
    "               1  2  3  4 ... 9 10 11 12         epochs\n",
    "    \n",
    "    '''\n",
    "    if config['lr_step'] > 0:\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                                                    optimizer,\n",
    "                                                    milestones=[i for i in range(\n",
    "                                                                                max(0, config['lr_decay_start'] - 1), # 내가 원하는 시작 에폭부터\n",
    "                                                                                (config['init_epoch'] - 1) + config['n_epochs'], # 내가 원하는 끝나는 에폭까지\n",
    "                                                                                config['lr_step'] # 1\n",
    "                                                                                )],\n",
    "                                                    gamma=config['lr_gamma'], # 0.5씩 작아짐.\n",
    "                                                    last_epoch=config['init_epoch'] - 1 if config['init_epoch'] > 1 else -1,\n",
    "                                                )\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "\n",
    "    return lr_scheduler\n",
    "\n",
    "\n",
    "lr_scheduler = get_scheduler(optimizer, config)\n",
    "\n",
    "\n",
    "\n",
    "target_engine_class(target_engine_class.train,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    lr_scheduler,\n",
    "                    config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0aa17e7-7a99-48df-bcd0-5f1c229996d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.Engine at 0x14c7d96d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Engine(target_engine_class.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1128783e-5be3-4ce4-b817-ca4193d0e0e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Engine' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_12813/589716278.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_engine_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Engine' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "Engine(target_engine_class.train).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021f769-5bb2-4b12-8114-8f981713b205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
