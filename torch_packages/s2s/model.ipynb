{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73bc3b7-a53f-43d1-97e0-b7e4ab64b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Be aware of value of 'batch_first' parameter.\n",
    "        # Also, its hidden_size is half of original hidden_size,\n",
    "        # because it is bidirectional.\n",
    "        self.rnn = nn.LSTM(\n",
    "            word_vec_size, # input shape\n",
    "            int(hidden_size / 2), # bidirectional 할 것이기 때문에, 나누기 2를 했다. -> 만약 소수점이 되버리면?\n",
    "            num_layers=n_layers, # stacking LSTM\n",
    "            dropout=dropout_p,\n",
    "            bidirectional=True,\n",
    "            batch_first=True, # batch의 쉐입이 첫번째가 아니라서 앞으로 오게 강제함\n",
    "        )\n",
    "\n",
    "    def forward(self, emb):\n",
    "        # |emb| = (batch_size, length, word_vec_size)\n",
    "\n",
    "        if isinstance(emb, tuple): # 임베딩 타입이 튜플이니? \n",
    "            x, lengths = emb\n",
    "            x = pack(x, lengths.tolist(), batch_first=True) # https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html\n",
    "            # input : input은 T*B*(*) /T는 가장긴 시퀀스/B는 배치사이즈,/(*)은 dim\n",
    "            # length : list of sequence lengths of each batch element\n",
    "\n",
    "\n",
    "            # Below is how pack_padded_sequence works.\n",
    "            # As you can see,\n",
    "            # PackedSequence object has information about mini-batch-wise information,\n",
    "            # not time-step-wise information.\n",
    "            # \n",
    "            # a = [torch.tensor([1,2,3]), \n",
    "            #      torch.tensor([3,4])]\n",
    "\n",
    "            # b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
    "            # >>>>\n",
    "            # tensor([[ 1,  2,  3],\n",
    "            #         [ 3,  4,  0]])\n",
    "            # torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2]\n",
    "            # >>>>PackedSequence(data=tensor([ 1,  3,  2,  4,  3]), batch_sizes=tensor([ 2,  2,  1]))\n",
    "        \n",
    "        else:\n",
    "            x = emb\n",
    "\n",
    "        y, h = self.rnn(x)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        # y: containing the output features (h_t) from the last layer of the LSTM, for each t // 모든 t시점에서 나온 hidden\n",
    "        # h: (containing the final hidden state for each element in the batch // containing the final cell state for each element in the batch.)\n",
    "        # |y| = (batch_size, length, hidden_size) : hidden_size * 2(정방향) / 2(역방향)\n",
    "        # |h[0]| = (num_layers * 2, batch_size, hidden_size / 2)\n",
    "                # num_layer * num_direction\n",
    "                # 바이다이렉셔널이라 num_layers * 2임 // ?배치사이즈 // ?(hidden_size / 2)\n",
    "\n",
    "        if isinstance(emb, tuple):\n",
    "            y, _ = unpack(y, batch_first=True) # 위에 packedsequence가 들어가있으면 풀어줘야 하기 때문에 씀.\n",
    "        \n",
    "        # y : [b, n, h]\n",
    "        # h : [l*2, b, h/2], [l*2, b, h/2]\n",
    "        return y, h\n",
    "\n",
    "\n",
    "data = torch.randn(64,30,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "213bbe9b-9dc1-42ae-85b7-95f3794a78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(10, 5, 4, 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298c5970-b735-438d-b404-09c79a6139b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db9ebfd-95fb-4e87-a066-9ec7a3462ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (rnn): LSTM(10, 2, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687eb0db-cade-4f99-bb43-28eee7e3493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y,h = encoder(data)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Be aware of value of 'batch_first' parameter and 'bidirectional' parameter.\n",
    "        self.rnn = nn.LSTM(\n",
    "            word_vec_size + hidden_size, # input feeding? 을 해줄거기 때문에(concat) 차원이 늘어난다.\n",
    "            hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout_p,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, emb_t, h_t_1_tilde, h_t_1):\n",
    "        '''\n",
    "        추론할때나, input feeding을 해줄것이기 때문에, 한스텝씩 들어올거야.\n",
    "\n",
    "        h_t_1_tilde : 저번에 예측한 hidden의 정보값. before softmax\n",
    "        h_t_1 : h_{t-1} = [h_{t-1}, c_{t-1}]   tuple임. // 전 스텝의 hidden값. //  [n layer, b, h]라는데(?)\n",
    "        '''\n",
    "        # |emb_t| = (batch_size, 1, word_vec_size)\n",
    "        # |h_t_1_tilde| = (batch_size, 1, hidden_size)\n",
    "        # |h_t_1[0]| = (n_layers, batch_size, hidden_size) : t-1 시점 전의 모든 히든들..같음 not sure yet\n",
    "        batch_size = emb_t.size(0) # [batch]\n",
    "        hidden_size = h_t_1[0].size(-1) # [hidden]\n",
    "        if h_t_1_tilde is None:\n",
    "            # If this is the first time-step, 이제 막 디코더가 시작한것임.\n",
    "            h_t_1_tilde = emb_t.new(batch_size, 1, hidden_size).zero_() # .new -> 텐서는 디바이스와, 타입이 같아야 arithmetic이 가능한데,.. 그러면 두번을 설정해 줘야함. 귀찮자나..\n",
    "                                                                                    # 가장 간단하게 하는 방법이. 저 텐서와 같은 디바이스, 타입인놈을 만들어줘. 하는게 new이다.\n",
    "                                                                        # .zero_() -> inplace 연산이다.\n",
    "\n",
    "        # Input feeding trick.\n",
    "        x = torch.cat([emb_t, h_t_1_tilde], dim=-1) # [b, 1, w + h]\n",
    "\n",
    "        # Unlike encoder, decoder must take an input for sequentially.\n",
    "        y, h = self.rnn(x, h_t_1)\n",
    "            # y : [b, n, h] // h: [l, b, h]\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae8d5af-0981-4321-9b9e-26f6361b84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_merge_encoder_hiddens(encoder_hiddens):\n",
    "        '''\n",
    "        parallel하게 해보자\n",
    "        encoder : [l*2, b, h/2], [l*2, b, h/2]\n",
    "        '''\n",
    "        # Merge bidirectional to uni-directional\n",
    "        # (layers*2, bs, hs/2) -> (layers, bs, hs).\n",
    "        # Thus, the converting operation will not working with just 'view' method.\n",
    "        h_0_tgt, c_0_tgt = encoder_hiddens # 두개 모두 [2layer, b, h/2]\n",
    "        batch_size = h_0_tgt.size(1)\n",
    "\n",
    "        # contiguous : 메모리상에 잘 붙어있게 선언하는것.\n",
    "        # transpose까지 하면 : [b, 2layer, h/2]\n",
    "        # view : [b, -1, hs] --> [b, layer, h]\n",
    "        # transpose : [layer, b, h]\n",
    "        h_0_tgt = h_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n",
    "                                                            -1,\n",
    "                                                            4\n",
    "                                                            ).transpose(0, 1).contiguous()\n",
    "        c_0_tgt = c_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n",
    "                                                            -1,\n",
    "                                                            4\n",
    "                                                            ).transpose(0, 1).contiguous()\n",
    "        # You can use 'merge_encoder_hiddens' method, instead of using above 3 lines.\n",
    "        # 'merge_encoder_hiddens' method works with non-parallel way.\n",
    "        # h_0_tgt = self.merge_encoder_hiddens(h_0_tgt)\n",
    "\n",
    "        # |h_src| = (batch_size, length, hidden_size)\n",
    "        # |h_0_tgt| = (n_layers, batch_size, hidden_size)\n",
    "        # [l, b, h], [l, b, h]\n",
    "        return h_0_tgt, c_0_tgt\n",
    "    \n",
    "    \n",
    "\n",
    "tgt = torch.randn(64,30,10)\n",
    "input_size = 30\n",
    "word_vec_size = 10\n",
    "    \n",
    "h_src, h_0_tgt = encoder(data) # packed_padded_sequence로 처리를 함.\n",
    "    # |h_src| = (b, n, h) : 인코더의 모든 t시점에서의 히든스테이트\n",
    "    # |h_0_tgt| = [l*2, b, h/2], [l*2, b, h/2] : 인코더에서 레이어마다 나온 마지막 히든스테이트(컨텍스트)\n",
    "        # -> 여기서 이친구를 decoder의 init hidden으로 넣어줘야 하는데,feature가 h/2임. 이걸 h로 변환해줘야함.\n",
    "\n",
    "h_0_tgt = fast_merge_encoder_hiddens(h_0_tgt)\n",
    "    # merge_encoder_hidden부터 살펴보자\n",
    "    # [l, b, h], [l, b, h]\n",
    "\n",
    "# teacher forcing이기 때문에 정답을 한꺼번에 만들어.\n",
    "emb_tgt = tgt\n",
    "    # |emb_tgt| = (b, l, emb)\n",
    "h_tilde = [] # 여기도 한방에 들어갈거야.\n",
    "\n",
    "h_t_tilde = None # 첫번째 타임스텝에서는 전에 있던 h_t_tilde는 없다.\n",
    "decoder_hidden = h_0_tgt # ([layer, bs, hs], [layer, bs, hs])\n",
    "\n",
    "\n",
    "decoder = Decoder(10,4,4,0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab9323d-020c-4ebc-a929-8337fd96ad5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emb_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_46122/1313174475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'emb_t' is not defined"
     ]
    }
   ],
   "source": [
    "emb_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e329c32-eed2-453d-b826-9aa6687d9cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (rnn): LSTM(14, 4, num_layers=4, batch_first=True, dropout=0.2)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83541fa8-b698-4be9-96e1-313843b5400d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False) # 맨처음에 projection needed for 가중치 refer to encoder part\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, h_src, h_t_tgt, mask=None):\n",
    "        # |h_src| = (batch_size, length, hidden_size) - 인코더의 모든 히든 스테잇\n",
    "        # |h_t_tgt| = (batch_size, 1, hidden_size) - 디코더의 히든 스테잇\n",
    "        # |mask| = (batch_size, length) - src의 마스킹할 정보\n",
    "\n",
    "        query = self.linear(h_t_tgt)                     # [B,1,H] * [B,H,H] = [B,1,H]\n",
    "        # |query| = (batch_size, 1, hidden_size)\n",
    "\n",
    "        weight = torch.bmm(query, h_src.transpose(1, 2)) # [B,1,H] * [B, H, L] => [B, 1, L] // bmm : batch multiplication\n",
    "        # |weight| = (batch_size, 1, length)\n",
    "        if mask is not None:\n",
    "            # Set each weight as -inf, if the mask value equals to 1.\n",
    "            # Since the softmax operation makes -inf to 0, \n",
    "            # masked weights would be set to 0 after softmax operation.\n",
    "            # Thus, if the sample is shorter than other samples in mini-batch,\n",
    "            # the weight for empty time-step would be set to 0.\n",
    "            weight.masked_fill_(mask.unsqueeze(1), -float('inf')) # mask가 있는 부분에 -float('inf')를 넣어줘\n",
    "        weight = self.softmax(weight)\n",
    "\n",
    "        context_vector = torch.bmm(weight, h_src)        # [B,1,L]*[B,L,H] -> [B,1,H]\n",
    "        # |context_vector| = (batch_size, 1, hidden_size)\n",
    "        # 해석으 해보면, 샘플 데이터에서, 디코더의 시점에서, 어텐션을 적용한 컨텐스트 벡터\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "def generate_mask(self, x, length):\n",
    "    '''\n",
    "    x : [bs, n]\n",
    "    length : [bs,] such as [4,3,1]\n",
    "    '''\n",
    "    mask = []\n",
    "\n",
    "    max_length = max(length)\n",
    "    for l in length:\n",
    "        if max_length - l > 0:\n",
    "            # If the length is shorter than maximum length among samples, \n",
    "            # set last few values to be 1s to remove attention weight.\n",
    "            mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
    "                                x.new_ones(1, (max_length - l))\n",
    "                                ], dim=-1)]\n",
    "        else:\n",
    "            # If the length of the sample equals to maximum length among samples, \n",
    "            # set every value in mask to be 0.\n",
    "            mask += [x.new_ones(1, l).zero_()]\n",
    "\n",
    "    mask = torch.cat(mask, dim=0).bool() # [[4,4], [4,4], [4,4]] -> [3, 4]짜리 텐서로 flatten\n",
    "\n",
    "    '''\n",
    "        length 에) 아래와 같은 텐서가 있을때 \n",
    "\n",
    "        --- --- --- ---\n",
    "        |  |   |   |  |  [4,\n",
    "        ___ ___ ___ ___\n",
    "        |  |   |   ||||   3,\n",
    "        --- --- --- ---\n",
    "        |   ||| ||| |||   1] 라는 x_length모양이 있을것임.\n",
    "        --- --- --- ---\n",
    "\n",
    "        --- --- --- ---\n",
    "        | 0|  0|  0| 0|  \n",
    "        ___ ___ ___ ___\n",
    "        | 0|  0|  0| 1|  \n",
    "        --- --- --- ---\n",
    "        | 0| 1| | 1| 1|   \n",
    "        --- --- --- ---\n",
    "        으로 나오게 한다.\n",
    "    '''\n",
    "    return mask\n",
    "\n",
    "    \n",
    "    \n",
    "hidden_size = 4\n",
    "attn = Attention(hidden_size)\n",
    "concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "tanh = nn.Tanh() \n",
    "\n",
    "mask = torch.randn(64,30).zero_()\n",
    "h_tilde = []\n",
    "\n",
    "\n",
    "for t in range(tgt.size(1)): # length of sentence\n",
    "    # Teacher Forcing: take each input from training set,\n",
    "    # not from the last time-step's output.\n",
    "    # Because of Teacher Forcing,\n",
    "    # training procedure and inference procedure becomes different.\n",
    "    # Of course, because of sequential running in decoder,\n",
    "    # this causes severe bottle-neck.\n",
    "    emb_t = emb_tgt[:, t, :].unsqueeze(1) # 한 단어씩 번갈아가면서 들어간다. // unsqueeze : 특정 차원에 차원을 추가한다.\n",
    "        # 인덱싱할 경우 [b, l, emb] -> [b,emb]되버릴 수 있다. 따라서 명시적으로 그냥 선언하자.\n",
    "    # |emb_t| = (batch_size, 1, word_vec_size)\n",
    "    # |h_t_tilde| = (batch_size, 1, hidden_size)\n",
    "\n",
    "    decoder_output, decoder_hidden = decoder(emb_t, # 현시점의 단어.\n",
    "                                                  h_t_tilde, # 지난 타임 스텝의 틸다\n",
    "                                                  decoder_hidden # [l, b, h], [l, b, h]\n",
    "                                                  )\n",
    "    # |decoder_output| = (batch_size, 1, hidden_size)\n",
    "    # |decoder_hidden| = (n_layers, batch_size, hidden_size)\n",
    "    \n",
    "    context_vector = attn(y, decoder_output, mask)\n",
    "    h_t_tilde = tanh(concat(torch.cat([decoder_output,\n",
    "                                                 context_vector\n",
    "                                                 ], dim=-1)))\n",
    "    # |h_t_tilde| = (batch_size, 1, hidden_size)\n",
    "    # self.concat -> 2h, h\n",
    "\n",
    "    h_tilde += [h_t_tilde]\n",
    "\n",
    "h_tilde = torch.cat(h_tilde, dim=1)\n",
    "    # h_tilde = (b, 1, h)\n",
    "    # concat on dim 1 => (b, m, h)\n",
    "    # |h_tilde| = (b, length, h)\n",
    "\n",
    "# y_hat = self.generator(h_tilde)\n",
    "# |y_hat| = (b, length, output_size:vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f38d7dc-a7fc-46c1-995a-32c0bf7355f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tilde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13363b64-5030-4e56-8632-363134ed730a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t_tilde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad83e8ae-5b61-4d70-8a8b-86edf9381eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0546b668-8f0c-41a1-b0ec-bd047fcbcccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5c9aa93-3589-425e-a0d9-3c93ae384de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t_tilde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3727bf80-2348-49d8-9032-d7f729d2e942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden[0].size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d0fbe667-e160-4900-9bc2-8b64a7d8b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(\n",
    "            10 + 4, # input feeding? 을 해줄거기 때문에(concat) 차원이 늘어난다.\n",
    "            4,\n",
    "            num_layers=4,\n",
    "            dropout=.2,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "x = torch.cat([emb_t, h_t_tilde], dim=-1) # [b, 1, w + h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8feca23a-d817-407d-b6e2-fc195a69450f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 4])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bb65202b-f194-4fe3-b09a-d2fa8851a762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 4])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z,zz = rnn(x, decoder_hidden)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bbd7fce8-aa35-4ef4-b43d-b5370be3653c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 4])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "119c8613-d0a0-43bb-bc98-db6f2a68f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10) # 3 5 10 \n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49b90980-4071-4c34-a310-2f28f5a6d60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 20])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
