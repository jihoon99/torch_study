{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "945c318b-47ea-4d74-a249-3a4c80c832fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4.3\n"
     ]
    }
   ],
   "source": [
    "!jupyter notebook --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599498ee-567d-4834-a0a4-e4d26cfe07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# is_decoding * torch.ne(y, data_loader.EOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa09abb-bde2-42d4-9be7-80c32ef45eef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf259cf1-e8aa-4e87-b9f3-3b8cce071eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# python train.py --train ./data/corpus.shuf.train.tok.bpe \n",
    "#                 --valid ./data/corpus.shuf.valid.tok.bpe \n",
    "#                 --lang enko \n",
    "#                 --gpu_id 0 \n",
    "#                 --batch_size 160 \n",
    "#                 --n_epochs 30 \n",
    "#                 --max_length 64 \n",
    "#                 --dropout .2 \n",
    "#                 --word_vec_size 512 \n",
    "#                 --hidden_size 768 \n",
    "#                 --n_layer 4 \n",
    "#                 --max_grad_norm 1e+8 \n",
    "#                 --iteration_per_update 2 \n",
    "#                 --lr 1e-3 \n",
    "#                 --lr_step 0 \n",
    "#                 --use_adam \n",
    "#                 --model_fn ./models/models.20200906/enko.bs-160.max_length-64.dropout-2.ws-512.hs-768.n_layers-4.iter_per_update-2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e0c006-1ad8-4c9f-a687-f7203c24f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchtext\n",
    "version = list(map(int, torchtext.__version__.split('.')))\n",
    "if version[0] <= 0 and version[1] < 9:\n",
    "    from torchtext import data, datasets\n",
    "else:\n",
    "    from torchtext.legacy import data, datasets\n",
    "\n",
    "PAD, BOS, EOS = 1, 2, 3\n",
    "# PAD의 번호는 1, BOS는 2, EOS는 3인가보네\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_fn=None,\n",
    "                 valid_fn=None,\n",
    "                 exts=None,\n",
    "                 batch_size=64,\n",
    "                 device=-1,\n",
    "                 max_vocab=99999999,\n",
    "                 max_length=255,\n",
    "                 fix_length=None,\n",
    "                 use_bos=True,\n",
    "                 use_eos=True,\n",
    "                 shuffle=True,\n",
    "                 dsl=False\n",
    "                 ):\n",
    "\n",
    "        super(DataLoader, self).__init__()\n",
    "\n",
    "        # Field -> fields -> TabularDataset -> build_vocab -> Bucket\n",
    "\n",
    "        # src와 tgt가 각각 있는 이유는, 파일이 각각 있었기 때문이다.\n",
    "            # torchtext.data.Field\n",
    "        self.src = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=True,\n",
    "            fix_length=fix_length, # None\n",
    "            init_token='<BOS>' if dsl else None, # dsl : dure learning할때 필요한것. 지금은 None이라고 보면 됨.\n",
    "            eos_token='<EOS>' if dsl else None,\n",
    "        )\n",
    "\n",
    "        self.tgt = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=True,\n",
    "            fix_length=fix_length,\n",
    "            init_token='<BOS>' if use_bos else None, # True .. learning에서는 필요 없고, 생성자 할때만 필요함(?)\n",
    "            eos_token='<EOS>' if use_eos else None,\n",
    "        )\n",
    "\n",
    "        if train_fn is not None and valid_fn is not None and exts is not None:\n",
    "            # TranslationDataset는 밑에 정의 되어있습니다.\n",
    "            train = TranslationDataset(\n",
    "                path=train_fn, # train file path\n",
    "                exts=exts, # en,ko path가 튜플로 들어가 있음.\n",
    "                fields=[('src', self.src), ('tgt', self.tgt)], # 사용할 필드 명\n",
    "                max_length=max_length\n",
    "            )\n",
    "            valid = TranslationDataset(\n",
    "                path=valid_fn,\n",
    "                exts=exts,\n",
    "                fields=[('src', self.src), ('tgt', self.tgt)],\n",
    "                max_length=max_length,\n",
    "            )\n",
    "\n",
    "            # bucketIterator가 하는 일을 실제 데이터를 가지고 와서. -> pad까지 체운 형태로 만들고\n",
    "            # 미니배치 단위로 만들어주는 역할을 한다.\n",
    "            # https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator\n",
    "            self.train_iter = data.BucketIterator(\n",
    "                train,\n",
    "                batch_size=batch_size,\n",
    "                device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "                shuffle=shuffle,\n",
    "                sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)), # ?????????????? what's x?\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "\n",
    "            self.valid_iter = data.BucketIterator(\n",
    "                valid,\n",
    "                batch_size=batch_size,\n",
    "                device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "                shuffle=False,\n",
    "                sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)),\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "\n",
    "            self.src.build_vocab(train, max_size=max_vocab)\n",
    "                # construct the vocab object for this field from one or more datasets.\n",
    "                # https://torchtext.readthedocs.io/en/latest/data.html\n",
    "                # it's word2idx : 어떤 단어가 몇번째 인덱스로 맵핑되는지.\n",
    "            self.tgt.build_vocab(train, max_size=max_vocab)\n",
    "\n",
    "    def load_vocab(self, src_vocab, tgt_vocab):\n",
    "        self.src.vocab = src_vocab\n",
    "        self.tgt.vocab = tgt_vocab\n",
    "\n",
    "\n",
    "# torchtext에는 maxlen을 잘라주는게 없어서 customizing했어.\n",
    "class TranslationDataset(data.Dataset):\n",
    "    \"\"\"Defines a dataset for machine translation.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "\n",
    "    def __init__(self, path, exts, fields, max_length=None, **kwargs):\n",
    "        \"\"\"Create a TranslationDataset given paths and fields.\n",
    "\n",
    "        Arguments:\n",
    "            path: Common prefix of paths to the data files for both languages.\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [('src', fields[0]), ('trg', fields[1])]\n",
    "\n",
    "        if not path.endswith('.'):\n",
    "            path += '.'\n",
    "\n",
    "        src_path, trg_path = tuple((path + x) for x in exts)\n",
    "\n",
    "        examples = []\n",
    "        with open(src_path, encoding='utf-8') as src_file, open(trg_path, encoding='utf-8') as trg_file:\n",
    "            for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                # max_length가 있을 경우에는 작업을 해줌.\n",
    "                if max_length and max_length < max(len(src_line.split()), len(trg_line.split())):\n",
    "                    continue\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    examples += [data.Example.fromlist([src_line, trg_line], fields)]\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed36d73-4aad-4301-9aa2-1040f28312b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rainism/Desktop/grad/torch_study/torch_packages/s2s\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6774199f-b630-4431-bdac-b3e292fd1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "\n",
    "# from simple_nmt.search import SingleBeamSearchBoard\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False) # 맨처음에 projection needed for 가중치 refer to encoder part\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, h_src, h_t_tgt, mask=None):\n",
    "        # |h_src| = (batch_size, length, hidden_size) - 인코더의 모든 히든 스테잇\n",
    "        # |h_t_tgt| = (batch_size, 1, hidden_size) - 디코더의 히든 스테잇\n",
    "        # |mask| = (batch_size, length) - src의 마스킹할 정보\n",
    "\n",
    "        query = self.linear(h_t_tgt)                     # [B,1,H] * [B,H,H] = [B,1,H]\n",
    "        # |query| = (batch_size, 1, hidden_size)\n",
    "\n",
    "        weight = torch.bmm(query, h_src.transpose(1, 2)) # [B,1,H] * [B, H, L] => [B, 1, L] // bmm : batch multiplication\n",
    "        # |weight| = (batch_size, 1, length)\n",
    "        if mask is not None:\n",
    "            # Set each weight as -inf, if the mask value equals to 1.\n",
    "            # Since the softmax operation makes -inf to 0, \n",
    "            # masked weights would be set to 0 after softmax operation.\n",
    "            # Thus, if the sample is shorter than other samples in mini-batch,\n",
    "            # the weight for empty time-step would be set to 0.\n",
    "            weight.masked_fill_(mask.unsqueeze(1), -float('inf')) # mask가 있는 부분에 -float('inf')를 넣어줘\n",
    "        weight = self.softmax(weight)\n",
    "\n",
    "        context_vector = torch.bmm(weight, h_src)        # [B,1,L]*[B,L,H] -> [B,1,H]\n",
    "        # |context_vector| = (batch_size, 1, hidden_size)\n",
    "        # 해석으 해보면, 샘플 데이터에서, 디코더의 시점에서, 어텐션을 적용한 컨텐스트 벡터\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Be aware of value of 'batch_first' parameter.\n",
    "        # Also, its hidden_size is half of original hidden_size,\n",
    "        # because it is bidirectional.\n",
    "        self.rnn = nn.LSTM(\n",
    "            word_vec_size, # input shape\n",
    "            int(hidden_size / 2), # bidirectional 할 것이기 때문에, 나누기 2를 했다. -> 만약 소수점이 되버리면?\n",
    "            num_layers=n_layers, # stacking LSTM\n",
    "            dropout=dropout_p,\n",
    "            bidirectional=True,\n",
    "            batch_first=True, # batch의 쉐입이 첫번째가 아니라서 앞으로 오게 강제함\n",
    "        )\n",
    "\n",
    "    def forward(self, emb):\n",
    "        # |emb| = (batch_size, length, word_vec_size)\n",
    "\n",
    "        if isinstance(emb, tuple): # 임베딩 타입이 튜플이니? \n",
    "            x, lengths = emb\n",
    "            x = pack(x, lengths.tolist(), batch_first=True) # https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html\n",
    "            # input : input은 T*B*(*) /T는 가장긴 시퀀스/B는 배치사이즈,/(*)은 dim\n",
    "            # length : list of sequence lengths of each batch element\n",
    "\n",
    "\n",
    "            # Below is how pack_padded_sequence works.\n",
    "            # As you can see,\n",
    "            # PackedSequence object has information about mini-batch-wise information,\n",
    "            # not time-step-wise information.\n",
    "            # \n",
    "            # a = [torch.tensor([1,2,3]), \n",
    "            #      torch.tensor([3,4])]\n",
    "\n",
    "            # b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
    "            # >>>>\n",
    "            # tensor([[ 1,  2,  3],\n",
    "            #         [ 3,  4,  0]])\n",
    "            # torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2]\n",
    "            # >>>>PackedSequence(data=tensor([ 1,  3,  2,  4,  3]), batch_sizes=tensor([ 2,  2,  1]))\n",
    "        \n",
    "        else:\n",
    "            x = emb\n",
    "\n",
    "        y, h = self.rnn(x)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        # y: containing the output features (h_t) from the last layer of the LSTM, for each t // 모든 t시점에서 나온 hidden\n",
    "        # h: (containing the final hidden state for each element in the batch // containing the final cell state for each element in the batch.)\n",
    "        # |y| = (batch_size, length, hidden_size) : hidden_size * 2(정방향) / 2(역방향)\n",
    "        # |h[0]| = (num_layers * 2, batch_size, hidden_size / 2)\n",
    "                # num_layer * num_direction\n",
    "                # 바이다이렉셔널이라 num_layers * 2임 // ?배치사이즈 // ?(hidden_size / 2)\n",
    "\n",
    "        if isinstance(emb, tuple):\n",
    "            y, _ = unpack(y, batch_first=True) # 위에 packedsequence가 들어가있으면 풀어줘야 하기 때문에 씀.\n",
    "        \n",
    "        # y : [b, n, h]\n",
    "        # h : [l*2, b, h/2], [l*2, b, h/2]\n",
    "        return y, h\n",
    "\n",
    "\n",
    "'''         y1            y2\n",
    "            |             |\n",
    "        |-------|     |-------|\n",
    "        |  RNN  | ->  |  RNN  | -> h            y1,y2...는 y에 나옴 // h : final hidden만 할당이 됨.\n",
    "        |_______|     |_______|\n",
    "            |             |\n",
    "            x1            x2\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Be aware of value of 'batch_first' parameter and 'bidirectional' parameter.\n",
    "        self.rnn = nn.LSTM(\n",
    "            word_vec_size + hidden_size, # input feeding? 을 해줄거기 때문에(concat) 차원이 늘어난다.\n",
    "            hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout_p,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, emb_t, h_t_1_tilde, h_t_1):\n",
    "        '''\n",
    "        추론할때나, input feeding을 해줄것이기 때문에, 한스텝씩 들어올거야.\n",
    "        h_t_1_tilde : 저번에 예측한 hidden의 정보값. before softmax\n",
    "        h_t_1 : h_{t-1} = [h_{t-1}, c_{t-1}]   tuple임. // 전 스텝의 hidden값. //  [n layer, b, h]라는데(?)\n",
    "        \n",
    "        # |emb_t| = (b, 1, word_vec_size)\n",
    "        # |h_t_1_tilde| = (b, 1, h)\n",
    "        # |h_t_1| = [(n_l, b, h),(n_l, b, h)] : t-1 시점 전의 모든 히든들..같음 not sure\n",
    "        '''\n",
    "        batch_size = emb_t.size(0) # [batch]\n",
    "        hidden_size = h_t_1[0].size(-1) # [hidden]\n",
    "\n",
    "        if h_t_1_tilde is None:\n",
    "            # If this is the first time-step, 이제 막 디코더가 시작한것임.\n",
    "            h_t_1_tilde = emb_t.new(batch_size, 1, hidden_size).zero_() # .new -> 텐서는 디바이스와, 타입이 같아야 arithmetic이 가능한데,.. 그러면 두번을 설정해 줘야함. 귀찮자나..\n",
    "                                                                                    # 가장 간단하게 하는 방법이. 저 텐서와 같은 디바이스, 타입인놈을 만들어줘. 하는게 new이다.\n",
    "                                                                        # .zero_() -> inplace 연산이다.\n",
    "\n",
    "        # Input feeding trick.\n",
    "        x = torch.cat([emb_t, h_t_1_tilde], dim=-1) # [b, 1, w + h]\n",
    "\n",
    "        # Unlike encoder, decoder must take an input for sequentially.\n",
    "        y, h = self.rnn(x, h_t_1) # h_t_1 : [(n_l, b, h), (n_l, b, h)] : 이전 시점의 hidden, context tensors, it is 0 when it's not provided.\n",
    "            # y : [b, n, h] // h: [l, b, h],[l,b,h]\n",
    "        return y, h\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.output = nn.Linear(hidden_size, output_size) # output_size : word vec size\n",
    "        self.softmax = nn.LogSoftmax(dim=-1) # logSoftmax를 함. (왜?)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, length, hidden_size) : 학습할때는 length길이 만큼 한번에 들어감. 왜냐하면 teacher forcing이니까.\n",
    "\n",
    "        y = self.softmax(self.output(x)) # linear에 한번 통과한다. 그러면 사이즈가 word sz로 바뀜.\n",
    "        # |y| = (batch_size, length, output_size)\n",
    "\n",
    "        # Return log-probability instead of just probability. : 미니배치, 각 샘플별, 각 단어별, 로그 확률값이 리턴이됨.\n",
    "        return y\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        word_vec_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        n_layers=4,\n",
    "        dropout_p=.2\n",
    "    ):\n",
    "\n",
    "        '''\n",
    "        input_size : input언어의 vocab size\n",
    "        word_vec_size : embed size\n",
    "        hidden_size : hidden sz\n",
    "        output_size : target언어의 vocab size\n",
    "        '''\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.word_vec_size = word_vec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "\n",
    "        # 임베드 정의\n",
    "        self.emb_src = nn.Embedding(input_size, word_vec_size)\n",
    "        self.emb_dec = nn.Embedding(output_size, word_vec_size)\n",
    "\n",
    "        # \n",
    "        self.encoder = Encoder(\n",
    "            word_vec_size, hidden_size,\n",
    "            n_layers=n_layers, dropout_p=dropout_p,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            word_vec_size, hidden_size,\n",
    "            n_layers=n_layers, dropout_p=dropout_p,\n",
    "        )\n",
    "        self.attn = Attention(hidden_size)\n",
    "\n",
    "        # attn에서 나온 context vec와 // decoder의 output하고 -> h_tilde\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.tanh = nn.Tanh() # 위 concat에 씌어줄 activation fn\n",
    "        self.generator = Generator(hidden_size, output_size)\n",
    "\n",
    "    def generate_mask(self, x, length):\n",
    "        '''\n",
    "        x : [bs, n]\n",
    "        length : [bs,] such as [4,3,1]\n",
    "        '''\n",
    "        mask = []\n",
    "\n",
    "        max_length = max(length)\n",
    "        for l in length:\n",
    "            if max_length - l > 0:\n",
    "                # If the length is shorter than maximum length among samples, \n",
    "                # set last few values to be 1s to remove attention weight.\n",
    "                mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
    "                                    x.new_ones(1, (max_length - l))\n",
    "                                    ], dim=-1)]\n",
    "            else:\n",
    "                # If the length of the sample equals to maximum length among samples, \n",
    "                # set every value in mask to be 0.\n",
    "                mask += [x.new_ones(1, l).zero_()]\n",
    "\n",
    "        mask = torch.cat(mask, dim=0).bool() # [[4,4], [4,4], [4,4]] -> [3, 4]짜리 텐서로 flatten\n",
    "\n",
    "        '''\n",
    "            length 에) 아래와 같은 텐서가 있을때 \n",
    "\n",
    "            --- --- --- ---\n",
    "            |  |   |   |  |  [4,\n",
    "            ___ ___ ___ ___\n",
    "            |  |   |   ||||   3,\n",
    "            --- --- --- ---\n",
    "            |   ||| ||| |||   1] 라는 x_length모양이 있을것임.\n",
    "            --- --- --- ---\n",
    "\n",
    "            --- --- --- ---\n",
    "            | 0|  0|  0| 0|  \n",
    "            ___ ___ ___ ___\n",
    "            | 0|  0|  0| 1|  \n",
    "            --- --- --- ---\n",
    "            | 0| 1| | 1| 1|   \n",
    "            --- --- --- ---\n",
    "            으로 나오게 한다.\n",
    "        '''\n",
    "        return mask\n",
    "\n",
    "\n",
    "\n",
    "    def merge_encoder_hiddens(self, encoder_hiddens):\n",
    "\n",
    "        '''\n",
    "        for loop을 하여 속도가 안좋음.\n",
    "        '''\n",
    "        new_hiddens = []\n",
    "        new_cells = []\n",
    "\n",
    "        hiddens, cells = encoder_hiddens\n",
    "            # encoder_hiddens는 hiddens와 cell_state두개를 갖고 있음.\n",
    "            # hiddens : [2*layers, batch, hidden/2]\n",
    "\n",
    "        # i-th and (i+1)-th layer is opposite direction.\n",
    "        # Also, each direction of layer is half hidden size.\n",
    "        # Therefore, we concatenate both directions to 1 hidden size layer.\n",
    "        for i in range(0, hiddens.size(0), 2): # 0~2*layers만큼 for문을 돌림.\n",
    "            new_hiddens += [torch.cat([hiddens[i], hiddens[i + 1]], dim=-1)] # 0,1 // 2,3 // 이런식으로 묶어서 넣어줌.\n",
    "                # new_hiddens : [bs, hs/2*2] -> [bs, hs]\n",
    "            new_cells += [torch.cat([cells[i], cells[i + 1]], dim=-1)]\n",
    "\n",
    "        new_hiddens, new_cells = torch.stack(new_hiddens), torch.stack(new_cells)\n",
    "                # new_hiddens : [layers, bs, hs]\n",
    "                # new_cells : [layers, bs, hs]\n",
    "        return (new_hiddens, new_cells)\n",
    "\n",
    "\n",
    "    def fast_merge_encoder_hiddens(self, encoder_hiddens):\n",
    "        '''\n",
    "        parallel하게 해보자\n",
    "        encoder : [l*2, b, h/2], [l*2, b, h/2]\n",
    "        '''\n",
    "        # Merge bidirectional to uni-directional\n",
    "        # (layers*2, bs, hs/2) -> (layers, bs, hs).\n",
    "        # Thus, the converting operation will not working with just 'view' method.\n",
    "        h_0_tgt, c_0_tgt = encoder_hiddens # 두개 모두 [2layer, b, h/2]\n",
    "        batch_size = h_0_tgt.size(1)\n",
    "\n",
    "        # contiguous : 메모리상에 잘 붙어있게 선언하는것.\n",
    "        # transpose까지 하면 : [b, 2layer, h/2]\n",
    "        # view : [b, -1, hs] --> [b, layer, h]\n",
    "        # transpose : [layer, b, h]\n",
    "        h_0_tgt = h_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n",
    "                                                            -1,\n",
    "                                                            self.hidden_size\n",
    "                                                            ).transpose(0, 1).contiguous()\n",
    "        c_0_tgt = c_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n",
    "                                                            -1,\n",
    "                                                            self.hidden_size\n",
    "                                                            ).transpose(0, 1).contiguous()\n",
    "        # You can use 'merge_encoder_hiddens' method, instead of using above 3 lines.\n",
    "        # 'merge_encoder_hiddens' method works with non-parallel way.\n",
    "        # h_0_tgt = self.merge_encoder_hiddens(h_0_tgt)\n",
    "\n",
    "        # |h_src| = (batch_size, length, hidden_size)\n",
    "        # |h_0_tgt| = (n_layers, batch_size, hidden_size)\n",
    "        # [l, b, h], [l, b, h]\n",
    "        return h_0_tgt, c_0_tgt\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        '''\n",
    "        학습할때는 teacher forcing을 할 것임.\n",
    "\n",
    "        src : input sentence = [bs, n, V_i]\n",
    "        tgt : target sentence = [bs, m, V_t]\n",
    "        '''\n",
    "        # output = [bs, m, V_t]\n",
    "\n",
    "        batch_size = tgt.size(0)\n",
    "\n",
    "        mask = None\n",
    "        x_length = None\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            '''\n",
    "            x_length에서 마스크 정보가 주어지면 generate_mask를 하라고 했음.\n",
    "            '''\n",
    "            # Based on the length information, gererate mask to prevent that\n",
    "            # shorter sample has wasted attention.\n",
    "            mask = self.generate_mask(x, x_length) \n",
    "            # //x : [bs, n] // x_length : [bs,] // mask : [bs, n]\n",
    "            # |mask| = (batch_size, length)\n",
    "            '''\n",
    "            length 에) 아래와 같은 텐서가 있을때 \n",
    "\n",
    "            --- --- --- ---\n",
    "            |  |   |   |  |  [4,\n",
    "            ___ ___ ___ ___\n",
    "            |  |   |   ||||   3,\n",
    "            --- --- --- ---\n",
    "            |   ||| ||| |||   1] 라는 x_length모양이 있을것임.\n",
    "            --- --- --- ---\n",
    "            \n",
    "            즉 [4,3,1]이 들어가 있음. 여기서 배치사이즈는 3임.\n",
    "            '''\n",
    "\n",
    "        else:\n",
    "            x = src\n",
    "\n",
    "        if isinstance(tgt, tuple):\n",
    "            tgt = tgt[0]\n",
    "\n",
    "\n",
    "        # Get word embedding vectors for every time-step of input sentence.\n",
    "        emb_src = self.emb_src(x) # |emb_src| = (b, n, emb)\n",
    "\n",
    "        # The last hidden state of the encoder would be a initial hidden state of decoder.\n",
    "        h_src, h_0_tgt = self.encoder((emb_src, x_length)) # packed_padded_sequence로 처리를 함.\n",
    "            # |h_src| = (b, n, h) : 인코더의 모든 t시점에서의 히든스테이트\n",
    "            # |h_0_tgt| = [l*2, b, h/2], [l*2, b, h/2] : 인코더에서 레이어마다 나온 마지막 히든스테이트(컨텍스트)\n",
    "                # -> 여기서 이친구를 decoder의 init hidden으로 넣어줘야 하는데,feature가 h/2임. 이걸 h로 변환해줘야함.\n",
    "\n",
    "        h_0_tgt = self.fast_merge_encoder_hiddens(h_0_tgt)\n",
    "            # merge_encoder_hidden부터 살펴보자\n",
    "            # [l, b, h], [l, b, h]\n",
    "\n",
    "        # teacher forcing이기 때문에 정답을 한꺼번에 만들어.\n",
    "        emb_tgt = self.emb_dec(tgt)\n",
    "            # |emb_tgt| = (b, l, emb)\n",
    "        h_tilde = [] # 여기도 한방에 들어갈거야.\n",
    "\n",
    "        h_t_tilde = None # 첫번째 타임스텝에서는 전에 있던 h_t_tilde는 없다.\n",
    "        decoder_hidden = h_0_tgt # ([layer, bs, hs], [layer, bs, hs])\n",
    "\n",
    "        # Run decoder until the end of the time-step.\n",
    "        for t in range(tgt.size(1)): # length of sentence\n",
    "            # Teacher Forcing: take each input from training set,\n",
    "            # not from the last time-step's output.\n",
    "            # Because of Teacher Forcing,\n",
    "            # training procedure and inference procedure becomes different.\n",
    "            # Of course, because of sequential running in decoder,\n",
    "            # this causes severe bottle-neck.\n",
    "            emb_t = emb_tgt[:, t, :].unsqueeze(1) # 한 단어씩 번갈아가면서 들어간다. // unsqueeze : 특정 차원에 차원을 추가한다.\n",
    "                # 인덱싱할 경우 [b, l, emb] -> [b,emb]되버릴 수 있다. 따라서 명시적으로 그냥 선언하자.\n",
    "            # |emb_t| = (batch_size, 1, word_vec_size)\n",
    "            # |h_t_tilde| = (batch_size, 1, hidden_size)\n",
    "\n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t, # 현시점의 단어.\n",
    "                                                          h_t_tilde, # 지난 타임 스텝의 틸다\n",
    "                                                          decoder_hidden # [l, b, h], [l, b, h]\n",
    "                                                          )\n",
    "            # |decoder_output| = (b, 1, h)\n",
    "            # |decoder_hidden| = (n, b, h), (n,b,h)\n",
    "\n",
    "\n",
    "            context_vector = self.attn(h_src, decoder_output, mask)\n",
    "            # |context_vector| = (batch_size, 1, hidden_size)\n",
    "\n",
    "            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n",
    "                                                         context_vector\n",
    "                                                         ], dim=-1)))\n",
    "            # |h_t_tilde| = (batch_size, 1, hidden_size)\n",
    "            # self.concat -> 2h, h\n",
    "\n",
    "            h_tilde += [h_t_tilde]\n",
    "\n",
    "        h_tilde = torch.cat(h_tilde, dim=1)\n",
    "            # h_tilde = (b, 1, h)\n",
    "            # concat on dim 1 => (b, m, h)\n",
    "            # |h_tilde| = (b, length, h)\n",
    "\n",
    "        y_hat = self.generator(h_tilde)\n",
    "        # |y_hat| = (b, length, output_size:vocab_size)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "\n",
    "    def search(self, src, is_greedy=True, max_length=255):\n",
    "        '''\n",
    "        추론을 위한 method\n",
    "\n",
    "        is_greedy : softmax에서 가장 높은 확률값을 갖는 친구를 return\n",
    "            - false일 경우 distribution sampling\n",
    "        '''\n",
    "        if isinstance(src, tuple):\n",
    "            # zero pad부분 masking\n",
    "            x, x_length = src\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "        else:\n",
    "            x, x_length = src, None\n",
    "            mask = None\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Same procedure as teacher forcing.\n",
    "        emb_src = self.emb_src(x) # [b, n, emb]\n",
    "        h_src, h_0_tgt = self.encoder((emb_src, x_length)) # (b,n,h), ([l*2, b, h/2], [l*2, b, h/2])\n",
    "        decoder_hidden = self.fast_merge_encoder_hiddens(h_0_tgt) # [l, b, h], [l, b, h]\n",
    "\n",
    "        # Fill a vector, which has 'batch_size' dimension, with BOS value.\n",
    "        y = x.new(batch_size, 1).zero_() + 2 # data_loader의 상단에 보면 BOS오브젝트 있음.\n",
    "            # x와 같은 타입, 디바이스를 [B, 1]을 0으로 채워서 만들고 거기다가 BOS인 2를 넣는다.\n",
    "            # 즉 [B,1] 2가 들어간 텐서가 만들어짐.\n",
    "        is_decoding = x.new_ones(batch_size, 1).bool()\n",
    "            # 배치마다 디코딩이 끝나는 부분이 다를것임.(?)\n",
    "            # 아직 디코딩 중이면, 1, 디코딩 끝낫으면 0\n",
    "        h_t_tilde, y_hats, indice = None, [], []\n",
    "        \n",
    "        # Repeat a loop while sum of 'is_decoding' flag is bigger than 0,\n",
    "        # or current time-step is smaller than maximum length.\n",
    "        while is_decoding.sum() > 0 and len(indice) < max_length:\n",
    "            # Unlike training procedure,\n",
    "            # take the last time-step's output during the inference.\n",
    "            emb_t = self.emb_dec(y) # 맨처음 y는 BOS임. 463번째 줄.\n",
    "            # |emb_t| = (batch_size, 1, word_vec_size)\n",
    "\n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t, # [B, 1, W]\n",
    "                                                          h_t_tilde, # None\n",
    "                                                          decoder_hidden) # [l,b,h],[l,b,h]\n",
    "                # decoder_output : [b, 1, h] \n",
    "                # decoder_hidden : [n,b,h], [n,b,h]\n",
    "            '''\n",
    "            decoder_output\n",
    "                 |\n",
    "                ____\n",
    "               |    | -> decoder_hidden\n",
    "                ----\n",
    "            \n",
    "            '''\n",
    "            context_vector = self.attn(h_src, decoder_output, mask)\n",
    "                # (b, 1, h)  # softmax(Q*W*K)\n",
    "            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n",
    "                                                         context_vector\n",
    "                                                         ], dim=-1)))\n",
    "            y_hat = self.generator(h_t_tilde)\n",
    "                # |y_hat| = (b, 1, output_size) 단어 분포가 나와.\n",
    "            y_hats += [y_hat]\n",
    "\n",
    "            if is_greedy:\n",
    "                y = y_hat.argmax(dim=-1)\n",
    "                # |y| = (batch_size, 1)\n",
    "            else:\n",
    "                # Take a random sampling based on the multinoulli distribution.\n",
    "                y = torch.multinomial(y_hat.exp().view(batch_size, -1), 1) # exponential이 왜필요할까?\n",
    "                # |y| = (batch_size, 1)\n",
    "\n",
    "            # Put PAD if the sample is done.\n",
    "            y = y.masked_fill_(~is_decoding, 1)\n",
    "                # masked_fill_ : ~is_decoding이 True이면, 1로 채움.\n",
    "                # ~ 는 리스트에 모두 -1을 해줌 -> 디코딩한 부분은 1에서 0으로 됨. \n",
    "                # 즉 디코딩한 부분은 PAD(1)으로 채워짐.\n",
    "                \n",
    "            # Update is_decoding if there is EOS token.\n",
    "            is_decoding = is_decoding * torch.ne(y, 3)\n",
    "            # |is_decoding| = (batch_size, 1)\n",
    "            indice += [y]\n",
    "\n",
    "        y_hats = torch.cat(y_hats, dim=1)\n",
    "        indice = torch.cat(indice, dim=1)\n",
    "        # |y_hat| = (batch_size, length, output_size)\n",
    "        # |indice| = (batch_size, length)\n",
    "\n",
    "        return y_hats, indice\n",
    "\n",
    "    #@profile\n",
    "    def batch_beam_search(\n",
    "        self,\n",
    "        src,\n",
    "        beam_size=5,\n",
    "        max_length=255,\n",
    "        n_best=1,\n",
    "        length_penalty=.2\n",
    "    ):\n",
    "        mask, x_length = None, None\n",
    "\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "            # |mask| = (batch_size, length)\n",
    "        else:\n",
    "            x = src\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        emb_src = self.emb_src(x)\n",
    "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
    "        # |h_src| = (batch_size, length, hidden_size)\n",
    "        h_0_tgt = self.fast_merge_encoder_hiddens(h_0_tgt)\n",
    "\n",
    "        # initialize 'SingleBeamSearchBoard' as many as batch_size\n",
    "        boards = [SingleBeamSearchBoard(\n",
    "            h_src.device,\n",
    "            {\n",
    "                'hidden_state': {\n",
    "                    'init_status': h_0_tgt[0][:, i, :].unsqueeze(1),\n",
    "                    'batch_dim_index': 1,\n",
    "                }, # |hidden_state| = (n_layers, batch_size, hidden_size)\n",
    "                'cell_state': {\n",
    "                    'init_status': h_0_tgt[1][:, i, :].unsqueeze(1),\n",
    "                    'batch_dim_index': 1,\n",
    "                }, # |cell_state| = (n_layers, batch_size, hidden_size)\n",
    "                'h_t_1_tilde': {\n",
    "                    'init_status': None,\n",
    "                    'batch_dim_index': 0,\n",
    "                }, # |h_t_1_tilde| = (batch_size, 1, hidden_size)\n",
    "            },\n",
    "            beam_size=beam_size,\n",
    "            max_length=max_length,\n",
    "        ) for i in range(batch_size)]\n",
    "        is_done = [board.is_done() for board in boards]\n",
    "\n",
    "        length = 0\n",
    "        # Run loop while sum of 'is_done' is smaller than batch_size, \n",
    "        # or length is still smaller than max_length.\n",
    "        while sum(is_done) < batch_size and length <= max_length:\n",
    "            # current_batch_size = sum(is_done) * beam_size\n",
    "\n",
    "            # Initialize fabricated variables.\n",
    "            # As far as batch-beam-search is running, \n",
    "            # temporary batch-size for fabricated mini-batch is \n",
    "            # 'beam_size'-times bigger than original batch_size.\n",
    "            fab_input, fab_hidden, fab_cell, fab_h_t_tilde = [], [], [], []\n",
    "            fab_h_src, fab_mask = [], []\n",
    "            \n",
    "            # Build fabricated mini-batch in non-parallel way.\n",
    "            # This may cause a bottle-neck.\n",
    "            for i, board in enumerate(boards):\n",
    "                # Batchify if the inference for the sample is still not finished.\n",
    "                if board.is_done() == 0:\n",
    "                    y_hat_i, prev_status = board.get_batch()\n",
    "                    hidden_i    = prev_status['hidden_state']\n",
    "                    cell_i      = prev_status['cell_state']\n",
    "                    h_t_tilde_i = prev_status['h_t_1_tilde']\n",
    "\n",
    "                    fab_input  += [y_hat_i]\n",
    "                    fab_hidden += [hidden_i]\n",
    "                    fab_cell   += [cell_i]\n",
    "                    fab_h_src  += [h_src[i, :, :]] * beam_size\n",
    "                    fab_mask   += [mask[i, :]] * beam_size\n",
    "                    if h_t_tilde_i is not None:\n",
    "                        fab_h_t_tilde += [h_t_tilde_i]\n",
    "                    else:\n",
    "                        fab_h_t_tilde = None\n",
    "\n",
    "            # Now, concatenate list of tensors.\n",
    "            fab_input  = torch.cat(fab_input,  dim=0)\n",
    "            fab_hidden = torch.cat(fab_hidden, dim=1)\n",
    "            fab_cell   = torch.cat(fab_cell,   dim=1)\n",
    "            fab_h_src  = torch.stack(fab_h_src)\n",
    "            fab_mask   = torch.stack(fab_mask)\n",
    "            if fab_h_t_tilde is not None:\n",
    "                fab_h_t_tilde = torch.cat(fab_h_t_tilde, dim=0)\n",
    "            # |fab_input|     = (current_batch_size, 1)\n",
    "            # |fab_hidden|    = (n_layers, current_batch_size, hidden_size)\n",
    "            # |fab_cell|      = (n_layers, current_batch_size, hidden_size)\n",
    "            # |fab_h_src|     = (current_batch_size, length, hidden_size)\n",
    "            # |fab_mask|      = (current_batch_size, length)\n",
    "            # |fab_h_t_tilde| = (current_batch_size, 1, hidden_size)\n",
    "\n",
    "            emb_t = self.emb_dec(fab_input)\n",
    "            # |emb_t| = (current_batch_size, 1, word_vec_size)\n",
    "\n",
    "            fab_decoder_output, (fab_hidden, fab_cell) = self.decoder(emb_t,\n",
    "                                                                      fab_h_t_tilde,\n",
    "                                                                      (fab_hidden, fab_cell))\n",
    "            # |fab_decoder_output| = (current_batch_size, 1, hidden_size)\n",
    "            context_vector = self.attn(fab_h_src, fab_decoder_output, fab_mask)\n",
    "            # |context_vector| = (current_batch_size, 1, hidden_size)\n",
    "            fab_h_t_tilde = self.tanh(self.concat(torch.cat([fab_decoder_output,\n",
    "                                                             context_vector\n",
    "                                                             ], dim=-1)))\n",
    "            # |fab_h_t_tilde| = (current_batch_size, 1, hidden_size)\n",
    "            y_hat = self.generator(fab_h_t_tilde)\n",
    "            # |y_hat| = (current_batch_size, 1, output_size)\n",
    "\n",
    "            # separate the result for each sample.\n",
    "            # fab_hidden[:, begin:end, :] = (n_layers, beam_size, hidden_size)\n",
    "            # fab_cell[:, begin:end, :]   = (n_layers, beam_size, hidden_size)\n",
    "            # fab_h_t_tilde[begin:end]    = (beam_size, 1, hidden_size)\n",
    "            cnt = 0\n",
    "            for board in boards:\n",
    "                if board.is_done() == 0:\n",
    "                    # Decide a range of each sample.\n",
    "                    begin = cnt * beam_size\n",
    "                    end = begin + beam_size\n",
    "\n",
    "                    # pick k-best results for each sample.\n",
    "                    board.collect_result(\n",
    "                        y_hat[begin:end],\n",
    "                        {\n",
    "                            'hidden_state': fab_hidden[:, begin:end, :],\n",
    "                            'cell_state'  : fab_cell[:, begin:end, :],\n",
    "                            'h_t_1_tilde' : fab_h_t_tilde[begin:end],\n",
    "                        },\n",
    "                    )\n",
    "                    cnt += 1\n",
    "\n",
    "            is_done = [board.is_done() for board in boards]\n",
    "            length += 1\n",
    "\n",
    "        # pick n-best hypothesis.\n",
    "        batch_sentences, batch_probs = [], []\n",
    "\n",
    "        # Collect the results.\n",
    "        for i, board in enumerate(boards):\n",
    "            sentences, probs = board.get_n_best(n_best, length_penalty=length_penalty)\n",
    "\n",
    "            batch_sentences += [sentences]\n",
    "            batch_probs     += [probs]\n",
    "\n",
    "        return batch_sentences, batch_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906c42bc-6172-4ec7-a38c-761ff7c93c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader('/Users/rainism/Desktop/grad/torch_study/transformer/data/corpus.shuf.train.tok.bpe',\n",
    "                       '/Users/rainism/Desktop/grad/torch_study/transformer/data/corpus.shuf.valid.tok.bpe',\n",
    "                       ['en','ko'],\n",
    "                       max_vocab = 255,\n",
    "                       max_length = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c178ec11-86cb-4ae2-afa3-94a728be5415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input, output size : (257, 259)\n"
     ]
    }
   ],
   "source": [
    "input_size, output_size = len(dataloader.src.vocab), len(dataloader.tgt.vocab)\n",
    "print(f'input, output size : {input_size, output_size}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97221bae-4096-4e3b-ac8f-c4902dbd246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(input_size,\n",
    "                word_vec_size = 512,\n",
    "                hidden_size = 768,\n",
    "                output_size = output_size,\n",
    "                n_layers = 4,\n",
    "                dropout_p = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d35864d-0a5b-4d81-8228-4bef9dfc3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crit(output_size, pad_index):\n",
    "    # Default weight for loss equals to 1, but we don't need to get loss for PAD token.\n",
    "    # Thus, set a weight for PAD to zero.\n",
    "    loss_weight = torch.ones(output_size)\n",
    "        # 패딩이 되어있는곳은 맞춰도 점수를 주고 싶지 않은것.\n",
    "    '''      ___________\n",
    "            |   |   |   |\n",
    "            -------------\n",
    "            |   |   |||||\n",
    "            |-----------|\n",
    "            |   |||||||||\n",
    "            -------------    \n",
    "        \n",
    "    '''\n",
    "    loss_weight[pad_index] = 0.\n",
    "        # 패드인덱스를 받아와서 거기다가 0을 할당해서, 맞춰도 점수를 주지마.\n",
    "    # Instead of using Cross-Entropy loss,\n",
    "    # we can use Negative Log-Likelihood(NLL) loss with log-probability.\n",
    "    crit = nn.NLLLoss(\n",
    "        weight=loss_weight,\n",
    "        reduction='sum'\n",
    "    )\n",
    "\n",
    "    return crit\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "def get_optimizer(model, config):\n",
    "    if config:\n",
    "#         if config.use_transformer:\n",
    "#             optimizer = optim.Adam(model.parameters(), lr=config.lr, betas=(.9, .98))\n",
    "#         else: # case of rnn based seq2seq.\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#     elif config.use_radam:\n",
    "#         optimizer = custom_optim.RAdam(model.parameters(), lr=config.lr)\n",
    "#     else:\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=config.lr) # initial lr = 1, 그리고 그래디언트가 폭주할거 같으면 클립핑을 함(?)-> 요놈은 한번도 안해봄.\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "141995a5-0dbe-44d9-a432-40c042b2352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = get_crit(output_size, 1)\n",
    "config = True\n",
    "optimizer = get_optimizer(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9239c15-6a0a-421b-933e-a8ac33b1384c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
