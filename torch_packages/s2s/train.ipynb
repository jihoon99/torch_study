{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e279f5b-3114-4951-a8b6-dca9377c372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_optimizer as custom_optim\n",
    "\n",
    "from simple_nmt.data_loader import DataLoader\n",
    "import simple_nmt.data_loader as data_loader\n",
    "\n",
    "from simple_nmt.models.seq2seq import Seq2Seq\n",
    "from simple_nmt.models.transformer import Transformer\n",
    "from simple_nmt.models.rnnlm import LanguageModel\n",
    "\n",
    "from simple_nmt.trainer import SingleTrainer\n",
    "from simple_nmt.rl_trainer import MinimumRiskTrainingEngine\n",
    "from simple_nmt.trainer import MaximumLikelihoodEstimationEngine\n",
    "\n",
    "\n",
    "def define_argparser(is_continue=False):\n",
    "    p = argparse.ArgumentParser()\n",
    "\n",
    "    if is_continue:\n",
    "        p.add_argument(\n",
    "            '--load_fn',\n",
    "            required=True,\n",
    "            help='Model file name to continue.'\n",
    "        )\n",
    "\n",
    "    p.add_argument(\n",
    "        '--model_fn',\n",
    "        required=not is_continue,\n",
    "        help='Model file name to save. Additional information would be annotated to the file name.'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--train',\n",
    "        required=not is_continue,\n",
    "        help='Training set file name except the extention. (ex: train.en --> train)'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--valid',\n",
    "        required=not is_continue,\n",
    "        help='Validation set file name except the extention. (ex: valid.en --> valid)'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--lang',\n",
    "        required=not is_continue,\n",
    "        help='Set of extention represents language pair. (ex: en + ko --> enko)'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--gpu_id',\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        help='GPU ID to train. Currently, GPU parallel is not supported. -1 for CPU. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--off_autocast',\n",
    "        action='store_true',\n",
    "        help='Turn-off Automatic Mixed Precision (AMP), which speed-up training.',\n",
    "    )\n",
    "\n",
    "    p.add_argument(\n",
    "        '--batch_size',\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help='Mini batch size for gradient descent. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--n_epochs',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='Number of epochs to train. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--verbose',\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help='VERBOSE_SILENT, VERBOSE_EPOCH_WISE, VERBOSE_BATCH_WISE = 0, 1, 2. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--init_epoch',\n",
    "        required=is_continue,\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Set initial epoch number, which can be useful in continue training. Default=%(default)s'\n",
    "    )\n",
    "\n",
    "    p.add_argument(\n",
    "        '--max_length',\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help='Maximum length of the training sequence. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--dropout',\n",
    "        type=float,\n",
    "        default=.2,\n",
    "        help='Dropout rate. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--word_vec_size',\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help='Word embedding vector dimension. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--hidden_size',\n",
    "        type=int,\n",
    "        default=768,\n",
    "        help='Hidden size of LSTM. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--n_layers',\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help='Number of layers in LSTM. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--max_grad_norm',\n",
    "        type=float,\n",
    "        default=5.,\n",
    "        help='Threshold for gradient clipping. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--iteration_per_update',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Number of feed-forward iterations for one parameter update. Default=%(default)s'\n",
    "    )\n",
    "\n",
    "    p.add_argument(\n",
    "        '--lr',\n",
    "        type=float,\n",
    "        default=1.,\n",
    "        help='Initial learning rate. Default=%(default)s',\n",
    "    )\n",
    "\n",
    "    p.add_argument(\n",
    "        '--lr_step',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Number of epochs for each learning rate decay. Default=%(default)s',\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--lr_gamma',\n",
    "        type=float,\n",
    "        default=.5,\n",
    "        help='Learning rate decay rate. Default=%(default)s',\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--lr_decay_start',\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help='Learning rate decay start at. Default=%(default)s',\n",
    "    )\n",
    "\n",
    "    p.add_argument(\n",
    "        '--use_adam',\n",
    "        action='store_true',\n",
    "        help='Use Adam as optimizer instead of SGD. Other lr arguments should be changed.',\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--use_radam',\n",
    "        action='store_true',\n",
    "        help='Use rectified Adam as optimizer. Other lr arguments should be changed.',\n",
    "    )\n",
    "\n",
    "    p.add_argument(\n",
    "        '--rl_lr',\n",
    "        type=float,\n",
    "        default=.01,\n",
    "        help='Learning rate for reinforcement learning. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--rl_n_samples',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Number of samples to get baseline. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--rl_n_epochs',\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help='Number of epochs for reinforcement learning. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--rl_n_gram',\n",
    "        type=int,\n",
    "        default=6,\n",
    "        help='Maximum number of tokens to calculate BLEU for reinforcement learning. Default=%(default)s'\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--rl_reward',\n",
    "        type=str,\n",
    "        default='gleu',\n",
    "        help='Method name to use as reward function for RL training. Default=%(default)s'\n",
    "    )\n",
    "\n",
    "    p.add_argument(\n",
    "        '--use_transformer',\n",
    "        action='store_true',\n",
    "        help='Set model architecture as Transformer.',\n",
    "    )\n",
    "    p.add_argument(\n",
    "        '--n_splits',\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help='Number of heads in multi-head attention in Transformer. Default=%(default)s',\n",
    "    )\n",
    "\n",
    "    config = p.parse_args()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_model(input_size, output_size, config):\n",
    "    if config.use_transformer:\n",
    "        model = Transformer(\n",
    "            input_size,                     # Source vocabulary size\n",
    "            config.hidden_size,             # Transformer doesn't need word_vec_size.\n",
    "            output_size,                    # Target vocabulary size\n",
    "            n_splits=config.n_splits,       # Number of head in Multi-head Attention.\n",
    "            n_enc_blocks=config.n_layers,   # Number of encoder blocks\n",
    "            n_dec_blocks=config.n_layers,   # Number of decoder blocks\n",
    "            dropout_p=config.dropout,       # Dropout rate on each block\n",
    "        )\n",
    "    else:\n",
    "        model = Seq2Seq(\n",
    "            input_size,\n",
    "            config.word_vec_size,           # Word embedding vector size\n",
    "            config.hidden_size,             # LSTM's hidden vector size\n",
    "            output_size,\n",
    "            n_layers=config.n_layers,       # number of layers in LSTM\n",
    "            dropout_p=config.dropout        # dropout-rate in LSTM\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_crit(output_size, pad_index):\n",
    "    # Default weight for loss equals to 1, but we don't need to get loss for PAD token.\n",
    "    # Thus, set a weight for PAD to zero.\n",
    "    loss_weight = torch.ones(output_size)\n",
    "    loss_weight[pad_index] = 0.\n",
    "    # Instead of using Cross-Entropy loss,\n",
    "    # we can use Negative Log-Likelihood(NLL) loss with log-probability.\n",
    "    crit = nn.NLLLoss(\n",
    "        weight=loss_weight,\n",
    "        reduction='sum'\n",
    "    )\n",
    "\n",
    "    return crit\n",
    "\n",
    "\n",
    "def get_optimizer(model, config):\n",
    "    if config.use_adam:\n",
    "        if config.use_transformer:\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config.lr, betas=(.9, .98))\n",
    "        else: # case of rnn based seq2seq.\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    elif config.use_radam:\n",
    "        optimizer = custom_optim.RAdam(model.parameters(), lr=config.lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config.lr)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, config):\n",
    "    if config.lr_step > 0:\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer,\n",
    "            milestones=[i for i in range(\n",
    "                max(0, config.lr_decay_start - 1),\n",
    "                (config.init_epoch - 1) + config.n_epochs,\n",
    "                config.lr_step\n",
    "            )],\n",
    "            gamma=config.lr_gamma,\n",
    "            last_epoch=config.init_epoch - 1 if config.init_epoch > 1 else -1,\n",
    "        )\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "\n",
    "    return lr_scheduler\n",
    "\n",
    "\n",
    "# # 결국 이친구가 돌아갈텐데\n",
    "# def main(config, model_weight=None, opt_weight=None):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0ddbed-faeb-4007-abc4-d8429ba89f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader():\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_fn=None,\n",
    "                 valid_fn=None,\n",
    "                 exts=None,\n",
    "                 batch_size=64,\n",
    "                 device='cpu',\n",
    "                 max_vocab=99999999,\n",
    "                 max_length=255,\n",
    "                 fix_length=None,\n",
    "                 use_bos=True,\n",
    "                 use_eos=True,\n",
    "                 shuffle=True,\n",
    "                 dsl=False\n",
    "                 ):\n",
    "\n",
    "        super(DataLoader, self).__init__()\n",
    "\n",
    "        # Field -> fields -> TabularDataset -> build_vocab -> Bucket\n",
    "\n",
    "        # src와 tgt가 각각 있는 이유는, 파일이 각각 있었기 때문이다.\n",
    "            # torchtext.data.Field\n",
    "        self.src = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=True,\n",
    "            fix_length=fix_length, # None\n",
    "            init_token='<BOS>' if dsl else None, # dsl : dure learning할때 필요한것. 지금은 None이라고 보면 됨.\n",
    "            eos_token='<EOS>' if dsl else None,\n",
    "        )\n",
    "\n",
    "        self.tgt = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=True,\n",
    "            fix_length=fix_length,\n",
    "            init_token='<BOS>' if use_bos else None, # True .. learning에서는 필요 없고, 생성자 할때만 필요함(?)\n",
    "            eos_token='<EOS>' if use_eos else None,\n",
    "        )\n",
    "\n",
    "        if train_fn is not None and valid_fn is not None and exts is not None:\n",
    "            # TranslationDataset는 밑에 정의 되어있습니다.\n",
    "            train = TranslationDataset(\n",
    "                path=train_fn, # train file path\n",
    "                exts=exts, # en,ko path가 튜플로 들어가 있음.\n",
    "                fields=[('src', self.src), ('tgt', self.tgt)], # 사용할 필드 명\n",
    "                max_length=max_length\n",
    "            )\n",
    "            valid = TranslationDataset(\n",
    "                path=valid_fn,\n",
    "                exts=exts,\n",
    "                fields=[('src', self.src), ('tgt', self.tgt)],\n",
    "                max_length=max_length,\n",
    "            )\n",
    "\n",
    "            # bucketIterator가 하는 일을 실제 데이터를 가지고 와서. -> pad까지 체운 형태로 만들고\n",
    "            # 미니배치 단위로 만들어주는 역할을 한다.\n",
    "            # https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator\n",
    "            self.train_iter = data.BucketIterator(\n",
    "                train,\n",
    "                batch_size=batch_size,\n",
    "                device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "                shuffle=shuffle,\n",
    "                sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)), # ?????????????? what's x?\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "\n",
    "            self.valid_iter = data.BucketIterator(\n",
    "                valid,\n",
    "                batch_size=batch_size,\n",
    "                device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "                shuffle=False,\n",
    "                sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)),\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "\n",
    "            self.src.build_vocab(train, max_size=max_vocab)\n",
    "                # construct the vocab object for this field from one or more datasets.\n",
    "                # https://torchtext.readthedocs.io/en/latest/data.html\n",
    "                # it's word2idx : 어떤 단어가 몇번째 인덱스로 맵핑되는지.\n",
    "            self.tgt.build_vocab(train, max_size=max_vocab)\n",
    "\n",
    "    def load_vocab(self, src_vocab, tgt_vocab):\n",
    "        self.src.vocab = src_vocab\n",
    "        self.tgt.vocab = tgt_vocab\n",
    "\n",
    "import os\n",
    "import torchtext\n",
    "version = list(map(int, torchtext.__version__.split('.')))\n",
    "if version[0] <= 0 and version[1] < 9:\n",
    "    from torchtext import data, datasets\n",
    "else:\n",
    "    from torchtext.legacy import data, datasets\n",
    "\n",
    "# torchtext에는 maxlen을 잘라주는게 없어서 customizing했어.\n",
    "class TranslationDataset(data.Dataset):\n",
    "    \"\"\"Defines a dataset for machine translation.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "\n",
    "    def __init__(self, path, exts, fields, max_length=None, **kwargs):\n",
    "        \"\"\"Create a TranslationDataset given paths and fields.\n",
    "\n",
    "        Arguments:\n",
    "            path: Common prefix of paths to the data files for both languages.\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [('src', fields[0]), ('trg', fields[1])]\n",
    "\n",
    "        if not path.endswith('.'):\n",
    "            path += '.'\n",
    "\n",
    "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
    "\n",
    "        examples = []\n",
    "        with open(src_path, encoding='utf-8') as src_file, open(trg_path, encoding='utf-8') as trg_file:\n",
    "            for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                # max_length가 있을 경우에는 작업을 해줌.\n",
    "                if max_length and max_length < max(len(src_line.split()), len(trg_line.split())):\n",
    "                    continue\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    examples += [data.Example.fromlist([src_line, trg_line], fields)]\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53b8f8-e783-4272-a38d-ba33c3d3b1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781570a8-01e9-48ad-a3a6-88bffd57831e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'corpus.shuf.en.en'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5242/1737312328.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loader = DataLoader(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'corpus.shuf.en'\u001b[0m\u001b[0;34m,\u001b[0m                           \u001b[0;31m# Train file name except extention, which is language.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'corpus.shuf.de'\u001b[0m\u001b[0;34m,\u001b[0m                           \u001b[0;31m# Validation file name except extension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ko'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# Source and target language. // 예) en, ko -> enko로 등록을 해야함.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5242/2523040400.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_fn, valid_fn, exts, batch_size, device, max_vocab, max_length, fix_length, use_bos, use_eos, shuffle, dsl)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalid_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# TranslationDataset는 밑에 정의 되어있습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             train = TranslationDataset(\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# train file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# en,ko path가 튜플로 들어가 있음.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_5242/2523040400.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, exts, fields, max_length, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrg_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msrc_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0msrc_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_line\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_line\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'corpus.shuf.en.en'"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(\n",
    "    'corpus.shuf.en',                           # Train file name except extention, which is language.\n",
    "    'corpus.shuf.de',                           # Validation file name except extension.\n",
    "    ('en', 'ko'),    # Source and target language. // 예) en, ko -> enko로 등록을 해야함.\n",
    "    batch_size=64,\n",
    "    device=-1,                              # Lazy loading\n",
    "    max_length=100,           # Loger sequence will be excluded.\n",
    "    dsl=False,                              # Turn-off Dual-supervised Learning mode.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ad32a-8c79-4e8e-9de1-a812c31f8254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1dc9c-99ed-4efc-a40a-eb17bd8eb8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cf3ef-e21e-406e-ba72-7b9a98dc52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_config(config):\n",
    "    '''\n",
    "    이쁘게 프린트 해주기\n",
    "    https://wikidocs.net/105471\n",
    "    '''\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(vars(config))\n",
    "print_config(config)\n",
    "\n",
    "\n",
    "loader = DataLoader(\n",
    "    config.train,                           # Train file name except extention, which is language.\n",
    "    config.valid,                           # Validation file name except extension.\n",
    "    (config.lang[:2], config.lang[-2:]),    # Source and target language. // 예) en, ko -> enko로 등록을 해야함.\n",
    "    batch_size=config.batch_size,\n",
    "    device=-1,                              # Lazy loading\n",
    "    max_length=config.max_length,           # Loger sequence will be excluded.\n",
    "    dsl=False,                              # Turn-off Dual-supervised Learning mode.\n",
    ")\n",
    "\n",
    "input_size, output_size = len(loader.src.vocab), len(loader.tgt.vocab)\n",
    "    # 이게 어떻게 작동하는거지??? loader는 Generator인데 어떻게 src에서 vocab을 가져오지?\n",
    "    # input언어의 vocab size, output언어의 vocab size\n",
    "model = get_model(input_size, output_size, config)\n",
    "crit = get_crit(output_size, data_loader.PAD)\n",
    "\n",
    "\n",
    "if model_weight is not None:\n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "# Pass models to GPU device if it is necessary.\n",
    "if config.gpu_id >= 0:\n",
    "    model.cuda(config.gpu_id)\n",
    "    crit.cuda(config.gpu_id)\n",
    "\n",
    "optimizer = get_optimizer(model, config)\n",
    "\n",
    "if opt_weight is not None and (config.use_adam or config.use_radam):\n",
    "    optimizer.load_state_dict(opt_weight)\n",
    "\n",
    "lr_scheduler = get_scheduler(optimizer, config)\n",
    "\n",
    "if config.verbose >= 2:\n",
    "    print(model)\n",
    "    print(crit)\n",
    "    print(optimizer)\n",
    "\n",
    "# Start training. This function maybe equivalant to 'fit' function in Keras.\n",
    "mle_trainer = SingleTrainer(MaximumLikelihoodEstimationEngine, config)\n",
    "mle_trainer.train(\n",
    "    model,\n",
    "    crit,\n",
    "    optimizer,\n",
    "    train_loader=loader.train_iter,\n",
    "    valid_loader=loader.valid_iter,\n",
    "    src_vocab=loader.src.vocab,\n",
    "    tgt_vocab=loader.tgt.vocab,\n",
    "    n_epochs=config.n_epochs,\n",
    "    lr_scheduler=lr_scheduler,\n",
    ")\n",
    "\n",
    "if config.rl_n_epochs > 0:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.rl_lr)\n",
    "    mrt_trainer = SingleTrainer(MinimumRiskTrainingEngine, config)\n",
    "\n",
    "    mrt_trainer.train(\n",
    "        model,\n",
    "        None, # We don't need criterion for MRT.\n",
    "        optimizer,\n",
    "        train_loader=loader.train_iter,\n",
    "        valid_loader=loader.valid_iter,\n",
    "        src_vocab=loader.src.vocab,\n",
    "        tgt_vocab=loader.tgt.vocab,\n",
    "        n_epochs=config.rl_n_epochs,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "# 이런식으로 정의하나보다.. 배웠다.\n",
    "config = define_argparser()\n",
    "main(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf8245-7890-484a-9754-3771095c6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(\n",
    "#                 max(0, 10 - 1),\n",
    "                9,\n",
    "                (1 - 1) + 10,\n",
    "                \n",
    "            )]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
