{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3bd2a5-3037-4643-8dda-0b95eda15d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have no idea where this fuck came from'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "\n",
    "'''i have no idea where this fuck came from'''\n",
    "# import torch_optimizer as custom_optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8d473-29f0-40d2-a049-2ced6e544d3f",
   "metadata": {},
   "source": [
    "# 1. train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0007703c-b8d0-47ca-b986-0091493aeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = argparse.ArgumentParser()\n",
    "args = p.parse_args(\"\")\n",
    "\n",
    "\n",
    "args.is_continue = False\n",
    "args.model_fn = '.'\n",
    "args.train = '/Users/rainism/desktop/grad/torch_study/transformer/data/tt.corpus.shuf.train.tok.bpe'\n",
    "args.valid = '/Users/rainism/desktop/grad/torch_study/transformer/data/tt.corpus.shuf.train.tok.bpe'\n",
    "args.lang = 'enko'\n",
    "args.gpu_id = -1\n",
    "args.store_true = False\n",
    "args.batch_size = 160\n",
    "args.n_epochs = 30\n",
    "args.verbose = 1\n",
    "args.init_epoch = 1\n",
    "args.max_length = 100\n",
    "args.dropout = .2\n",
    "args.word_vec_size = 512 # word vec size\n",
    "args.hidden_size = 768\n",
    "args.n_layers = 4\n",
    "args.max_grad_norm = 5. # 'Threshold for gradient clipping. Default=%(default)s'\n",
    "args.iteration_per_update = 1 # 'Number of feed-forward iterations for one parameter update. Default=%(default)s\n",
    "args.lr = 1.\n",
    "args.lr_step = 1\n",
    "args.lr_gamma = .5\n",
    "args.lr_decay_start = 10\n",
    "args.use_adam = True\n",
    "args.rl_lr = .01\n",
    "args.rl_n_samples = 1\n",
    "args.rl_n_epochs = 10\n",
    "args.rl_n_gram = 6\n",
    "args.rl_reward = 'gleu'\n",
    "args.use_transformer = False\n",
    "args.n_splits = 8\n",
    "args.off_autocast = False\n",
    "\n",
    "config = args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faecc11-dfc9-4eb1-b76a-bcb3c56d1b2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d20af772-8fe0-4e72-8553-37892428084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchtext\n",
    "version = list(map(int, torchtext.__version__.split('.')))\n",
    "if version[0] <= 0 and version[1] < 9:\n",
    "    from torchtext import data, datasets\n",
    "else:\n",
    "    from torchtext.legacy import data, datasets\n",
    "\n",
    "PAD, BOS, EOS = 1, 2, 3\n",
    "# PAD의 번호는 1, BOS는 2, EOS는 3인가보네\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_fn=None,\n",
    "                 valid_fn=None,\n",
    "                 exts=None,\n",
    "                 batch_size=64,\n",
    "                 device='cpu',\n",
    "                 max_vocab=99999999,\n",
    "                 max_length=255,\n",
    "                 fix_length=None,\n",
    "                 use_bos=True,\n",
    "                 use_eos=True,\n",
    "                 shuffle=True,\n",
    "                 dsl=False\n",
    "                 ):\n",
    "\n",
    "        super(DataLoader, self).__init__() # ??? 상속받을게 없는데?\n",
    "\n",
    "        # Field -> fields -> TabularDataset -> build_vocab -> Bucket\n",
    "\n",
    "        # src와 tgt가 각각 있는 이유는, 파일이 각각 있었기 때문이다.\n",
    "            # torchtext.data.Field\n",
    "        self.src = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=True,\n",
    "            fix_length=fix_length, # None\n",
    "            init_token='<BOS>' if dsl else None, # dsl : dure learning할때 필요한것. 지금은 None이라고 보면 됨.\n",
    "            eos_token='<EOS>' if dsl else None,\n",
    "        )\n",
    "\n",
    "        self.tgt = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=True,\n",
    "            fix_length=fix_length,\n",
    "            init_token='<BOS>' if use_bos else None, # True .. learning에서는 필요 없고, 생성자 할때만 필요함(?)\n",
    "            eos_token='<EOS>' if use_eos else None,\n",
    "        )\n",
    "\n",
    "        if train_fn is not None and valid_fn is not None and exts is not None:\n",
    "            # TranslationDataset는 밑에 정의 되어있습니다.\n",
    "            train = TranslationDataset(\n",
    "                path=train_fn, # train file path\n",
    "                exts=exts, # en,ko path가 튜플로 들어가 있음.\n",
    "                fields=[('src', self.src), ('tgt', self.tgt)], # 사용할 필드 명\n",
    "                max_length=max_length\n",
    "            )\n",
    "            valid = TranslationDataset(\n",
    "                path=valid_fn,\n",
    "                exts=exts,\n",
    "                fields=[('src', self.src), ('tgt', self.tgt)],\n",
    "                max_length=max_length,\n",
    "            )\n",
    "\n",
    "            # bucketIterator가 하는 일을 실제 데이터를 가지고 와서. -> pad까지 체운 형태로 만들고\n",
    "            # 미니배치 단위로 만들어주는 역할을 한다.\n",
    "            # https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator\n",
    "            self.train_iter = data.BucketIterator(\n",
    "                train,\n",
    "                batch_size=batch_size,\n",
    "                device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "                shuffle=shuffle,\n",
    "                sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)), # ?????????????? what's x?\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "\n",
    "            self.valid_iter = data.BucketIterator(\n",
    "                valid,\n",
    "                batch_size=batch_size,\n",
    "                device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "                shuffle=False,\n",
    "                sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)),\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "\n",
    "            self.src.build_vocab(train, max_size=max_vocab)\n",
    "            self.tgt.build_vocab(train, max_size=max_vocab)\n",
    "\n",
    "    def load_vocab(self, src_vocab, tgt_vocab):\n",
    "        self.src.vocab = src_vocab\n",
    "        self.tgt.vocab = tgt_vocab\n",
    "\n",
    "\n",
    "# torchtext에는 maxlen을 잘라주는게 없어서 customizing했어.\n",
    "class TranslationDataset(data.Dataset):\n",
    "    \"\"\"Defines a dataset for machine translation.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "\n",
    "    def __init__(self, path, exts, fields, max_length=None, **kwargs):\n",
    "        \"\"\"Create a TranslationDataset given paths and fields.\n",
    "\n",
    "        Arguments:\n",
    "            path: Common prefix of paths to the data files for both languages.\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            # fields가 [('src',src),('tgt',tgt)]형태가 아닐때 다시 정의를 함.\n",
    "            fields = [('src', fields[0]), ('trg', fields[1])]\n",
    "\n",
    "        if not path.endswith('.'):\n",
    "            # 주소의 끝에 .이 없다면 추가로 넣어줘.\n",
    "            path += '.'\n",
    "\n",
    "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
    "\n",
    "        examples = []\n",
    "        with open(src_path, encoding='utf-8') as src_file, open(trg_path, encoding='utf-8') as trg_file:\n",
    "            # src, trg path에서 파일을 불러오고 한줄씩 for문\n",
    "            for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip() # 오른쪽끝 스페이스 제거.\n",
    "                # max_length가 있을 경우에는 작업을 해줌.\n",
    "                if max_length and max_length < max(len(src_line.split()), len(trg_line.split())): \n",
    "                    # 스페이스를 띄어쓰기라고 가정, max_len보다 클때(?) 이부분 잘못된거 같은데...\n",
    "                    '''?????????????????????????????????????????'''\n",
    "                    continue\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    # 별일 없을때 examples에 데이터를 추가.\n",
    "                    examples += [data.Example.fromlist([src_line, trg_line], fields)]\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "\n",
    "loader = DataLoader(\n",
    "    config.train,                           # Train file name except extention, which is language.\n",
    "    config.valid,                           # Validation file name except extension.\n",
    "    (config.lang[:2], config.lang[-2:]),    # Source and target language. // 예) en, ko -> enko로 등록을 해야함.\n",
    "    batch_size=config.batch_size,\n",
    "    device=-1,                              # Lazy loading\n",
    "    max_length=config.max_length,           # Loger sequence will be excluded.\n",
    "    dsl=False,                              # Turn-off Dual-supervised Learning mode.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565e931-17cf-4c36-86be-63a14eaded37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3.Back to train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b72cf28e-e8fc-4d3c-ae6f-611710cfefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, output_size = len(loader.src.vocab), len(loader.tgt.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d79dca0-99d0-46ea-93eb-87513bfd4eec",
   "metadata": {},
   "source": [
    "## 3-1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21bc06e9-b716-4162-936b-1d63ba03c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False) # 맨처음에 projection needed for 가중치 refer to encoder part\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, h_src, h_t_tgt, mask=None):\n",
    "        # |h_src| = (batch_size, length, hidden_size) - 인코더의 모든 히든 스테잇\n",
    "        # |h_t_tgt| = (batch_size, 1, hidden_size) - 디코더의 히든 스테잇\n",
    "        # |mask| = (batch_size, length) - src의 마스킹할 정보\n",
    "\n",
    "        query = self.linear(h_t_tgt)                     # [B,1,H] * [B,H,H] = [B,1,H]\n",
    "            # |query| = (batch_size, 1, hidden_size)\n",
    "\n",
    "        weight = torch.bmm(query, h_src.transpose(1, 2)) # [B,1,H] * [B, H, L] => [B, 1, L] // bmm : batch multiplication\n",
    "            # |weight| = (batch_size, 1, length)\n",
    "            \n",
    "        if mask is not None:\n",
    "            weight.masked_fill_(mask.unsqueeze(1), -float('inf')) # mask가 있는 부분에 -float('inf')를 넣어줘\n",
    "        weight = self.softmax(weight)\n",
    "\n",
    "        context_vector = torch.bmm(weight, h_src)        # [B,1,L]*[B,L,H] -> [B,1,H]\n",
    "        # |context_vector| = (batch_size, 1, hidden_size)\n",
    "        # 해석으 해보면, 샘플 데이터에서, 디코더의 시점에서, 어텐션을 적용한 컨텐스트 벡터\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            word_vec_size, # input shape\n",
    "            int(hidden_size / 2), # bidirectional 할 것이기 때문에, 나누기 2를 했다. -> 만약 소수점이 되버리면?\n",
    "            num_layers=n_layers, # stacking LSTM\n",
    "            dropout=dropout_p,\n",
    "            bidirectional=True,\n",
    "            batch_first=True, # batch의 쉐입이 첫번째가 아니라서 앞으로 오게 강제함\n",
    "        )\n",
    "\n",
    "    def forward(self, emb):\n",
    "        # |emb| = (batch_size, length, word_vec_size)\n",
    "\n",
    "        if isinstance(emb, tuple): # 임베딩 타입이 튜플이니? \n",
    "            x, lengths = emb\n",
    "            x = pack(x, lengths.tolist(), batch_first=True) # https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html\n",
    "        else:\n",
    "            x = emb\n",
    "\n",
    "        y, h = self.rnn(x)\n",
    "\n",
    "        if isinstance(emb, tuple):\n",
    "            y, _ = unpack(y, batch_first=True) # 위에 packedsequence가 들어가있으면 풀어줘야 하기 때문에 씀.\n",
    "        \n",
    "        # y : [b, n, h]\n",
    "        # h : [l*2, b, h/2], [l*2, b, h/2]\n",
    "        return y, h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Be aware of value of 'batch_first' parameter and 'bidirectional' parameter.\n",
    "        self.rnn = nn.LSTM(\n",
    "            word_vec_size + hidden_size, # input feeding? 을 해줄거기 때문에(concat) 차원이 늘어난다.\n",
    "            hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout_p,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, emb_t, h_t_1_tilde, h_t_1):\n",
    "        '''\n",
    "        추론할때나, input feeding을 해줄것이기 때문에, 한스텝씩 들어올거야.\n",
    "        h_t_1_tilde : 저번에 예측한 hidden의 정보값. before softmax\n",
    "        h_t_1 : h_{t-1} = [h_{t-1}, c_{t-1}]   tuple임. // 전 스텝의 hidden값. //  [n layer, b, h]라는데(?)\n",
    "        \n",
    "        # |emb_t| = (b, 1, word_vec_size)\n",
    "        # |h_t_1_tilde| = (b, 1, h)\n",
    "        # |h_t_1| = [(n_l, b, h),(n_l, b, h)] : t-1 시점 전의 모든 히든들..같음 not sure\n",
    "        '''\n",
    "        batch_size = emb_t.size(0) # [batch]\n",
    "        hidden_size = h_t_1[0].size(-1) # [hidden]\n",
    "\n",
    "        if h_t_1_tilde is None:\n",
    "            # If this is the first time-step, 이제 막 디코더가 시작한것임.\n",
    "            h_t_1_tilde = emb_t.new(batch_size, 1, hidden_size).zero_() # .new -> 텐서는 디바이스와, 타입이 같아야 arithmetic이 가능한데,.. 그러면 두번을 설정해 줘야함. 귀찮자나..\n",
    "                                                                                    # 가장 간단하게 하는 방법이. 저 텐서와 같은 디바이스, 타입인놈을 만들어줘. 하는게 new이다.\n",
    "                                                                        # .zero_() -> inplace 연산이다.\n",
    "\n",
    "        # Input feeding trick.\n",
    "        x = torch.cat([emb_t, h_t_1_tilde], dim=-1) # [b, 1, w + h]\n",
    "\n",
    "        # Unlike encoder, decoder must take an input for sequentially.\n",
    "        y, h = self.rnn(x, h_t_1) # h_t_1 : [(n_l, b, h), (n_l, b, h)] : 이전 시점의 hidden, context tensors, it is 0 when it's not provided.\n",
    "            # y : [b, 1, h] // h: [l, b, h],[l,b,h]\n",
    "            # |decoder_output| = (b, 1, h)\n",
    "            # |decoder_hidden| = (n, b, h), (n,b,h)\n",
    "        return y, h\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.output = nn.Linear(hidden_size, output_size) # output_size : word vec size\n",
    "        self.softmax = nn.LogSoftmax(dim=-1) # logSoftmax를 함. (왜?)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, length, hidden_size) : 학습할때는 length길이 만큼 한번에 들어감. 왜냐하면 teacher forcing이니까.\n",
    "\n",
    "        y = self.softmax(self.output(x)) # linear에 한번 통과한다. 그러면 사이즈가 word sz로 바뀜.\n",
    "        # |y| = (batch_size, length, output_size)\n",
    "\n",
    "        # Return log-probability instead of just probability. : 미니배치, 각 샘플별, 각 단어별, 로그 확률값이 리턴이됨.\n",
    "        return y\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        word_vec_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        n_layers=4,\n",
    "        dropout_p=.2\n",
    "    ):\n",
    "\n",
    "        '''\n",
    "        input_size : input언어의 vocab size\n",
    "        word_vec_size : embed size\n",
    "        hidden_size : hidden sz\n",
    "        output_size : target언어의 vocab size\n",
    "        '''\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.word_vec_size = word_vec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "\n",
    "        # 임베드 정의\n",
    "        self.emb_src = nn.Embedding(input_size, word_vec_size)\n",
    "        self.emb_dec = nn.Embedding(output_size, word_vec_size)\n",
    "\n",
    "        # \n",
    "        self.encoder = Encoder(\n",
    "            word_vec_size, hidden_size,\n",
    "            n_layers=n_layers, dropout_p=dropout_p,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            word_vec_size, hidden_size,\n",
    "            n_layers=n_layers, dropout_p=dropout_p,\n",
    "        )\n",
    "        self.attn = Attention(hidden_size)\n",
    "\n",
    "        # attn에서 나온 context vec와 // decoder의 output하고 -> h_tilde\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.tanh = nn.Tanh() # 위 concat에 씌어줄 activation fn\n",
    "        self.generator = Generator(hidden_size, output_size)\n",
    "\n",
    "    def generate_mask(self, x, length):\n",
    "        '''\n",
    "        x : [bs, n]\n",
    "        length : [bs,] such as [4,3,1]\n",
    "        '''\n",
    "        mask = []\n",
    "\n",
    "        max_length = max(length)\n",
    "        for l in length:\n",
    "            if max_length - l > 0:\n",
    "                # If the length is shorter than maximum length among samples, \n",
    "                # set last few values to be 1s to remove attention weight.\n",
    "                mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
    "                                    x.new_ones(1, (max_length - l))\n",
    "                                    ], dim=-1)]\n",
    "            else:\n",
    "                # If the length of the sample equals to maximum length among samples, \n",
    "                # set every value in mask to be 0.\n",
    "                mask += [x.new_ones(1, l).zero_()]\n",
    "\n",
    "        mask = torch.cat(mask, dim=0).bool() # [[4,4], [4,4], [4,4]] -> [3, 4]짜리 텐서로 flatten\n",
    "\n",
    "        '''\n",
    "            length 에) 아래와 같은 텐서가 있을때 \n",
    "\n",
    "            --- --- --- ---\n",
    "            |  |   |   |  |  [4,\n",
    "            ___ ___ ___ ___\n",
    "            |  |   |   ||||   3,\n",
    "            --- --- --- ---\n",
    "            |   ||| ||| |||   1] 라는 x_length모양이 있을것임.\n",
    "            --- --- --- ---\n",
    "\n",
    "            --- --- --- ---\n",
    "            | 0|  0|  0| 0|  \n",
    "            ___ ___ ___ ___\n",
    "            | 0|  0|  0| 1|  \n",
    "            --- --- --- ---\n",
    "            | 0| 1| | 1| 1|   \n",
    "            --- --- --- ---\n",
    "            으로 나오게 한다.\n",
    "        '''\n",
    "        return mask\n",
    "\n",
    "\n",
    "\n",
    "    def merge_encoder_hiddens(self, encoder_hiddens):\n",
    "\n",
    "        '''\n",
    "        for loop을 하여 속도가 안좋음.\n",
    "        '''\n",
    "        new_hiddens = []\n",
    "        new_cells = []\n",
    "\n",
    "        hiddens, cells = encoder_hiddens\n",
    "            # encoder_hiddens는 hiddens와 cell_state두개를 갖고 있음.\n",
    "            # hiddens : [2*layers, batch, hidden/2]\n",
    "\n",
    "        # i-th and (i+1)-th layer is opposite direction.\n",
    "        # Also, each direction of layer is half hidden size.\n",
    "        # Therefore, we concatenate both directions to 1 hidden size layer.\n",
    "        for i in range(0, hiddens.size(0), 2): # 0~2*layers만큼 for문을 돌림.\n",
    "            new_hiddens += [torch.cat([hiddens[i], hiddens[i + 1]], dim=-1)] # 0,1 // 2,3 // 이런식으로 묶어서 넣어줌.\n",
    "                # new_hiddens : [bs, hs/2*2] -> [bs, hs]\n",
    "            new_cells += [torch.cat([cells[i], cells[i + 1]], dim=-1)]\n",
    "\n",
    "        new_hiddens, new_cells = torch.stack(new_hiddens), torch.stack(new_cells)\n",
    "                # new_hiddens : [layers, bs, hs]\n",
    "                # new_cells : [layers, bs, hs]\n",
    "        return (new_hiddens, new_cells)\n",
    "\n",
    "\n",
    "    def fast_merge_encoder_hiddens(self, encoder_hiddens):\n",
    "        '''\n",
    "        parallel하게 해보자\n",
    "        encoder : [l*2, b, h/2], [l*2, b, h/2]\n",
    "        '''\n",
    "        # Merge bidirectional to uni-directional\n",
    "        # (layers*2, bs, hs/2) -> (layers, bs, hs).\n",
    "        # Thus, the converting operation will not working with just 'view' method.\n",
    "        h_0_tgt, c_0_tgt = encoder_hiddens # 두개 모두 [2layer, b, h/2]\n",
    "        batch_size = h_0_tgt.size(1)\n",
    "\n",
    "        # contiguous : 메모리상에 잘 붙어있게 선언하는것.\n",
    "        # transpose까지 하면 : [b, 2layer, h/2]\n",
    "        # view : [b, -1, hs] --> [b, layer, h]\n",
    "        # transpose : [layer, b, h]\n",
    "        h_0_tgt = h_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n",
    "                                                            -1,\n",
    "                                                            self.hidden_size\n",
    "                                                            ).transpose(0, 1).contiguous()\n",
    "        c_0_tgt = c_0_tgt.transpose(0, 1).contiguous().view(batch_size,\n",
    "                                                            -1,\n",
    "                                                            self.hidden_size\n",
    "                                                            ).transpose(0, 1).contiguous()\n",
    "        # You can use 'merge_encoder_hiddens' method, instead of using above 3 lines.\n",
    "        # 'merge_encoder_hiddens' method works with non-parallel way.\n",
    "        # h_0_tgt = self.merge_encoder_hiddens(h_0_tgt)\n",
    "\n",
    "        # |h_src| = (batch_size, length, hidden_size)\n",
    "        # |h_0_tgt| = (n_layers, batch_size, hidden_size)\n",
    "        # [l, b, h], [l, b, h]\n",
    "        return h_0_tgt, c_0_tgt\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        '''\n",
    "        학습할때는 teacher forcing을 할 것임.\n",
    "\n",
    "        src : input sentence = [bs, n, V_i]\n",
    "        tgt : target sentence = [bs, m, V_t]\n",
    "        '''\n",
    "        # output = [bs, m, V_t]\n",
    "\n",
    "        batch_size = tgt.size(0)\n",
    "\n",
    "        mask = None\n",
    "        x_length = None\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            '''\n",
    "            x_length에서 마스크 정보가 주어지면 generate_mask를 하라고 했음.\n",
    "            '''\n",
    "            # Based on the length information, gererate mask to prevent that\n",
    "            # shorter sample has wasted attention.\n",
    "            mask = self.generate_mask(x, x_length) \n",
    "            # //x : [bs, n] // x_length : [bs,] // mask : [bs, n]\n",
    "            # |mask| = (batch_size, length)\n",
    "            '''\n",
    "            length 에) 아래와 같은 텐서가 있을때 \n",
    "\n",
    "            --- --- --- ---\n",
    "            |  |   |   |  |  [4,\n",
    "            ___ ___ ___ ___\n",
    "            |  |   |   ||||   3,\n",
    "            --- --- --- ---\n",
    "            |   ||| ||| |||   1] 라는 x_length모양이 있을것임.\n",
    "            --- --- --- ---\n",
    "            \n",
    "            즉 [4,3,1]이 들어가 있음. 여기서 배치사이즈는 3임.\n",
    "            '''\n",
    "\n",
    "        else:\n",
    "            x = src\n",
    "\n",
    "        if isinstance(tgt, tuple):\n",
    "            tgt = tgt[0]\n",
    "\n",
    "\n",
    "        # Get word embedding vectors for every time-step of input sentence.\n",
    "        emb_src = self.emb_src(x) # |emb_src| = (b, n, emb)\n",
    "\n",
    "        # The last hidden state of the encoder would be a initial hidden state of decoder.\n",
    "        h_src, h_0_tgt = self.encoder((emb_src, x_length)) # packed_padded_sequence로 처리를 함.\n",
    "            # |h_src| = (b, n, h) : 인코더의 모든 t시점에서의 히든스테이트\n",
    "            # |h_0_tgt| = [l*2, b, h/2], [l*2, b, h/2] : 인코더에서 레이어마다 나온 마지막 히든스테이트(컨텍스트)\n",
    "                # -> 여기서 이친구를 decoder의 init hidden으로 넣어줘야 하는데,feature가 h/2임. 이걸 h로 변환해줘야함.\n",
    "\n",
    "        h_0_tgt = self.fast_merge_encoder_hiddens(h_0_tgt)\n",
    "            # merge_encoder_hidden부터 살펴보자\n",
    "            # [l, b, h], [l, b, h]\n",
    "\n",
    "        # teacher forcing이기 때문에 정답을 한꺼번에 만들어.\n",
    "        emb_tgt = self.emb_dec(tgt)\n",
    "            # |emb_tgt| = (b, l, emb)\n",
    "        h_tilde = [] # 여기도 한방에 들어갈거야.\n",
    "\n",
    "        h_t_tilde = None # 첫번째 타임스텝에서는 전에 있던 h_t_tilde는 없다.\n",
    "        decoder_hidden = h_0_tgt # ([layer, bs, hs], [layer, bs, hs])\n",
    "\n",
    "        # Run decoder until the end of the time-step.\n",
    "        for t in range(tgt.size(1)): # length of sentence\n",
    "            # Teacher Forcing: take each input from training set,\n",
    "            # not from the last time-step's output.\n",
    "            # Because of Teacher Forcing,\n",
    "            # training procedure and inference procedure becomes different.\n",
    "            # Of course, because of sequential running in decoder,\n",
    "            # this causes severe bottle-neck.\n",
    "            emb_t = emb_tgt[:, t, :].unsqueeze(1) # 한 단어씩 번갈아가면서 들어간다. // unsqueeze : 특정 차원에 차원을 추가한다.\n",
    "                # 인덱싱할 경우 [b, l, emb] -> [b,emb]되버릴 수 있다. 따라서 명시적으로 그냥 선언하자.\n",
    "            # |emb_t| = (batch_size, 1, word_vec_size)\n",
    "            # |h_t_tilde| = (batch_size, 1, hidden_size)\n",
    "\n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t, # 현시점의 단어.\n",
    "                                                          h_t_tilde, # 지난 타임 스텝의 틸다\n",
    "                                                          decoder_hidden # [l, b, h], [l, b, h]\n",
    "                                                          )\n",
    "            # |decoder_output| = (b, 1, h)\n",
    "            # |decoder_hidden| = (n, b, h), (n,b,h)\n",
    "\n",
    "\n",
    "            context_vector = self.attn(h_src, decoder_output, mask)\n",
    "            # |context_vector| = (batch_size, 1, hidden_size)\n",
    "\n",
    "            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n",
    "                                                         context_vector\n",
    "                                                         ], dim=-1)))\n",
    "            # |h_t_tilde| = (batch_size, 1, hidden_size)\n",
    "            # self.concat -> 2h, h\n",
    "\n",
    "            h_tilde += [h_t_tilde]\n",
    "\n",
    "        h_tilde = torch.cat(h_tilde, dim=1)\n",
    "            # h_tilde = (b, 1, h)\n",
    "            # concat on dim 1 => (b, m, h)\n",
    "            # |h_tilde| = (b, length, h)\n",
    "\n",
    "        y_hat = self.generator(h_tilde)\n",
    "        # |y_hat| = (b, length, output_size:vocab_size)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_model(input_size, output_size, config):\n",
    "    if config.use_transformer:\n",
    "        model = Transformer(\n",
    "            input_size,                     # Source vocabulary size\n",
    "            config.hidden_size,             # Transformer doesn't need word_vec_size.\n",
    "            output_size,                    # Target vocabulary size\n",
    "            n_splits=config.n_splits,       # Number of head in Multi-head Attention.\n",
    "            n_enc_blocks=config.n_layers,   # Number of encoder blocks\n",
    "            n_dec_blocks=config.n_layers,   # Number of decoder blocks\n",
    "            dropout_p=config.dropout,       # Dropout rate on each block\n",
    "        )\n",
    "    else:\n",
    "        model = Seq2Seq(\n",
    "            input_size,\n",
    "            config.word_vec_size,           # Word embedding vector size\n",
    "            config.hidden_size,             # LSTM's hidden vector size\n",
    "            output_size,\n",
    "            n_layers=config.n_layers,       # number of layers in LSTM\n",
    "            dropout_p=config.dropout        # dropout-rate in LSTM\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model(input_size, output_size, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabf6d7-9245-41f1-a26c-2fe115c709fd",
   "metadata": {},
   "source": [
    "## 3-2 criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1487df-eee3-4eae-94a7-bd983318232d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6f99a368-1880-4b38-9b61-54e7e6c8abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crit(output_size, pad_index):\n",
    "    loss_weight = torch.ones(output_size)\n",
    "        # 패딩이 되어있는곳은 맞춰도 점수를 주고 싶지 않은것.\n",
    "\n",
    "    loss_weight[pad_index] = 0.\n",
    "        # 패드인덱스를 받아와서 거기다가 0을 할당해서, 맞춰도 점수를 주지마.\n",
    "    '''???????????????????????????????????????????????????????????'''\n",
    "        # pad_index를 가져와야하는데, main함수에서 보면 data_loader의 PAD ; 1을 가져왔으. 왜?\n",
    "    # Instead of using Cross-Entropy loss,\n",
    "    # we can use Negative Log-Likelihood(NLL) loss with log-probability.\n",
    "    crit = nn.NLLLoss(\n",
    "        weight=loss_weight,\n",
    "        reduction='sum'\n",
    "    )\n",
    "\n",
    "    return crit\n",
    "\n",
    "\n",
    "crit = get_crit(output_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed481b-b9ea-407f-8c30-c87e773cf041",
   "metadata": {},
   "source": [
    "## 3-3 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c38a7be-f935-40cb-9b1f-dadb79cad2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, config):\n",
    "    if config.use_adam:\n",
    "        if config.use_transformer:\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config.lr, betas=(.9, .98))\n",
    "        else: # case of rnn based seq2seq.\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    elif config.use_radam:\n",
    "        optimizer = custom_optim.RAdam(model.parameters(), lr=config.lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config.lr) # initial lr = 1, 그리고 그래디언트가 폭주할거 같으면 클립핑을 함(?)-> 요놈은 한번도 안해봄.\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "optimizer = get_optimizer(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0ef54-879b-4991-8c85-343b483ddbd5",
   "metadata": {},
   "source": [
    "## 3-4 lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "64ee7cfe-8753-401a-8088-3968b7be728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, config):\n",
    "    '''Adam 같은 경우는 쓸 필요가 없고, SGD같은 경우 써야한데'''\n",
    "    '''\n",
    "         LR |\n",
    "          1 |-----------------\n",
    "            |\n",
    "            |                  --\n",
    "            |                      -- ... 10번부터는 lr을 반씩 줄여가면서 진행\n",
    "            |_____________________________________\n",
    "               1  2  3  4 ... 9 10 11 12         epochs\n",
    "    \n",
    "    '''\n",
    "    if config.lr_step > 0:\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                        milestones=[i for i in range(\n",
    "                                                        max(0, config.lr_decay_start - 1), # 내가 원하는 시작 에폭부터\n",
    "                                                        (config.init_epoch - 1) + config.n_epochs, # 내가 원하는 끝나는 에폭까지\n",
    "                                                        config.lr_step # 1\n",
    "                                                        )],\n",
    "                                                        gamma=config.lr_gamma, # 0.5씩 작아짐.\n",
    "                                                        last_epoch=config.init_epoch - 1 if config.init_epoch > 1 else -1,\n",
    "                                                        )\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "\n",
    "    return lr_scheduler\n",
    "\n",
    "\n",
    "lr_scheduler = get_scheduler(optimizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "84208c3c-8622-4a46-ae87-490370c0811e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(max(0, config.lr_decay_start - 1), # 내가 원하는 시작 에폭부터\n",
    "                (config.init_epoch - 1) + config.n_epochs, # 내가 원하는 끝나는 에폭까지\n",
    "                config.lr_step # 1\n",
    "                )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc042695-e27c-4699-a706-e0e51562c85b",
   "metadata": {},
   "source": [
    "# 4. trainer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "443b7ac2-271b-4404-bde9-4a25ae113cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.utils as torch_utils\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "\n",
    "# Core of the library, contains an engine for training and evaluating, most of the classic machine learning metrics \n",
    "# and a variety of handlers to ease the pain of training and validation of neural networks.\n",
    "\n",
    "from ignite.engine import Engine # Runs a given process_function over each batch of a dataset, emitting events as it goes.\n",
    "from ignite.engine import Events # Events that are fired by the Engine during execution.\n",
    "from ignite.metrics import RunningAverage # Compute running average of a metric or the output of process function.\n",
    "from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
    "\n",
    "from decompress.simple_nmt.utils import get_grad_norm, get_parameter_norm\n",
    "\n",
    "\n",
    "VERBOSE_SILENT = 0\n",
    "VERBOSE_EPOCH_WISE = 1\n",
    "VERBOSE_BATCH_WISE = 2\n",
    "\n",
    "# Gradient init -> FeedForward -> Loss -> back prop -> Gradient descent -> 현제상태 출력 // 을 만들거야. \n",
    "# Train, Valid engine두개를 선언하고\n",
    "# 그 두개를 물고 있는 엔진을 하나 또 설정해\n",
    "class MaximumLikelihoodEstimationEngine(Engine):\n",
    "\n",
    "    def __init__(self, func, model, crit, optimizer, lr_scheduler, config):\n",
    "        self.model = model\n",
    "        self.crit = crit # criterion (loss)\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.config = config\n",
    "\n",
    "        super().__init__(func) # 여기서 func은 MaximumLikelihoodEstimationEngine.train임.\n",
    "        # func을 Engine class에 보내 실행하기.. 근데 그럼 어떤 효과가 잇는거지?\n",
    "        '''https://pytorch.org/ignite/_modules/ignite/engine/engine.html#Engine'''\n",
    "        ''' 이해 안되면 이거 해보기.\n",
    "        class adding():\n",
    "            def __init__(self, a,b):\n",
    "                print(a+b)\n",
    "                return a+b\n",
    "            \n",
    "        class bdding(adding):\n",
    "            def __init__(self):\n",
    "                z = 1\n",
    "                super(bdding, self).__init__(1,3)\n",
    "\n",
    "        bdding()\n",
    "        '''    \n",
    "        \n",
    "\n",
    "        self.best_loss = np.inf\n",
    "        self.scaler = GradScaler() \n",
    "\n",
    "\n",
    "    # static : https://wikidocs.net/21054\n",
    "    @staticmethod\n",
    "    #@profile\n",
    "    def train(engine, mini_batch):\n",
    "        '''\n",
    "            engine : 여기에 Engine을 받나본데? // train engine, valid engine\n",
    "            mini_batch : train_loader가 return하는 객체\n",
    "                pytorch.DataLoader? 이런 비슷한거 있음.\n",
    "        '''\n",
    "        # You have to reset the gradients of all model parameters\n",
    "        # before to take another step in gradient descent.\n",
    "\n",
    "        '''\n",
    "            engine.state.iteration ; Number of iterations the engine has completed. Initialized as 0 and the first iteration is 1.\n",
    "                https://pytorch.org/ignite/v0.4.6/concepts.html#state\n",
    "            engine.iteration_per_update : update당 Iteration 숫자... 먼지 모르겟음.\n",
    "        '''\n",
    "        # accumulate gradient update\n",
    "        engine.model.train() # 파라미터 학습 할 것임.\n",
    "        if engine.state.iteration % engine.config.iteration_per_update == 1 or engine.config.iteration_per_update == 1: # 만약 설정을 config.iteration_per_update 을 1로 했을 경우, 매 iter마다 업데이트, 만약 설정을 10으로 하면 11마다 zero grad함.\n",
    "            if engine.state.iteration > 1:\n",
    "                engine.optimizer.zero_grad()\n",
    "\n",
    "        device = next(engine.model.parameters()).device # 모델이 어느 gpu에 있는지 따오는거\n",
    "        # 현재 모델이 어느 gpu에 있는지 구해서, 미니배치안에 잇는 텐서들을 해당 디바이스에 보내준다.\n",
    "\n",
    "        mini_batch.src = (mini_batch.src[0].to(device), mini_batch.src[1]) # encoder tuple, length // 근데 tuple gpu로 옮기는데 length는 안옮기네\n",
    "        mini_batch.tgt = (mini_batch.tgt[0].to(device), mini_batch.tgt[1]) # decoder x, y\n",
    "            # tuple은 tensor와 length가 있음.\n",
    "\n",
    "            # Raw target variable has both BOS and EOS token. \n",
    "            # The output of sequence-to-sequence does not have BOS token. \n",
    "            # Thus, remove BOS token for reference.\n",
    "        x, y = mini_batch.src, mini_batch.tgt[0][:, 1:]\n",
    "            # |x| = (batch_size, length) : x에서는 tensor, length를 가져가고\n",
    "            # |y| = (batch_size, length) : y(decoder)에서는 length가 빠지고, tensor들만 들어감. \n",
    "            # encoder에는 isinstance라는게 들어가서 tuple인지 확인했었어.\n",
    "            # y의 원래 텐서 모습은 {BOS, y_1, y_2, EOS}이건데, Teacher forcing답안용이기때문에 {y_1, y_2, eos} // 모델에 학습 데이터 들어갈땐 {BOS, y_1, y_2} 들어갈거야.\n",
    "\n",
    "        with autocast(not engine.config.off_autocast):\n",
    "                # Take feed-forward\n",
    "                # Similar as before, the input of decoder does not have EOS token.\n",
    "                # Thus, remove EOS token for decoder input.\n",
    "            y_hat = engine.model(x, mini_batch.tgt[0][:, :-1])  \n",
    "                # 모델에 학습 데이터 들어갈땐 {BOS, y_1, y_2} 들어갈거야.\n",
    "                # |y_hat| = (b, l, |V|) : 각 미니배치별, length별, 단어별 로그 확률 값이 들어가 있음.\n",
    "\n",
    "            loss = engine.crit(\n",
    "                y_hat.contiguous().view(-1, y_hat.size(-1)), # flatten\n",
    "                y.contiguous().view(-1) # flatten\n",
    "            )\n",
    "\n",
    "            backward_target = loss.div(y.size(0)).div(engine.config.iteration_per_update) \n",
    "                # NLL을 보면 -1/n *sigma(sgima)인데, Loss정의시 1/n을 안했음. 그래서 직접 나눠줘야함.\n",
    "                # 그래서 미니 배치 사이즈로 나눠주고, 이터레이션 퍼 업데이트로 나눠줌.\n",
    "\n",
    "        # gpu가 있을때 autocast가 수행\n",
    "        if engine.config.gpu_id >= 0 and not engine.config.off_autocast:\n",
    "            engine.scaler.scale(backward_target).backward()\n",
    "        else:\n",
    "            backward_target.backward()\n",
    "\n",
    "        word_count = int(mini_batch.tgt[1].sum())\n",
    "            # 단어의 갯수를 알아야 로스를 정확하게 구할 수 있음.\n",
    "            # 1 : 에는 각 batch별 length가 들어가 있을 것이기 때문에\n",
    "        p_norm = float(get_parameter_norm(engine.model.parameters())) # parameter norm\n",
    "        g_norm = float(get_grad_norm(engine.model.parameters())) # gradient norm : 안정적일수록 작아짐.\n",
    "        '''\n",
    "                @torch.no_grad()\n",
    "                def get_grad_norm(parameters, norm_type=2):\n",
    "                    parameters = list(filter(lambda p: p.grad is not None, parameters)) # 파라미터의수\n",
    "\n",
    "                    total_norm = 0\n",
    "\n",
    "                    try:\n",
    "                        for p in parameters:\n",
    "                            total_norm += (p.grad.data**norm_type).sum() # ||L2||_norm\n",
    "                        total_norm = total_norm ** (1. / norm_type) # \n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                    return total_norm\n",
    "\n",
    "\n",
    "                @torch.no_grad()\n",
    "                def get_parameter_norm(parameters, norm_type=2):\n",
    "                    total_norm = 0\n",
    "\n",
    "                    try:\n",
    "                        for p in parameters:\n",
    "                            total_norm += (p.data**norm_type).sum()\n",
    "                        total_norm = total_norm ** (1. / norm_type)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                    return total_norm\n",
    "        \n",
    "        '''\n",
    "\n",
    "\n",
    "        if engine.state.iteration % engine.config.iteration_per_update == 0 and \\\n",
    "            engine.state.iteration > 0:\n",
    "                # In order to avoid gradient exploding, we apply gradient clipping.\n",
    "            torch_utils.clip_grad_norm_(\n",
    "                engine.model.parameters(),\n",
    "                engine.config.max_grad_norm,\n",
    "            )\n",
    "\n",
    "            # Take a step of gradient descent.\n",
    "            # GPU가 있을 경우에, scale을 함.\n",
    "            if engine.config.gpu_id >= 0 and not engine.config.off_autocast:\n",
    "                # Use scaler instead of engine.optimizer.step() if using GPU.\n",
    "                engine.scaler.step(engine.optimizer)\n",
    "                engine.scaler.update()\n",
    "            else:\n",
    "                engine.optimizer.step()\n",
    "\n",
    "            # 만약 lr 스케쥴을 한다그러면..\n",
    "            # if engine.config.use_noam_decay and engine.lr_scheduler is not None:\n",
    "            #     engine.lr_scheduler.step()\n",
    "\n",
    "\n",
    "        loss = float(loss / word_count)\n",
    "            # 단어단 로스를 구할 수 있음.\n",
    "        ppl = np.exp(loss)\n",
    "\n",
    "        return {\n",
    "            'loss': loss, # 나중에 화면에 출력할 것.\n",
    "            'ppl': ppl,\n",
    "            '|param|': p_norm if not np.isnan(p_norm) and not np.isinf(p_norm) else 0., # 학습 초기에 None, inf뜨는 경우가 있었다. 그러면 학습이 안되므로 0을 리턴하도록한다.\n",
    "            '|g_param|': g_norm if not np.isnan(g_norm) and not np.isinf(g_norm) else 0.,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def validate(engine, mini_batch):\n",
    "        engine.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            device = next(engine.model.parameters()).device\n",
    "            mini_batch.src = (mini_batch.src[0].to(device), mini_batch.src[1])\n",
    "            mini_batch.tgt = (mini_batch.tgt[0].to(device), mini_batch.tgt[1])\n",
    "\n",
    "            x, y = mini_batch.src, mini_batch.tgt[0][:, 1:]\n",
    "            # |x| = (batch_size, length)\n",
    "            # |y| = (batch_size, length)\n",
    "\n",
    "            with autocast(not engine.config.off_autocast):\n",
    "                y_hat = engine.model(x, mini_batch.tgt[0][:, :-1])\n",
    "                # |y_hat| = (batch_size, n_classes)\n",
    "                loss = engine.crit(\n",
    "                    y_hat.contiguous().view(-1, y_hat.size(-1)),\n",
    "                    y.contiguous().view(-1),\n",
    "                )\n",
    "        \n",
    "        word_count = int(mini_batch.tgt[1].sum())\n",
    "        loss = float(loss / word_count)\n",
    "        ppl = np.exp(loss)\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'ppl': ppl,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attach(\n",
    "        train_engine, validation_engine,\n",
    "        training_metric_names = ['loss', 'ppl', '|param|', '|g_param|'],\n",
    "        validation_metric_names = ['loss', 'ppl'],\n",
    "        verbose=VERBOSE_BATCH_WISE, # 0,1,2 중 하나.\n",
    "    ):\n",
    "        # Attaching would be repaeted for serveral metrics.\n",
    "        # Thus, we can reduce the repeated codes by using this function.\n",
    "        def attach_running_average(engine, metric_name):\n",
    "            RunningAverage(output_transform=lambda x: x[metric_name]).attach(\n",
    "                engine,\n",
    "                metric_name,\n",
    "            )\n",
    "\n",
    "        for metric_name in training_metric_names:\n",
    "            attach_running_average(train_engine, metric_name)\n",
    "\n",
    "        if verbose >= VERBOSE_BATCH_WISE:\n",
    "            pbar = ProgressBar(bar_format=None, ncols=120) # progressbar만들어서 성과나오고.\n",
    "            pbar.attach(train_engine, training_metric_names)\n",
    "\n",
    "        if verbose >= VERBOSE_EPOCH_WISE:\n",
    "\n",
    "            # 에폭이 끝날때마다 train_engine에서 print안에 있는 친구들을 출력하라고 함.\n",
    "            @train_engine.on(Events.EPOCH_COMPLETED)\n",
    "            def print_train_logs(engine):\n",
    "                avg_p_norm = engine.state.metrics['|param|']\n",
    "                avg_g_norm = engine.state.metrics['|g_param|']\n",
    "                avg_loss = engine.state.metrics['loss']\n",
    "\n",
    "                print('Epoch {} - |param|={:.2e} |g_param|={:.2e} loss={:.4e} ppl={:.2f}'.format(\n",
    "                    engine.state.epoch,\n",
    "                    avg_p_norm,\n",
    "                    avg_g_norm,\n",
    "                    avg_loss,\n",
    "                    np.exp(avg_loss),\n",
    "                ))\n",
    "\n",
    "        # valid에 대해서도 동일하게 진행\n",
    "        for metric_name in validation_metric_names:\n",
    "            attach_running_average(validation_engine, metric_name)\n",
    "\n",
    "        if verbose >= VERBOSE_BATCH_WISE:\n",
    "            pbar = ProgressBar(bar_format=None, ncols=120)\n",
    "            pbar.attach(validation_engine, validation_metric_names)\n",
    "\n",
    "        if verbose >= VERBOSE_EPOCH_WISE:\n",
    "            @validation_engine.on(Events.EPOCH_COMPLETED)\n",
    "            def print_valid_logs(engine):\n",
    "                avg_loss = engine.state.metrics['loss']\n",
    "\n",
    "                print('Validation - loss={:.4e} ppl={:.2f} best_loss={:.4e} best_ppl={:.2f}'.format(\n",
    "                    avg_loss,\n",
    "                    np.exp(avg_loss),\n",
    "                    engine.best_loss,\n",
    "                    np.exp(engine.best_loss),\n",
    "                ))\n",
    "\n",
    "    @staticmethod\n",
    "    def resume_training(engine, resume_epoch):\n",
    "        engine.state.iteration = (resume_epoch - 1) * len(engine.state.dataloader)\n",
    "        engine.state.epoch = (resume_epoch - 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def check_best(engine):\n",
    "        loss = float(engine.state.metrics['loss'])\n",
    "        if loss <= engine.best_loss:\n",
    "            engine.best_loss = loss\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(engine, train_engine, config, src_vocab, tgt_vocab):\n",
    "        avg_train_loss = train_engine.state.metrics['loss']\n",
    "        avg_valid_loss = engine.state.metrics['loss']\n",
    "\n",
    "        # Set a filename for model of last epoch.\n",
    "        # We need to put every information to filename, as much as possible.\n",
    "        model_fn = config.model_fn.split('.')\n",
    "        \n",
    "        model_fn = model_fn[:-1] + ['%02d' % train_engine.state.epoch,\n",
    "                                    '%.2f-%.2f' % (avg_train_loss,\n",
    "                                                   np.exp(avg_train_loss)\n",
    "                                                   ),\n",
    "                                    '%.2f-%.2f' % (avg_valid_loss,\n",
    "                                                   np.exp(avg_valid_loss)\n",
    "                                                   )\n",
    "                                    ] + [model_fn[-1]]\n",
    "\n",
    "        model_fn = '.'.join(model_fn)\n",
    "\n",
    "        # Unlike other tasks, we need to save current model, not best model.\n",
    "        torch.save(\n",
    "            {\n",
    "                'model': engine.model.state_dict(),\n",
    "                'opt': train_engine.optimizer.state_dict(),\n",
    "                'config': config,\n",
    "                'src_vocab': src_vocab,\n",
    "                'tgt_vocab': tgt_vocab,\n",
    "            }, model_fn\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5a2f736b-e754-4c42-93a1-fd461270bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_event_handlers': defaultdict(list, {}),\n",
       " 'logger': <Logger ignite.engine.engine.Engine (WARNING)>,\n",
       " '_process_function': <function __main__.MaximumLikelihoodEstimationEngine.train(engine, mini_batch)>,\n",
       " 'last_event_name': None,\n",
       " 'should_terminate': False,\n",
       " 'should_terminate_single_epoch': False,\n",
       " 'state': State:\n",
       " \titeration: 0\n",
       " \tepoch: 0\n",
       " \tepoch_length: <class 'NoneType'>\n",
       " \tmax_epochs: <class 'NoneType'>\n",
       " \toutput: <class 'NoneType'>\n",
       " \tbatch: <class 'NoneType'>\n",
       " \tmetrics: <class 'dict'>\n",
       " \tdataloader: <class 'NoneType'>\n",
       " \tseed: <class 'NoneType'>\n",
       " \ttimes: <class 'dict'>,\n",
       " '_state_dict_user_keys': [],\n",
       " '_allowed_events': [<Events.EPOCH_STARTED: 'epoch_started'>,\n",
       "  <Events.EPOCH_COMPLETED: 'epoch_completed'>,\n",
       "  <Events.STARTED: 'started'>,\n",
       "  <Events.COMPLETED: 'completed'>,\n",
       "  <Events.ITERATION_STARTED: 'iteration_started'>,\n",
       "  <Events.ITERATION_COMPLETED: 'iteration_completed'>,\n",
       "  <Events.EXCEPTION_RAISED: 'exception_raised'>,\n",
       "  <Events.GET_BATCH_STARTED: 'get_batch_started'>,\n",
       "  <Events.GET_BATCH_COMPLETED: 'get_batch_completed'>,\n",
       "  <Events.DATALOADER_STOP_ITERATION: 'dataloader_stop_iteration'>,\n",
       "  <Events.TERMINATE: 'terminate'>,\n",
       "  <Events.TERMINATE_SINGLE_EPOCH: 'terminate_single_epoch'>],\n",
       " '_dataloader_iter': None,\n",
       " '_init_iter': []}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(Engine(MaximumLikelihoodEstimationEngine.train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2aad70-9362-4aa6-977b-0900d5b477b2",
   "metadata": {},
   "source": [
    "### 4-1 ignite 해부하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "086f02ca-20d5-40db-975c-c64474dfe92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_trainer = SingleTrainer(MaximumLikelihoodEstimationEngine, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "989cc48f-8bda-446a-b0dc-345593944e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_engine_class = MaximumLikelihoodEstimationEngine\n",
    "train_engine = target_engine_class(target_engine_class.train,\n",
    "                                  model,\n",
    "                                  crit,\n",
    "                                  optimizer,\n",
    "                                  lr_scheduler,\n",
    "                                  config)\n",
    "\n",
    "'''\n",
    "    trainer.py -> MaximumLikelihoodEstimationEngine.train 메서드\n",
    "                    이 매서드 : Engine(MaximumLikelihoodEstimationEngine.train) 이렇게 되는것임.\n",
    "                    그리고 기폰 format이  ---- def (engind, batch) ---- 로 정의하는것.임\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "15b02fc7-e213-48b3-8499-aaa55826850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (emb_src): Embedding(1711, 512)\n",
       "  (emb_dec): Embedding(1421, 512)\n",
       "  (encoder): Encoder(\n",
       "    (rnn): LSTM(512, 384, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn): LSTM(1280, 768, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  )\n",
       "  (attn): Attention(\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (concat): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (generator): Generator(\n",
       "    (output): Linear(in_features=768, out_features=1421, bias=True)\n",
       "    (softmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_engine.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6aeb8d04-f65e-4884-98f2-986614cfc0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 0\n",
       "\tepoch: 0\n",
       "\tepoch_length: <class 'NoneType'>\n",
       "\tmax_epochs: <class 'NoneType'>\n",
       "\toutput: <class 'NoneType'>\n",
       "\tbatch: <class 'NoneType'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'NoneType'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_engine.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "efff1022-a14a-4cb3-b92d-8f5a9e8f8d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_continue': False,\n",
       " 'model_fn': '.',\n",
       " 'train': '/Users/rainism/desktop/grad/torch_study/transformer/data/tt.corpus.shuf.train.tok.bpe',\n",
       " 'valid': '/Users/rainism/desktop/grad/torch_study/transformer/data/tt.corpus.shuf.train.tok.bpe',\n",
       " 'lang': 'enko',\n",
       " 'gpu_id': -1,\n",
       " 'store_true': False,\n",
       " 'batch_size': 160,\n",
       " 'n_epochs': 30,\n",
       " 'verbose': 1,\n",
       " 'init_epoch': 1,\n",
       " 'max_length': 100,\n",
       " 'dropout': 0.2,\n",
       " 'word_vec_size': 512,\n",
       " 'hidden_size': 768,\n",
       " 'n_layers': 4,\n",
       " 'max_grad_norm': 5.0,\n",
       " 'iteration_per_update': 1,\n",
       " 'lr': 1.0,\n",
       " 'lr_step': 1,\n",
       " 'lr_gamma': 0.5,\n",
       " 'lr_decay_start': 10,\n",
       " 'use_adam': True,\n",
       " 'rl_lr': 0.01,\n",
       " 'rl_n_samples': 1,\n",
       " 'rl_n_epochs': 10,\n",
       " 'rl_n_gram': 6,\n",
       " 'rl_reward': 'gleu',\n",
       " 'use_transformer': False,\n",
       " 'n_splits': 8}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_engine.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1abea815-3cc0-4e91-8b10-0816c88e77f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    initial_lr: 1.0\n",
       "    lr: 1.0\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_engine.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3a6685b9-0d1e-4067-b51d-4645d610e64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_engine.model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fd28b9b4-ced0-41f5-8a02-9607277fb38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 292,   11, 1603,  ...,  117,  140,    4],\n",
       "         [  23,   97,    5,  ...,    1,    1,    1],\n",
       "         [  23, 1373,   78,  ...,    1,    1,    1],\n",
       "         ...,\n",
       "         [  42,   18,  318,  ...,    1,    1,    1],\n",
       "         [1171, 1236,  346,  ...,    1,    1,    1],\n",
       "         [ 304,  203,    8,  ...,    1,    1,    1]]),\n",
       " tensor([90, 86, 83, 67, 66, 66, 64, 61, 59, 58, 57, 54, 54, 53, 53, 53, 53, 51,\n",
       "         51, 51, 51, 50, 49, 49, 49, 48, 47, 47, 46, 45, 44, 44, 42, 42, 41, 41,\n",
       "         40, 39, 39, 37, 37, 36, 36, 36, 34, 33, 33, 33, 32, 32, 31, 31, 31, 30,\n",
       "         29, 29, 28, 27, 26, 25, 25, 24, 23, 23, 22, 21, 21, 21, 20, 20, 20, 20,\n",
       "         19, 18, 18, 17, 17, 16, 16, 15, 15, 15, 15, 14, 14, 14, 13, 13, 13, 12,\n",
       "         12, 12, 11, 11, 11,  9,  9,  8,  6,  6]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter)).src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "24714f1d-13fe-428c-8bba-a4d1b586fbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  96,  107,  214,  ...,    1,    1,    1],\n",
       "        [ 155,   11,  873,  ...,    1,    1,    1],\n",
       "        [ 318,  396,   19,  ..., 1178,    4,    3],\n",
       "        ...,\n",
       "        [  47, 1228,  115,  ...,    1,    1,    1],\n",
       "        [  71,  838,  933,  ...,    1,    1,    1],\n",
       "        [ 114,  217, 1115,  ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter)).tgt[0][:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3ac06bba-54af-4a02-a7aa-9bdc8f4da078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.cuda.amp.autocast_mode.autocast"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f06fb369-0387-4a2c-bf5b-1a28d41979d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLLLoss()"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_engine.crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "df38d421-07cc-4b39-9793-5eed7089665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_iter)).src, next(iter(train_iter)).tgt[0][:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "934092da-1400-437b-9aad-eb19c322770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = train_engine.model(x, next(iter(train_iter)).tgt[0][:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fed66fcd-dc66-4e3f-9ecb-c340add19a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 76, 1421])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "335ec816-d833-4fc7-9ea7-78baec23fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_engine.crit(y_hat.contiguous().view(-1, y_hat.size(-1)), y.contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "edf63623-378a-4fbe-9bb2-ec6f6b440e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_target = loss.div(y.size(0)).div(train_engine.config.iteration_per_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5d3226f8-1a41-419e-9381-3bc3b4e5910a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([60, 65, 77, 44, 59, 46, 62, 49, 45, 62, 49, 45, 45, 59, 56, 51, 48, 58,\n",
       "        52, 47, 46, 46, 56, 48, 44, 50, 51, 42, 52, 36, 55, 40, 49, 45, 46, 42,\n",
       "        42, 52, 44, 37, 34, 37, 37, 35, 44, 40, 39, 37, 50, 37, 36, 35, 29, 38,\n",
       "        36, 26, 31, 27, 36, 34, 25, 22, 24, 21, 27, 26, 25, 21, 27, 21, 19, 19,\n",
       "        26, 30, 22, 25, 25, 24, 18, 23, 22, 18, 16, 22, 17, 16, 20, 18,  9, 20,\n",
       "        19, 17, 16, 14, 12, 16, 14, 15, 13,  9])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter)).tgt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "65d9e3c4-74f7-49c7-9086-5c4cec6d8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# float16 사용하려고함. 한번 뤱핑해주는거\n",
    "\n",
    "train_engine.scaler.scale(backward_target).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "38e69964-55a9-4eee-aebc-303c48f07e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = int(next(iter(train_iter)).tgt[1].sum())\n",
    "loss = float(loss/word_count)\n",
    "ppl = np.exp(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebd959-9261-41b8-96e9-a67b954fcecf",
   "metadata": {},
   "source": [
    "### 4-2 validation enigne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "609bb1ff-33aa-4284-ab6f-d41af8dffccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rainism/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "validation_engine = target_engine_class(target_engine_class.validate,\n",
    "                                  model,\n",
    "                                  crit,\n",
    "                                  optimizer,\n",
    "                                  lr_scheduler,\n",
    "                                  config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9c740-4f5f-479d-b15d-5d33d9f89dd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4-3 attatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3871d174-88db-483a-b428-302bbf8673c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metric_names = ['loss', 'ppl', '|param|', '|g_param|'],\n",
    "\n",
    "def attach_running_average(engine, metrix_name):\n",
    "    RunningAverage(output_transform = lambda x: x[metric_name]).attach(engine, metric_name)\n",
    "    \n",
    "for metric_name in training_metric_names:\n",
    "    attach_running_average(train_engine, metric_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9d4e5cf3-a744-4bb3-a1ab-99c553342f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_engine.state.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c46457f3-f7ce-40dd-8fd2-fc52ccb88406",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_engine_class.attach(train_engine, validation_engine, verbose = config.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "704a6f68-4357-4091-9ca5-6e7c00698aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              '__init__': <function __main__.MaximumLikelihoodEstimationEngine.__init__(self, func, model, crit, optimizer, lr_scheduler, config)>,\n",
       "              'train': <staticmethod at 0x146aec880>,\n",
       "              'validate': <staticmethod at 0x146aecac0>,\n",
       "              'attach': <staticmethod at 0x146aece80>,\n",
       "              'resume_training': <staticmethod at 0x146aec370>,\n",
       "              'check_best': <staticmethod at 0x146aecee0>,\n",
       "              'save_model': <staticmethod at 0x1471150d0>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(target_engine_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "92a69406-9295-468e-8143-1da5100c80cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': Seq2Seq(\n",
       "   (emb_src): Embedding(1711, 512)\n",
       "   (emb_dec): Embedding(1421, 512)\n",
       "   (encoder): Encoder(\n",
       "     (rnn): LSTM(512, 384, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "   )\n",
       "   (decoder): Decoder(\n",
       "     (rnn): LSTM(1280, 768, num_layers=4, batch_first=True, dropout=0.2)\n",
       "   )\n",
       "   (attn): Attention(\n",
       "     (linear): Linear(in_features=768, out_features=768, bias=False)\n",
       "     (softmax): Softmax(dim=-1)\n",
       "   )\n",
       "   (concat): Linear(in_features=1536, out_features=768, bias=True)\n",
       "   (tanh): Tanh()\n",
       "   (generator): Generator(\n",
       "     (output): Linear(in_features=768, out_features=1421, bias=True)\n",
       "     (softmax): LogSoftmax(dim=-1)\n",
       "   )\n",
       " ),\n",
       " 'crit': NLLLoss(),\n",
       " 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     initial_lr: 1.0\n",
       "     lr: 1.0\n",
       "     weight_decay: 0\n",
       " ),\n",
       " 'lr_scheduler': <torch.optim.lr_scheduler.MultiStepLR at 0x146da7160>,\n",
       " 'config': Namespace(batch_size=160, dropout=0.2, gpu_id=-1, hidden_size=768, init_epoch=1, is_continue=False, iteration_per_update=1, lang='enko', lr=1.0, lr_decay_start=10, lr_gamma=0.5, lr_step=1, max_grad_norm=5.0, max_length=100, model_fn='.', n_epochs=30, n_layers=4, n_splits=8, rl_lr=0.01, rl_n_epochs=10, rl_n_gram=6, rl_n_samples=1, rl_reward='gleu', store_true=False, train='/Users/rainism/desktop/grad/torch_study/transformer/data/tt.corpus.shuf.train.tok.bpe', use_adam=True, use_transformer=False, valid='/Users/rainism/desktop/grad/torch_study/transformer/data/tt.corpus.shuf.train.tok.bpe', verbose=1, word_vec_size=512),\n",
       " '_event_handlers': defaultdict(list,\n",
       "             {<Events.EPOCH_STARTED: 'epoch_started'>: [(<bound method Metric.started of <ignite.metrics.running_average.RunningAverage object at 0x146cea0a0>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,),\n",
       "                {}),\n",
       "               (<bound method Metric.started of <ignite.metrics.running_average.RunningAverage object at 0x146d6cf40>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,),\n",
       "                {}),\n",
       "               (<bound method Metric.started of <ignite.metrics.running_average.RunningAverage object at 0x146d5c2b0>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,),\n",
       "                {}),\n",
       "               (<bound method Metric.started of <ignite.metrics.running_average.RunningAverage object at 0x146cea2b0>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,),\n",
       "                {})],\n",
       "              <Events.ITERATION_COMPLETED: 'iteration_completed'>: [(<bound method Metric.iteration_completed of <ignite.metrics.running_average.RunningAverage object at 0x146cea0a0>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,),\n",
       "                {}),\n",
       "               (<bound method Metric.completed of <ignite.metrics.running_average.RunningAverage object at 0x146cea0a0>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,\n",
       "                 'loss'),\n",
       "                {}),\n",
       "               (<bound method Metric.iteration_completed of <ignite.metrics.running_average.RunningAverage object at 0x146d6cf40>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,),\n",
       "                {}),\n",
       "               (<bound method Metric.completed of <ignite.metrics.running_average.RunningAverage object at 0x146d6cf40>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,\n",
       "                 'ppl'),\n",
       "                {}),\n",
       "               (<bound method Metric.iteration_completed of <ignite.metrics.running_average.RunningAverage object at 0x146d5c2b0>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,),\n",
       "                {}),\n",
       "               (<bound method Metric.completed of <ignite.metrics.running_average.RunningAverage object at 0x146d5c2b0>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,\n",
       "                 '|param|'),\n",
       "                {}),\n",
       "               (<bound method Metric.iteration_completed of <ignite.metrics.running_average.RunningAverage object at 0x146cea2b0>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,),\n",
       "                {}),\n",
       "               (<bound method Metric.completed of <ignite.metrics.running_average.RunningAverage object at 0x146cea2b0>>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,\n",
       "                 '|g_param|'),\n",
       "                {})],\n",
       "              <Events.EPOCH_COMPLETED: 'epoch_completed'>: [(<function __main__.MaximumLikelihoodEstimationEngine.attach.<locals>.print_train_logs(engine)>,\n",
       "                (<__main__.MaximumLikelihoodEstimationEngine at 0x147381760>,),\n",
       "                {})]}),\n",
       " 'logger': <Logger ignite.engine.engine.MaximumLikelihoodEstimationEngine (WARNING)>,\n",
       " '_process_function': <function __main__.MaximumLikelihoodEstimationEngine.train(engine, mini_batch)>,\n",
       " 'last_event_name': None,\n",
       " 'should_terminate': False,\n",
       " 'should_terminate_single_epoch': False,\n",
       " 'state': State:\n",
       " \titeration: 0\n",
       " \tepoch: 0\n",
       " \tepoch_length: <class 'NoneType'>\n",
       " \tmax_epochs: <class 'NoneType'>\n",
       " \toutput: <class 'NoneType'>\n",
       " \tbatch: <class 'NoneType'>\n",
       " \tmetrics: <class 'dict'>\n",
       " \tdataloader: <class 'NoneType'>\n",
       " \tseed: <class 'NoneType'>\n",
       " \ttimes: <class 'dict'>,\n",
       " '_state_dict_user_keys': [],\n",
       " '_allowed_events': [<Events.EPOCH_STARTED: 'epoch_started'>,\n",
       "  <Events.EPOCH_COMPLETED: 'epoch_completed'>,\n",
       "  <Events.STARTED: 'started'>,\n",
       "  <Events.COMPLETED: 'completed'>,\n",
       "  <Events.ITERATION_STARTED: 'iteration_started'>,\n",
       "  <Events.ITERATION_COMPLETED: 'iteration_completed'>,\n",
       "  <Events.EXCEPTION_RAISED: 'exception_raised'>,\n",
       "  <Events.GET_BATCH_STARTED: 'get_batch_started'>,\n",
       "  <Events.GET_BATCH_COMPLETED: 'get_batch_completed'>,\n",
       "  <Events.DATALOADER_STOP_ITERATION: 'dataloader_stop_iteration'>,\n",
       "  <Events.TERMINATE: 'terminate'>,\n",
       "  <Events.TERMINATE_SINGLE_EPOCH: 'terminate_single_epoch'>],\n",
       " '_dataloader_iter': None,\n",
       " '_init_iter': [],\n",
       " 'best_loss': inf,\n",
       " 'scaler': <torch.cuda.amp.grad_scaler.GradScaler at 0x147381250>}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "93ed6c32-c0ab-4987-8879-375185fa0945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Field.numericalize of <torchtext.data.field.Field object at 0x1473e77f0>>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.src.numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "81393a2a-7a79-479c-95ff-6c762331a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [1,2,3,4,5]\n",
    "original_indice = [5,4,3,2,1]\n",
    "lines = torch.randn(5,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b331beb3-8574-4b52-a5d4-761d4deefc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[ 0.4472,  0.3944, -0.0200,  0.3257, -1.0150,  1.1418, -1.7212,  1.0884,\n",
       "            0.4792,  0.4483],\n",
       "          [ 1.9648,  0.1412, -0.7832, -0.6083, -0.4342, -0.4592,  2.2842, -1.3607,\n",
       "            0.7157, -0.0124],\n",
       "          [-0.8316, -0.4368,  1.0706, -1.1156,  0.3405,  0.1276,  0.7088, -1.4126,\n",
       "           -1.0587, -0.6145],\n",
       "          [-0.9105,  0.1868,  0.4701, -0.6926, -0.4354,  1.1676, -2.5950, -0.1424,\n",
       "            0.1556, -1.3227],\n",
       "          [-0.3083,  1.0997,  0.9196,  0.9776,  0.3829,  0.7150,  1.1635, -1.0842,\n",
       "           -0.7882,  0.6775]]),\n",
       "  5,\n",
       "  1),\n",
       " (tensor([[-4.2061e-01, -1.1096e+00,  1.1334e+00,  1.0778e+00, -3.8335e-01,\n",
       "            2.3949e-01,  6.2579e-01, -1.7524e-01, -3.3979e-04, -9.2623e-01],\n",
       "          [-1.0018e+00,  9.2463e-01, -1.7597e+00,  6.6523e-01, -7.4228e-01,\n",
       "           -8.4601e-02,  1.1326e+00,  1.0051e+00, -1.5662e-01, -1.0116e-01],\n",
       "          [-8.0829e-01,  8.4494e-01,  2.1979e+00,  1.5576e+00,  4.8203e-01,\n",
       "            4.8864e-01, -1.0841e+00,  5.3555e-01, -3.0181e-01,  2.0956e-01],\n",
       "          [ 1.6614e-01,  3.7987e-03,  2.5932e+00, -1.1776e+00,  4.0703e-01,\n",
       "            2.2625e+00, -3.1538e-01,  3.5957e-01, -2.9241e-01,  1.1672e+00],\n",
       "          [-1.5418e+00, -1.0666e-01,  4.0628e-01, -4.7393e-01, -1.2449e-01,\n",
       "           -3.1118e-02,  2.2982e+00,  1.6456e+00,  9.8704e-01, -3.8113e-01]]),\n",
       "  4,\n",
       "  2),\n",
       " (tensor([[ 1.1884, -1.9162,  0.5564, -0.1100,  1.0281,  0.7469,  0.6835, -0.1437,\n",
       "           -1.9823, -0.0866],\n",
       "          [ 0.4224,  0.2688,  0.4354,  0.5749,  0.8064,  1.6561,  0.2085, -0.3951,\n",
       "           -0.3025,  0.2700],\n",
       "          [ 0.5662,  0.1892, -1.0911,  0.9688, -1.6000,  0.3163,  0.7622,  1.0369,\n",
       "           -0.4985,  0.8796],\n",
       "          [ 0.6607, -0.1609, -0.0455, -1.3456, -0.8547,  2.0956, -0.3104, -0.2835,\n",
       "            1.2195, -0.6208],\n",
       "          [ 0.8195, -1.1281, -1.1838, -0.6869, -1.5154, -0.4129, -0.5196, -0.9411,\n",
       "           -0.4681, -0.3124]]),\n",
       "  3,\n",
       "  3),\n",
       " (tensor([[-0.2386, -0.6674, -0.9797,  1.7366,  0.1936, -1.7410, -0.3180, -1.2852,\n",
       "            0.7713, -0.1845],\n",
       "          [ 0.9299, -0.9705, -2.0700,  0.3743,  0.0729,  0.0102, -0.0916, -1.4365,\n",
       "           -0.8061, -0.2250],\n",
       "          [ 0.3708,  2.3772,  0.4029,  0.8010, -0.1907, -0.2008,  0.4845,  0.3948,\n",
       "           -0.8607, -0.3253],\n",
       "          [ 0.6613,  1.5158, -1.1819,  0.3171, -1.6885,  1.1862,  0.3704,  0.5654,\n",
       "            1.9033,  0.5426],\n",
       "          [ 0.2631,  0.7884, -0.8081, -0.8560,  0.5737,  0.1466,  0.2740,  1.1554,\n",
       "            1.2182,  0.7583]]),\n",
       "  2,\n",
       "  4),\n",
       " (tensor([[-0.0723, -1.2596,  0.4781,  1.4812, -0.9870,  0.2062, -0.7661,  1.2392,\n",
       "            1.1758, -0.8672],\n",
       "          [ 0.4374,  0.9998, -1.9295, -0.1487, -0.6280,  0.0046, -0.8745,  0.6571,\n",
       "           -0.9455,  0.2068],\n",
       "          [-0.8616, -0.1041, -0.1576, -0.0583,  0.7372,  1.6594, -0.2645, -0.4740,\n",
       "            0.5876, -0.5042],\n",
       "          [ 0.3270, -0.1066, -0.8372, -1.4475, -0.9849,  0.3794,  2.2484,  0.9457,\n",
       "            0.1077, -2.5530],\n",
       "          [-0.9831,  1.3794, -0.7910,  1.0350, -0.0907,  1.2655,  0.2830,  0.0502,\n",
       "           -1.1804,  0.9083]]),\n",
       "  1,\n",
       "  5)]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted(zip(lines, lengths, original_indice), key = itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "58073afc-de91-47b5-954f-a8580eef172b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a9bc3-a288-4f21-9734-cefd8d46a2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
