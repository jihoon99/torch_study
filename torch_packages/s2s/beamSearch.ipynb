{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a5d8a7-e309-405f-9e29-6078acdef90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c86a4d6-623f-4ea2-8775-40a3f67266b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 5\n",
    "max_length = 50\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bd091b1-9841-4677-9889-728149af64ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_indice = [tensor([1, 1, 1, 1, 1])]\n",
      "beam_indice = [tensor([-1, -1, -1, -1, -1])]\n",
      "cumulative_probs = [tensor([0., -inf, -inf, -inf, -inf])]\n",
      "masks = [tensor([False, False, False, False, False])]\n",
      "--------------------------------------------\n",
      "prev_state_0 {'init_status': None, 'batch_dim_index': 0}\n",
      "prev_state_1 {'init_status': None, 'batch_dim_index': 0}\n"
     ]
    }
   ],
   "source": [
    "word_indice = [torch.LongTensor(beam_size).zero_().to(device)+1]\n",
    "print(f'word_indice = {word_indice}')\n",
    "beam_indice = [torch.LongTensor(beam_size).zero_().to(device)-1]\n",
    "print(f'beam_indice = {beam_indice}')\n",
    "cumulative_probs = [torch.FloatTensor([.0] + [-float('inf')] * (beam_size - 1)).to(device)]\n",
    "print(f'cumulative_probs = {cumulative_probs}')\n",
    "masks = [torch.BoolTensor(beam_size).zero_().to(device)]\n",
    "print(f'masks = {masks}')\n",
    "\n",
    "print('--------------------------------------------')\n",
    "\n",
    "prev_status = {}\n",
    "batch_dims = {}\n",
    "prev_status_config = {\n",
    "    'prev_state_0': {                   # input에 해당하는 state\n",
    "        'init_status': None,\n",
    "        'batch_dim_index': 0,\n",
    "    },\n",
    "    'prev_state_1': {                   # 첫번째 블락을 통과한 state\n",
    "        'init_status': None,\n",
    "        'batch_dim_index': 0,\n",
    "    }}\n",
    "\n",
    "for prev_status_name, each_config in prev_status_config.items():\n",
    "    print(prev_status_name, each_config)\n",
    "    init_status = each_config['init_status']\n",
    "    batch_dim_index = each_config['batch_dim_index']\n",
    "    if init_status is not None:\n",
    "        prev_status[prev_status_name] = torch.cat([init_status]*beam_size, dim = batch_dim_index)\n",
    "    else:\n",
    "        prev_status[prev_status_name] = None\n",
    "    batch_dims[prev_status_name] = batch_dim_index\n",
    "current_time_step = 0\n",
    "done_cnt = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e0e7481-dbc4-4f7a-b1e3-d42653b9cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = word_indice[-1].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "867c52ed-d5f7-4db8-af94-51bbbda4dc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54909af8-571d-43a3-beff-4f33510eddb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev_state_0': None, 'prev_state_1': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21f93f1e-3380-44fa-9adc-732e14bf1e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fab_input = [torch.tensor([0,0,0,0,0]).unsqueeze(-1), torch.tensor([1,0,0,0,0]).unsqueeze(-1)]\n",
    "torch.cat(fab_input, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a292ba-dd46-4ad0-8fac-febef65e28bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df8c73bc-5705-4a2a-80eb-e837de8d3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import simple_nmt.data_loader as data_loader\n",
    "\n",
    "LENGTH_PENALTY = .2\n",
    "MIN_LENGTH = 5\n",
    "\n",
    "\n",
    "class SingleBeamSearchBoard():\n",
    "    '''\n",
    "    From Board\n",
    "        - input : x_t\n",
    "        - last hidden : h_t_1\n",
    "        - last cell : c_t_1\n",
    "        - last hidden_tilde : h_tilde_t_1\n",
    "    \n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        prev_status_config, # Fake minibatch를 만들기 위해선, input, hidden, cell, hidden_tilde가 필요함. 이게 prev_status_config에 들어감.// type은 dict의 dict\n",
    "        beam_size=5,\n",
    "        max_length=255,\n",
    "    ):\n",
    "        '''\n",
    "        init에 previous_status를 저장함. -> prev_status\n",
    "        \n",
    "\n",
    "        '''\n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # To put data to same device.\n",
    "        self.device = device\n",
    "        # Inferred word index for each time-step. For now, initialized with initial time-step. 첫번째니까 빔 사이즈 만큼 모두 BOS가 들어가야함.\n",
    "        self.word_indice = [torch.LongTensor(beam_size).zero_().to(self.device) + 1]\n",
    "            # [tensor([0,0,0,0,0])]\n",
    "            # 추후에 tensor([1,0,0,0,0]) 이런게 쌓임.\n",
    "        # Beam index for selected word index, at each time-step. 빔 사이즈 만큼 -1을 채워넣은 텐서\n",
    "        self.beam_indice = [torch.LongTensor(beam_size).zero_().to(self.device) - 1]\n",
    "        # Cumulative log-probability for each beam. \n",
    "        self.cumulative_probs = [torch.FloatTensor([.0] + [-float('inf')]*(beam_size - 1)).to(self.device)]\n",
    "            # 처음 cumulative_probs에서 [0, -inf, -inf, -inf, -inf]로 하고싶음. 왜냐면 BOS는 확정적인거라 1의 확률을 갖는데 log(BOS) = 0임.\n",
    "            # 가지가 분할하기 전에는 한개의 확률만 갖으므로 나머지는 -inf로 채움.\n",
    "            # 첫번째 빔에서(0)만 5개의 후보가 뽑힐거야\n",
    "        # 1 if it is done else 0\n",
    "        self.masks = [torch.BoolTensor(beam_size).zero_().to(self.device)]\n",
    "            # 0이면 진행, 1(EOS)면 끝.\n",
    "\n",
    "        # We don't need to remember every time-step of hidden states:\n",
    "        #       prev_hidden, prev_cell, prev_h_t_tilde\n",
    "        # What we need is remember just last one.\n",
    "\n",
    "        #-------------------- 맨처음 세팅해줘야 하는것 -----------------------\n",
    "        self.prev_status = {}\n",
    "        self.batch_dims = {}\n",
    "        for prev_status_name, each_config in prev_status_config.items():\n",
    "            init_status = each_config['init_status'] # 바로 전의 state을 가져오고\n",
    "            batch_dim_index = each_config['batch_dim_index'] # 배치 인덱스를 가져와\n",
    "            if init_status is not None:\n",
    "                self.prev_status[prev_status_name] = torch.cat([init_status] * beam_size,\n",
    "                                                               dim=batch_dim_index)\n",
    "                    # s2s - hidden, cell :  [L, B*beam_size, H] 여기서 B는 1임.\n",
    "            else:\n",
    "                self.prev_status[prev_status_name] = None\n",
    "                    # s2s - h_tilde : [B*beam, 1, hidden]\n",
    "            self.batch_dims[prev_status_name] = batch_dim_index\n",
    "                # s2s - {hidden_state : 1, ...}\n",
    "        self.current_time_step = 0\n",
    "        self.done_cnt = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_length_penalty(\n",
    "        self,\n",
    "        length,\n",
    "        alpha=LENGTH_PENALTY,\n",
    "        min_length=MIN_LENGTH,\n",
    "    ):\n",
    "        # Calculate length-penalty,\n",
    "        # because shorter sentence usually have bigger probability.\n",
    "        # In fact, we represent this as log-probability, which is negative value.\n",
    "        # Thus, we need to multiply bigger penalty for shorter one.\n",
    "        p = ((min_length + 1) / (min_length + length))**alpha\n",
    "        # 6/(5+1)\n",
    "        return p\n",
    "\n",
    "\n",
    "\n",
    "    def is_done(self):\n",
    "        '''\n",
    "        빔사이즈보다, done_cnt가 크거나 같으면 1을 리턴, 아니면 0을 리턴.   \n",
    "        done_cnt는 collect_result에서 업데이트 될것.     \n",
    "        '''\n",
    "\n",
    "        # Return 1, if we had EOS more than 'beam_size'-times.\n",
    "        if self.done_cnt >= self.beam_size:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "    def get_batch(self):\n",
    "        '''\n",
    "        returning [beam_size,1] : last step output(V_t)\n",
    "                [baem_size, L, H] : prev_state_i  = 이거 튜플임.\n",
    "        '''\n",
    "        y_hat = self.word_indice[-1].unsqueeze(-1)\n",
    "            # word_indice : tensor([0,0,0,0,0]) 이전 타임 스탭의 출력물을 가져옴. -> unsqueeze(-1) : [5,1]\n",
    "        # |y_hat| = (beam_size, 1)\n",
    "        # if model != transformer:\n",
    "        #     |hidden| = |cell| = (n_layers, beam_size, hidden_size)\n",
    "        #     |h_t_tilde| = (beam_size, 1, hidden_size) or None\n",
    "        # else:\n",
    "        #     |prev_state_i| = (beam_size, length, hidden_size),\n",
    "        #     where i is an index of layer.\n",
    "        return y_hat, self.prev_status\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #@profile \n",
    "    def collect_result(self, y_hat, prev_status):\n",
    "        # |y_hat| = (beam_size, 1, output_size)\n",
    "        # prev_status is a dict, which has following keys:\n",
    "        # if model != transformer:\n",
    "        #     |hidden| = |cell| = (n_layers, beam_size, hidden_size)\n",
    "        #     |h_t_tilde| = (beam_size, 1, hidden_size)\n",
    "        # else:\n",
    "        #     |prev_state_i| = (beam_size, length, hidden_size),\n",
    "        #     where i is an index of layer.\n",
    "        output_size = y_hat.size(-1)\n",
    "\n",
    "        self.current_time_step += 1\n",
    "\n",
    "        #---------------- Calculate cumulative log-probability. ----------------------\n",
    "        # First, fill -inf value to last cumulative probability, if the beam is already finished.\n",
    "        # Second, expand -inf filled cumulative probability to fit to 'y_hat'.\n",
    "        # (beam_size) --> (beam_size, 1, 1) --> (beam_size, 1, output_size)\n",
    "        # Third, add expanded cumulative probability to 'y_hat'\n",
    "        cumulative_prob = self.cumulative_probs[-1].masked_fill_(self.masks[-1], -float('inf'))\n",
    "            # cumulative_probs들이 5개씩 탁탁탁 쌓일텐데 그중 마지막걸 가져와서\n",
    "            # 마지막 마스킹 정보를 갖고와서, True이면 마스크를 하고 with -inf로, 아니면 마스킹을 하지 않는다.\n",
    "        cumulative_prob = y_hat + cumulative_prob.view(-1, 1, 1).expand(self.beam_size, 1, output_size) # broadcasting되기 때문에 expand안해도됨.\n",
    "        # |cumulative_prob| = (beam_size, 1, output_size)\n",
    "\n",
    "        # Now, we have new top log-probability and its index.\n",
    "        # We picked top index as many as 'beam_size'.\n",
    "        # Be aware that we picked top-k from whole batch through 'view(-1)'.\n",
    "\n",
    "        # Following lines are using torch.topk, which is slower than torch.sort.\n",
    "        # top_log_prob, top_indice = torch.topk(\n",
    "        #     cumulative_prob.view(-1), # (beam_size * output_size,)\n",
    "        #     self.beam_size,\n",
    "        #     dim=-1,\n",
    "        # )\n",
    "\n",
    "        # Following lines are using torch.sort, instead of using torch.topk.\n",
    "        top_log_prob, top_indice = cumulative_prob.view(-1).sort(descending=True)\n",
    "            # torch.sort를 사용하면 : values, indice두개를 내뱉는다.\n",
    "        top_log_prob, top_indice = top_log_prob[:self.beam_size], top_indice[:self.beam_size]\n",
    "            # 상위 5개만 갖고온다.\n",
    "\n",
    "        # |top_log_prob| = (beam_size,)\n",
    "        # |top_indice| = (beam_size,)\n",
    "\n",
    "        # Because we picked from whole batch, original word index should be calculated again.\n",
    "        self.word_indice += [top_indice.fmod(output_size)]\n",
    "            # fmod : element-wise나머지 구하기. // devided by output_size\n",
    "            # outputsize로 나누면 원래 vocab_index가 리턴이 되겠네\n",
    "        # Also, we can get an index of beam, which has top-k log-probability search result.\n",
    "        self.beam_indice += [top_indice.div(float(output_size)).long()]\n",
    "            # 41030 -> 4번째 빔에서, 1030번째 단어. 여기서 구하고자 하는것은 몇번째 빔인지 구하고 싶음.\n",
    "\n",
    "        # Add results to history boards.\n",
    "        self.cumulative_probs += [top_log_prob]\n",
    "        self.masks += [torch.eq(self.word_indice[-1], 2)] # Set finish mask if we got EOS.\n",
    "            # torch equal (word_indice[-1], data_loader.EOS) -> 1 if it is, else 0\n",
    "        # Calculate a number of finished beams.\n",
    "        self.done_cnt += self.masks[-1].float().sum() # EOS가 몇개 있엇는지 확인\n",
    "\n",
    "        # In beam search procedure, we only need to memorize latest status.\n",
    "        # For seq2seq, it would be lastest hidden and cell state, and h_t_tilde.\n",
    "        # The problem is hidden(or cell) state and h_t_tilde has different dimension order.\n",
    "        # In other words, a dimension for batch index is different.\n",
    "        # Therefore self.batch_dims stores the dimension index for batch index.\n",
    "        # For transformer, lastest status is each layer's decoder output from the biginning.\n",
    "        # Unlike seq2seq, transformer has to memorize every previous output for attention operation.\n",
    "        for prev_status_name, prev_status in prev_status.items():\n",
    "            self.prev_status[prev_status_name] = torch.index_select(\n",
    "                prev_status,\n",
    "                dim=self.batch_dims[prev_status_name], # 어떤 차원을 뽑아올지\n",
    "                index=self.beam_indice[-1] # 정해진 dim에서 몇번째 데이터를 뽑아올지.\n",
    "            ).contiguous()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_n_best(self, n=1, length_penalty=.2):\n",
    "        '''\n",
    "        output : 최고의 확률을 갖는 5개를 선별하고, \n",
    "        sentences와 probs를 return한다.\n",
    "\n",
    "        sentences : [[2021, 3, 1394, ...],\n",
    "                    [3019, ,20, 391, ...],\n",
    "                    ...\n",
    "                    [1010, 50, 0203, ...]]\n",
    "\n",
    "        probs : [0.3, 0.5, 0.1, 0.3, 0.4]\n",
    "\n",
    "        '''\n",
    "\n",
    "        sentences, probs, founds = [], [], []\n",
    "\n",
    "        for t in range(len(self.word_indice)):  # for each time-step,,,\n",
    "            # word_indice : [[0,0,0,0,0],[1,0,0,0,0],[1,0,0,0,0],...]\n",
    "            for b in range(self.beam_size):  # for each beam, // 5번\n",
    "                if self.masks[t][b] == 1:  # if we had EOS on this time-step and beam, EOS를 찾으면,\n",
    "                    # Take a record of penaltified log-proability.\n",
    "                    probs += [self.cumulative_probs[t][b] * self.get_length_penalty(t, alpha=length_penalty)]\n",
    "                    founds += [(t, b)] # 어디서 EOS를 찾았는지 -> 그 확률이 어떻게 되나 나중에 역추적할라고\n",
    "                    # 재수가 없으면 EOS없이 max len으로 끝날수 있음.\n",
    "\n",
    "        # Also, collect log-probability from last time-step, for the case of EOS is not shown.\n",
    "        for b in range(self.beam_size):\n",
    "            if self.cumulative_probs[-1][b] != -float('inf'): # If this beam does not have EOS,\n",
    "                if not (len(self.cumulative_probs) - 1, b) in founds:\n",
    "                    probs += [self.cumulative_probs[-1][b] * self.get_length_penalty(len(self.cumulative_probs),\n",
    "                                                                                     alpha=length_penalty)]\n",
    "                    founds += [(t, b)]\n",
    "\n",
    "        # Sort and take n-best.\n",
    "        sorted_founds_with_probs = sorted(\n",
    "            zip(founds, probs),\n",
    "            key=itemgetter(1),\n",
    "            reverse=True,\n",
    "        )[:n]\n",
    "        probs = []\n",
    "\n",
    "        for (end_index, b), prob in sorted_founds_with_probs:\n",
    "            sentence = []\n",
    "\n",
    "            # Trace from the end.\n",
    "            for t in range(end_index, 0, -1):\n",
    "                sentence = [self.word_indice[t][b]] + sentence\n",
    "                b = self.beam_indice[t][b] # 빔따라서 거꾸로 올라감.\n",
    "\n",
    "            sentences += [sentence]\n",
    "            probs += [prob]\n",
    "\n",
    "        return sentences, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76806b62-73bb-4398-9574-99324e80ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.Sequential()\n",
    "\n",
    "\n",
    "def _generate_mask( x, length):\n",
    "    '''\n",
    "\n",
    "    에) length : [4,3,2]\n",
    "    '''\n",
    "    mask = []\n",
    "\n",
    "    max_length = max(length)\n",
    "    for l in length:\n",
    "        if max_length - l > 0:\n",
    "            # If the length is shorter than maximum length among samples,\n",
    "            # set last few values to be 1s to remove attention weight.\n",
    "            mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
    "                                x.new_ones(1, (max_length - l))\n",
    "                                ], dim=-1)]\n",
    "        else:\n",
    "            # If length of sample equals to maximum length among samples,\n",
    "            # set every value in mask to be 0.\n",
    "            mask += [x.new_ones(1, l).zero_()]\n",
    "\n",
    "    mask = torch.cat(mask, dim=0).bool()\n",
    "    # |mask| = (batch_size, max_length)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "x = [torch.randn(64,20,100), torch.randint(20,[64])]\n",
    "\n",
    "batch_size = x[0].size(0)\n",
    "n_dec_layers = 10\n",
    "\n",
    "mask = _generate_mask(x[0], x[1])\n",
    "# |mask| = (batch_size, n)\n",
    "x = x[0]\n",
    "\n",
    "mask_enc = mask.unsqueeze(1).expand(mask.size(0), x.size(1), mask.size(-1))\n",
    "mask_dec = mask.unsqueeze(1)\n",
    "# |mask_enc| = (batch_size, n, n)\n",
    "# |mask_dec| = (batch_size, 1, n)\n",
    "\n",
    "z = torch.randn(64, 20, 100)\n",
    "# |z| = (batch_size, n, hidden_size)\n",
    "# --------------------여기까지 search 함수와 똑같음 --------------------------\n",
    "\n",
    "\n",
    "prev_status_config = {}\n",
    "for layer_index in range(n_dec_layers + 1):\n",
    "    prev_status_config['prev_state_%d' % layer_index] = {\n",
    "        'init_status': None,\n",
    "        'batch_dim_index': 0,\n",
    "    }\n",
    "    \n",
    "    \n",
    "boards = [\n",
    "    SingleBeamSearchBoard(\n",
    "        z.device,\n",
    "        prev_status_config,\n",
    "        beam_size=beam_size,\n",
    "        max_length=max_length,\n",
    "    ) for _ in range(batch_size)\n",
    "]\n",
    "done_cnt = [board.is_done() for board in boards]\n",
    "\n",
    "length = 0\n",
    "\n",
    "while sum(done_cnt) < batch_size and length <= max_length:\n",
    "    fab_input, fab_z, fab_mask = [],[],[]\n",
    "    fab_prevs = [[] for _ in range(n_dec_layers + 1)]\n",
    "    \n",
    "    for i, board in enumerate(boards):\n",
    "        if board.is_done() == 0:\n",
    "            y_hat_i, prev_status = board.get_batch()\n",
    "            \n",
    "            fab_input += [y_hat_i]\n",
    "            fab_z += [z[i].unsqueeze(0)]*beam_size\n",
    "            fab_mask += [mask_dec[i].unsqueeze(0)]*beam_size\n",
    "            \n",
    "            for layer_index in range(n_dec_layers + 1):\n",
    "                prev_i = prev_status['prev_state_%d' % layer_index]\n",
    "                if prev_i is not None:\n",
    "                    fab_prevs[layer_index] += [prev_i]\n",
    "                else:\n",
    "                    fab_prevs[layer_index] = None\n",
    "    fab_input = torch.cat(fab_input, dim=0)\n",
    "    fab_z     = torch.cat(fab_z,     dim=0)\n",
    "    fab_mask  = torch.cat(fab_mask,  dim=0)\n",
    "    for i, fab_prev in enumerate(fab_prevs): # i == layer_index\n",
    "        if fab_prev is not None:\n",
    "            fab_prevs[i] = torch.cat(fab_prev, dim=0)\n",
    "            \n",
    "    h_t = fab_input.unsqueeze(-1).expand(*fab_input.size(), 100)\n",
    "    \n",
    "    if fab_prevs[0] is None:\n",
    "        fab_prevs[0] = h_t\n",
    "    else:\n",
    "        fab_prevs[0] = torch.cat([fab_prevs[0], h_t], dim = 1)\n",
    "        \n",
    "    for layer_index, block in enumerate(decoder._modules.values()):\n",
    "            \n",
    "            \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c3e690cc-5c62-4516-b341-129c31cca0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 1, 100])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fab_input.unsqueeze(-1).expand(*fab_input.size(), 100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0737d259-18df-4ad3-b9b5-022872d04144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fab_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f3cb159-5989-4d18-be6a-1db1c52f3164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 20, 100])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fab_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b96ea2b9-95f0-453e-9266-827eb7afaaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 20, 100])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([z[0].unsqueeze(0)]*5, dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516254e9-2d33-4032-a221-4496726db235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10841a6-8e71-4829-b60b-5b59f5587412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014d348-93ae-4f91-ab96-9d672e6063e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
