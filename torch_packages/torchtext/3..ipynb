{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0f8b3-dd5d-42e5-927c-6be222871042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "english_txt = open('train_WMT_english.txt', encoding = 'utf8').read().split('\\n')\n",
    "german_txt = open('train_WMT_german.txt', encoding = 'utf8').read.split('\\n')\n",
    "                                                                        \n",
    "raw_data = {'English' : [line for line in english_txt[1:1000]],\n",
    "            'German': [line for line in german_txt[1:1000]]}\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns = ['English', 'German'])\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.2)\n",
    "\n",
    "train.to_json('train.json', orient = 'records', lines = True)\n",
    "test.to_json('test.json', orient = 'records', lines = True)\n",
    "\n",
    "# train.to_csv(\"train.csv\", index = False)\n",
    "# test.to_csv(\"test.csv\", index = False)\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    return [te.split() for tx in text]\n",
    "\n",
    "def tokenize_ger(text):\n",
    "    return [te.split() for tx in text]\n",
    "\n",
    "english = Field(sequential = True,\n",
    "                use_vocab = True,\n",
    "                tokenize = tokenize_eng,\n",
    "                lower = True)\n",
    "\n",
    "german = Field(sequential = True,\n",
    "               use_vocab = True,\n",
    "               tokenize = tokenize_ger,\n",
    "               lower = True)\n",
    "\n",
    "fields = {'English' : ('eng', english), 'German':('ger', german)}\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "                                        path = '',\n",
    "                                        train = 'train.json',\n",
    "                                        test = 'test.json',\n",
    "                                        forma = 'json',\n",
    "                                        fields = fields)\n",
    "\n",
    "english.build_vocab(train_data, max_size = 10000, min_freq = 2)\n",
    "german.build_vocab(train_data, max_size = 10000, min_freq = 2)\n",
    "\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "                                                (train_data, test_data),\n",
    "                                                batch_size = 32,\n",
    "                                                device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe6527-e39d-47e2-8dec-17fb877bb1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
