{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc5e292-1c4e-4f73-bb11-ca08fcb9bde3",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c101d5-3cf1-4fd4-9023-5a6a8153b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'the quick for jumped over a lazy dog'\n",
    "# 1. tokenize\n",
    "# -> ['the','quick',,,]\n",
    "\n",
    "# 2. vocab\n",
    "# -> ['the' -> 0,\n",
    "#    'quick' -> 1,\n",
    "#    ...\n",
    "\n",
    "# 3. numericalize\n",
    "# text -> idx\n",
    "\n",
    "# 4. embedding\n",
    "# idx -> latent vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e11d7-f3af-4a32-afb3-ce5fbd7cd7f1",
   "metadata": {},
   "source": [
    "# Process\n",
    "1. specify how preprocessing should be done => Fields\n",
    "2. Use Dataset to load the data => TabularDataset (Json/csv/tsv)\n",
    "3. Construct an iterator to do batching & padding => BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373fed7a-df0e-4d32-a278-ac9d8769e3c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mydata/train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/t5vx8lsx1fs7nd9c58w611bh0000gn/T/ipykernel_79900/1232436485.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# ---------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m train_data, test_data = TabularDataset.splits(\n\u001b[0m\u001b[1;32m     21\u001b[0m                                     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mydata'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         train_data = None if train is None else cls(\n\u001b[0m\u001b[1;32m     78\u001b[0m             os.path.join(path, train), **kwargs)\n\u001b[1;32m     79\u001b[0m         val_data = None if validation is None else cls(\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m             'tsv': Example.fromCSV, 'csv': Example.fromCSV}[format]\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode_csv_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcsv_reader_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mydata/train.json'"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import spacy\n",
    "\n",
    "# ---------0. tokenizer ----------------------------\n",
    "# tokenize = lambda x: x.split()\n",
    "spacy_en = spacy.load(\"en\") # python -m spacy download en\n",
    "\n",
    "def tokenize(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# --------- 1. how the text should be tokenized ----------\n",
    "quote = Field(sequential = True,\n",
    "              use_vocab = True, # if false data should be numerical\n",
    "              tokenize = tokenize,\n",
    "              lower = True)\n",
    "    # 정의한 토크나이저로 전처리 하는 메서드\n",
    "score = Field(sequential = False, # 시계열이 아님\n",
    "              use_vocab = False)\n",
    "    # target : sentiment analysis\n",
    "\n",
    "# -------------------- this goes to TabularDataset ------------------------\n",
    "fields = {\"quote\": ('q', quote), 'score':('s',score)}\n",
    "    # 데이터 셋에 있는 어떤 필드를 사용하고 싶나(?)\n",
    "    \n",
    "# ---------------------------------------------------------\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "                                    path = 'mydata',\n",
    "                                    train = 'train.json',\n",
    "                                    test = 'test.json',\n",
    "                                    #validation = 'validation.json'\n",
    "                                    format = 'json',\n",
    "                                    fields = fields)\n",
    "    # 데이터를 불러와서, fields를 통해 전처리를 하고 이를 train, test에 보낸다.\n",
    "\n",
    "# print(train_data[0].__dict__.keys())\n",
    "# print(train_data[0].__dict__.values())\n",
    "\n",
    "\n",
    "quote.build_vocab(train_data,\n",
    "                  max_size = 10000,\n",
    "                  min_freq = 2, # 2번이상은 나와야 워드로 인식\n",
    "                  #vectors = 'glove.6B.100d',\n",
    "                 )\n",
    "\n",
    "# 만약 머신 트랜스레이터라면 score도 build_vocab해줘야겟지? \n",
    "\n",
    "\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "                                    (train_data, test_data),\n",
    "                                    batch_size = 2,\n",
    "                                    device = 'cuda')\n",
    "\n",
    "\n",
    "\n",
    "for batch in train_iterator:\n",
    "    print(batch.q) # train_iterator에 들어있는 X 텐서들이 나옴. padding은 1로 부여되었음.\n",
    "    print(batch.s) # train_iterator에 있는 Y텐서들이 나옴.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7a6f9-4a03-48f9-bb01-b5f83cb52e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
