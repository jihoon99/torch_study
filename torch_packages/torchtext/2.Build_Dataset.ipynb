{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5bd38f-9c22-4ea6-bdea-faf224ca0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f9109-395b-46f2-885e-63e0ae7ad3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda uninstall -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5ba54c0-fb54-48f3-baf6-5cdb0a35cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en\n",
    "# !python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "480c5810-a6df-4c87-b571-c6308f97699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64 from MULTI30K]\n",
      "\t[.src]:[torch.LongTensor of size 21x64]\n",
      "\t[.trg]:[torch.LongTensor of size 25x64]\n",
      "4607\n",
      "word\n"
     ]
    }
   ],
   "source": [
    "tokenize_eng = lambda x : x.split()\n",
    "tokenize_ger = lambda x : x.split()\n",
    "\n",
    "english = Field(sequential = True,\n",
    "                use_vocab = True, # 단어가 숫자가 아니니까 True\n",
    "                tokenize = tokenize_eng,\n",
    "                lower = True)\n",
    "\n",
    "german = Field(sequential = True,\n",
    "               use_vocab = True,\n",
    "               tokenize = tokenize_ger,\n",
    "               lower = True)\n",
    "\n",
    "train_data, validation_data, text_data = Multi30k.splits(exts = ('.de', '.en'), # x, y\n",
    "                                                        fields = (german, english))\n",
    "\n",
    "english.build_vocab(train_data,\n",
    "                    max_size = 10000,\n",
    "                    min_freq = 2)\n",
    "\n",
    "german.build_vocab(train_data,\n",
    "                   max_size = 10000,\n",
    "                   min_freq = 2)\n",
    "\n",
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n",
    "                                                                (train_data, validation_data, text_data),\n",
    "                                                                batch_size = 64,\n",
    "                                                                device = 'cpu')\n",
    "\n",
    "for batch in train_iterator:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "# word2idx\n",
    "print(english.vocab.stoi['word']) # String TO Index\n",
    "# idx2word\n",
    "print(english.vocab.itos[4607])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
