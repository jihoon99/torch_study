{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c6d929-2be0-4045-91e0-e89e8ddae3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([23., 13., 10.,  4.]),\n",
       "indices=tensor([0, 2, 3, 1]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "l = torch.Tensor([23,4,13,10])\n",
    "l.sort(descending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb68759-a73b-431e-a699-dc24562cc193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 3, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.sort(descending = True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "156a277a-994a-4090-8f5e-6cd25a42aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25675\n",
      "21437\n",
      "torch.Size([128, 37])\n",
      "torch.Size([128, 57])\n",
      "torch.Size([128, 46])\n",
      "torch.Size([128, 55])\n",
      "torch.Size([128, 53])\n",
      "torch.Size([128, 75])\n",
      "torch.Size([128, 43])\n",
      "torch.Size([128, 77])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchtext\n",
    "version = list(map(int, torchtext.__version__.split('.')))\n",
    "if version[0] <= 0 and version[1] < 9:\n",
    "    from torchtext import data, datasets\n",
    "else:\n",
    "    from torchtext.legacy import data, datasets\n",
    "\n",
    "PAD, BOS, EOS = 1, 2, 3\n",
    "# PAD의 번호는 1, BOS는 2, EOS는 3인가보네\n",
    "\n",
    "\n",
    "# DataLoader : Field {src, tgt} -> TranslationDataset {train, valid} -> BucketIteration {train_iter, valid_iter}\n",
    "# nex(iter(train_iter)) -> \n",
    "'''\n",
    "(tensor([[ 292,   11, 1603,  ...,  117,  140,    4],\n",
    "        [  23,   97,    5,  ...,    1,    1,    1],\n",
    "        [  23, 1373,   78,  ...,    1,    1,    1],\n",
    "        ...,\n",
    "        [  42,   18,  318,  ...,    1,    1,    1],\n",
    "        [1171, 1236,  346,  ...,    1,    1,    1],\n",
    "        [ 304,  203,    8,  ...,    1,    1,    1]]),\n",
    "tensor([90, 86, 83, 67, 66, 66, 64, 61, 59, 58, 57, 54, 54, 53, 53, 53, 53, 51,\n",
    "        51, 51, 51, 50, 49, 49, 49, 48, 47, 47, 46, 45, 44, 44, 42, 42, 41, 41,\n",
    "        40, 39, 39, 37, 37, 36, 36, 36, 34, 33, 33, 33, 32, 32, 31, 31, 31, 30,\n",
    "        29, 29, 28, 27, 26, 25, 25, 24, 23, 23, 22, 21, 21, 21, 20, 20, 20, 20,\n",
    "        19, 18, 18, 17, 17, 16, 16, 15, 15, 15, 15, 14, 14, 14, 13, 13, 13, 12,\n",
    "        12, 12, 11, 11, 11,  9,  9,  8,  6,  6]))\n",
    "두번째 텐서가 뭔지 모르겟네.     \n",
    "        '''\n",
    "\n",
    "class DataLoader():\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_fn=None,\n",
    "                 valid_fn=None,\n",
    "                 exts=None,\n",
    "                 batch_size=64,\n",
    "                 device='cpu',\n",
    "                 max_vocab=99999999,\n",
    "                 max_length=255,\n",
    "                 fix_length=None,\n",
    "                 use_bos=True,\n",
    "                 use_eos=True,\n",
    "                 shuffle=True,\n",
    "                 dsl=False\n",
    "                 ):\n",
    "\n",
    "        super(DataLoader, self).__init__() # ??? 상속받을게 없는데?\n",
    "\n",
    "        # Field -> fields -> TabularDataset -> build_vocab -> Bucket\n",
    "\n",
    "        # src와 tgt가 각각 있는 이유는, 파일이 각각 있었기 때문이다.\n",
    "            # torchtext.data.Field\n",
    "        self.src = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=True,\n",
    "            fix_length=fix_length, # None\n",
    "            init_token='<BOS>' if dsl else None, # dsl : dure learning할때 필요한것. 지금은 None이라고 보면 됨.\n",
    "            eos_token='<EOS>' if dsl else None,\n",
    "        )\n",
    "\n",
    "        self.tgt = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=True,\n",
    "            fix_length=fix_length,\n",
    "            init_token='<BOS>' if use_bos else None, # True .. learning에서는 필요 없고, 생성자 할때만 필요함(?)\n",
    "            eos_token='<EOS>' if use_eos else None,\n",
    "        )\n",
    "\n",
    "        if train_fn is not None and valid_fn is not None and exts is not None:\n",
    "            # TranslationDataset는 밑에 정의 되어있습니다.\n",
    "            train = TranslationDataset(\n",
    "                path=train_fn, # train file path\n",
    "                exts=exts, # en,ko path가 튜플로 들어가 있음.\n",
    "                fields=[('src', self.src), ('tgt', self.tgt)], # 사용할 필드 명\n",
    "                max_length=max_length\n",
    "            )\n",
    "            valid = TranslationDataset(\n",
    "                path=valid_fn,\n",
    "                exts=exts,\n",
    "                fields=[('src', self.src), ('tgt', self.tgt)],\n",
    "                max_length=max_length,\n",
    "            )\n",
    "\n",
    "            # bucketIterator가 하는 일을 실제 데이터를 가지고 와서. -> pad까지 체운 형태로 만들고\n",
    "            # 미니배치 단위로 만들어주는 역할을 한다.\n",
    "            # https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator\n",
    "            self.train_iter = data.BucketIterator(\n",
    "                train,\n",
    "                batch_size=batch_size,\n",
    "                device='cpu',\n",
    "                shuffle=shuffle,\n",
    "                sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)), # ?????????????? what's x?\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "            # next(iter(train_iter)) 하면, \n",
    "\n",
    "            self.valid_iter = data.BucketIterator(\n",
    "                valid,\n",
    "                batch_size=batch_size,\n",
    "                device='cpu',\n",
    "                shuffle=False,\n",
    "                sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)),\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "\n",
    "            self.src.build_vocab(train, max_size=max_vocab)\n",
    "                # construct the vocab object for this field from one or more datasets.\n",
    "                # https://torchtext.readthedocs.io/en/latest/data.html\n",
    "                # it's word2idx : 어떤 단어가 몇번째 인덱스로 맵핑되는지.\n",
    "            self.tgt.build_vocab(train, max_size=max_vocab)\n",
    "\n",
    "    def load_vocab(self, src_vocab, tgt_vocab):\n",
    "        '''?????????????????????????????????????'''\n",
    "        self.src.vocab = src_vocab\n",
    "        self.tgt.vocab = tgt_vocab\n",
    "\n",
    "\n",
    "# torchtext에는 maxlen을 잘라주는게 없어서 customizing했어.\n",
    "class TranslationDataset(data.Dataset):\n",
    "    \"\"\"Defines a dataset for machine translation.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "\n",
    "    def __init__(self, path, exts, fields, max_length=None, **kwargs):\n",
    "        \"\"\"Create a TranslationDataset given paths and fields.\n",
    "\n",
    "        Arguments:\n",
    "            path: Common prefix of paths to the data files for both languages.\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            # fields가 [('src',src),('tgt',tgt)]형태가 아닐때 다시 정의를 함.\n",
    "            fields = [('src', fields[0]), ('trg', fields[1])]\n",
    "\n",
    "        if not path.endswith('.'):\n",
    "            # 주소의 끝에 .이 없다면 추가로 넣어줘.\n",
    "            path += '.'\n",
    "\n",
    "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
    "\n",
    "        examples = []\n",
    "        with open(src_path, encoding='utf-8') as src_file, open(trg_path, encoding='utf-8') as trg_file:\n",
    "            # src, trg path에서 파일을 불러오고 한줄씩 for문\n",
    "            for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip() # 오른쪽끝 스페이스 제거.\n",
    "                # max_length가 있을 경우에는 작업을 해줌.\n",
    "                if max_length and max_length < max(len(src_line.split()), len(trg_line.split())): \n",
    "                    # 스페이스를 띄어쓰기라고 가정, max_len보다 클때(?) 이부분 잘못된거 같은데...\n",
    "                    '''?????????????????????????????????????????'''\n",
    "                    continue\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    # 별일 없을때 examples에 데이터를 추가.\n",
    "                    examples += [data.Example.fromlist([src_line, trg_line], fields)]\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    loader = DataLoader(\n",
    "        '/Users/rainism/Desktop/grad/torch_study/transformer/data/corpus.shuf.test.tok.bpe',\n",
    "        '/Users/rainism/Desktop/grad/torch_study/transformer/data/corpus.shuf.test.tok.bpe',\n",
    "        ('ko','en'),\n",
    "        batch_size=128\n",
    "    )\n",
    "\n",
    "    print(len(loader.src.vocab))\n",
    "    print(len(loader.tgt.vocab))\n",
    "\n",
    "    for batch_index, batch in enumerate(loader.train_iter):\n",
    "        print(batch.src[0].shape)\n",
    "        print(batch.tgt[0].shape)\n",
    "        \n",
    "        if batch_index > 2:\n",
    "            break\n",
    "        '''???????????????????????????????????????????????????????'''\n",
    "        # batch.src의 shape이 계속 바뀌는데... 어떻게 처리할까?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8905afa7-e061-4fbf-8557-695b1004d8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 38])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader.train_iter)).src[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "666f9f87-7b7f-422c-ba4a-68995267ccce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 39])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader.train_iter)).src[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a25795f7-5ee8-4a5b-b7b1-68ab696798cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 112])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader.train_iter)).src[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8df95ee7-78b1-4802-b99b-09cf866f87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, length = next(iter(loader.train_iter)).src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "733c428e-8703-4cb5-9d14-b5d870199eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  512,    17,   728,  ..., 10720,     6,     2],\n",
       "        [   83,  8547,   204,  ...,     3,     6,     2],\n",
       "        [  196,  4479,     8,  ...,    44,     6,     2],\n",
       "        ...,\n",
       "        [  219,   673,     8,  ...,     6,     2,     1],\n",
       "        [  219,  9017,  1229,  ...,    72,     2,     1],\n",
       "        [  368,  1350,     8,  ...,     6,     2,     1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "444418d9-6580-4254-9e50-7ef35893e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indice = length.sort(descending = False)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b208fce1-a437-4d50-81fc-8765c6973fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 368, 1350,    8,  ...,    6,    2,    1],\n",
       "        [ 219, 9017, 1229,  ...,   72,    2,    1],\n",
       "        [ 219,  673,    8,  ...,    6,    2,    1],\n",
       "        ...,\n",
       "        [ 196, 4479,    8,  ...,   44,    6,    2],\n",
       "        [  83, 8547,  204,  ...,    3,    6,    2],\n",
       "        [ 844,  468,    4,  ...,  331,    6,    2]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.index_select(dim = 0, index = new_indice).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b2f10aa-0019-4123-abe4-c100cd36f7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  512,    17,   728,  ..., 10720,     6,     2],\n",
       "        [   83,  8547,   204,  ...,     3,     6,     2],\n",
       "        [  196,  4479,     8,  ...,    44,     6,     2],\n",
       "        ...,\n",
       "        [  219,   673,     8,  ...,     6,     2,     1],\n",
       "        [  219,  9017,  1229,  ...,    72,     2,     1],\n",
       "        [  368,  1350,     8,  ...,     6,     2,     1]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae530751-ad10-49a1-b3e9-48d53455ef43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f10b38-6641-4a69-98c2-706ec897109b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2c93d-6d1e-44e7-9180-1970404aeba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
